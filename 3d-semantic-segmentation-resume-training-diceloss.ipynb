{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803f48a6",
   "metadata": {
    "papermill": {
     "duration": 0.007766,
     "end_time": "2024-09-05T05:51:13.450605",
     "exception": false,
     "start_time": "2024-09-05T05:51:13.442839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3D Cervical Vertebrae Segmentation (RSNA 2022) with MONAI - Resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3196c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:51:13.466582Z",
     "iopub.status.busy": "2024-09-05T05:51:13.465953Z",
     "iopub.status.idle": "2024-09-05T05:52:06.957153Z",
     "shell.execute_reply": "2024-09-05T05:52:06.956089Z"
    },
    "papermill": {
     "duration": 53.501772,
     "end_time": "2024-09-05T05:52:06.959578",
     "exception": false,
     "start_time": "2024-09-05T05:51:13.457806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-gdcm\r\n",
      "  Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\r\n",
      "Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: python-gdcm\r\n",
      "Successfully installed python-gdcm-3.0.24.1\r\n",
      "Collecting pylibjpeg\r\n",
      "  Downloading pylibjpeg-2.0.1-py3-none-any.whl.metadata (7.8 kB)\r\n",
      "Collecting pylibjpeg-libjpeg\r\n",
      "  Downloading pylibjpeg_libjpeg-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: pydicom in /opt/conda/lib/python3.10/site-packages (2.4.4)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pylibjpeg) (1.26.4)\r\n",
      "Downloading pylibjpeg-2.0.1-py3-none-any.whl (24 kB)\r\n",
      "Downloading pylibjpeg_libjpeg-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pylibjpeg-libjpeg, pylibjpeg\r\n",
      "Successfully installed pylibjpeg-2.0.1 pylibjpeg-libjpeg-2.2.0\r\n",
      "Collecting pyjpegls\r\n",
      "  Downloading pyjpegls-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\r\n",
      "Requirement already satisfied: numpy>=1.24 in /opt/conda/lib/python3.10/site-packages (from pyjpegls) (1.26.4)\r\n",
      "Downloading pyjpegls-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyjpegls\r\n",
      "Successfully installed pyjpegls-1.4.0\r\n",
      "Collecting monai\r\n",
      "  Downloading monai-1.3.2-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from monai) (2.1.2)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from monai) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (2024.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\r\n",
      "Downloading monai-1.3.2-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: monai\r\n",
      "Successfully installed monai-1.3.2\r\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "! pip install python-gdcm\n",
    "! pip install pylibjpeg pylibjpeg-libjpeg pydicom\n",
    "! pip install pyjpegls\n",
    "! pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fdd8bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:06.981819Z",
     "iopub.status.busy": "2024-09-05T05:52:06.981241Z",
     "iopub.status.idle": "2024-09-05T05:52:48.935929Z",
     "shell.execute_reply": "2024-09-05T05:52:48.934845Z"
    },
    "papermill": {
     "duration": 41.96893,
     "end_time": "2024-09-05T05:52:48.938835",
     "exception": false,
     "start_time": "2024-09-05T05:52:06.969905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 05:52:39.777644: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-05 05:52:39.777795: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-05 05:52:39.915529: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "from glob import glob\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "# DICOM image files (.dcm)\n",
    "import pydicom\n",
    "from pydicom import dcmread\n",
    "\n",
    "# NIfTI image files (.nii)\n",
    "import nibabel as nib\n",
    "\n",
    "# Required dependencies\n",
    "import gdcm\n",
    "import pylibjpeg\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "# Monai\n",
    "import monai\n",
    "from monai.data import ArrayDataset, DataLoader, decollate_batch\n",
    "from monai.networks.layers import Norm\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    "    RandFlip,\n",
    "    RandAffine,\n",
    "    RandGridDistortion\n",
    ")\n",
    "from monai.utils import set_determinism, first\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d7f8a6",
   "metadata": {
    "papermill": {
     "duration": 0.010151,
     "end_time": "2024-09-05T05:52:48.959723",
     "exception": false,
     "start_time": "2024-09-05T05:52:48.949572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resuming Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6acf55be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:48.984496Z",
     "iopub.status.busy": "2024-09-05T05:52:48.983786Z",
     "iopub.status.idle": "2024-09-05T05:52:48.994814Z",
     "shell.execute_reply": "2024-09-05T05:52:48.993817Z"
    },
    "papermill": {
     "duration": 0.026876,
     "end_time": "2024-09-05T05:52:48.996953",
     "exception": false,
     "start_time": "2024-09-05T05:52:48.970077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: CT7359RN\n",
      "spatial_size: (128, 128, 128)\n",
      "prob: 0.5\n",
      "k: 5\n",
      "batch_size: 4\n",
      "epochs: 480\n",
      "lr: 0.0001\n",
      "loss_weights: (0.0, 1.0)\n",
      "channels: (16, 32, 64, 128, 256)\n",
      "strides: (2, 2, 2, 2)\n",
      "kernel_size: 3\n",
      "up_kernel_size: 3\n",
      "num_res_units: 2\n",
      "act: PRELU\n",
      "dropout: 0.0\n",
      "bias: True\n",
      "val_fold_idx: 1\n"
     ]
    }
   ],
   "source": [
    "# Path \n",
    "base_path = \"../input/rsna-2022-cervical-spine-fracture-detection\"\n",
    "TRAIN_IMAGES_PATH = f'{base_path}/train_images'\n",
    "SEGMENTATIONS_PATH = f'{base_path}/segmentations'\n",
    "OUTPUT_DIR = '.'\n",
    "\n",
    "\n",
    "last_model_output = 'CT7359RN_last_model' # INSERT MODEL ID\n",
    "last_config_pkl = 'CT7359RN_config.pkl' # INSERT MODEL ID\n",
    "MODEL_PATH = f'/kaggle/input/ct7359rn-400-480-epochs/{last_model_output}' # INSERT DATASET\n",
    "CONFIG_PATH = f'/kaggle/input/ct7359rn-400-480-epochs/{last_config_pkl}' # INSERT DATASET\n",
    "\n",
    "# Load config from the pkl file\n",
    "with open(CONFIG_PATH, 'rb') as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "for k, v in config.items(): print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6e1c98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.020612Z",
     "iopub.status.busy": "2024-09-05T05:52:49.020283Z",
     "iopub.status.idle": "2024-09-05T05:52:49.024915Z",
     "shell.execute_reply": "2024-09-05T05:52:49.024019Z"
    },
    "papermill": {
     "duration": 0.018949,
     "end_time": "2024-09-05T05:52:49.026940",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.007991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get output and config path file\n",
    "OUTPUT_FILE = OUTPUT_DIR + f'/{config[\"ID\"]}_train_val_losses.csv'                   \n",
    "CONFIG_FILE = OUTPUT_DIR + f'/{config[\"ID\"]}_config.pkl'\n",
    "\n",
    "# New max number of epochs\n",
    "config['epochs'] = 560 # INSERT MAX EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fc755b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.049446Z",
     "iopub.status.busy": "2024-09-05T05:52:49.049154Z",
     "iopub.status.idle": "2024-09-05T05:52:49.053140Z",
     "shell.execute_reply": "2024-09-05T05:52:49.052437Z"
    },
    "papermill": {
     "duration": 0.01724,
     "end_time": "2024-09-05T05:52:49.054967",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.037727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save config to pickle file\n",
    "with open(CONFIG_FILE, 'wb') as f:\n",
    "    pickle.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2da642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.076316Z",
     "iopub.status.busy": "2024-09-05T05:52:49.076075Z",
     "iopub.status.idle": "2024-09-05T05:52:49.082013Z",
     "shell.execute_reply": "2024-09-05T05:52:49.081331Z"
    },
    "papermill": {
     "duration": 0.01856,
     "end_time": "2024-09-05T05:52:49.083798",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.065238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set deterministic training for reproducibility\n",
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13834d11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.105102Z",
     "iopub.status.busy": "2024-09-05T05:52:49.104866Z",
     "iopub.status.idle": "2024-09-05T05:52:49.210476Z",
     "shell.execute_reply": "2024-09-05T05:52:49.209353Z"
    },
    "papermill": {
     "duration": 0.118497,
     "end_time": "2024-09-05T05:52:49.212369",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.093872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla T4 is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Enabling GPU\n",
    "# https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "    \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# Enable cuDNN benchmark. Set to True whenever the input model does not change over training, False if, eg, some layers are deactivated\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd505ecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.234291Z",
     "iopub.status.busy": "2024-09-05T05:52:49.234008Z",
     "iopub.status.idle": "2024-09-05T05:52:49.238092Z",
     "shell.execute_reply": "2024-09-05T05:52:49.237242Z"
    },
    "papermill": {
     "duration": 0.017327,
     "end_time": "2024-09-05T05:52:49.240165",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.222838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Masks to be reverted \n",
    "revert_mask = [\n",
    "    '1.2.826.0.1.3680043.1363',\n",
    "    '1.2.826.0.1.3680043.20120',\n",
    "    '1.2.826.0.1.3680043.2243',\n",
    "    '1.2.826.0.1.3680043.24606',\n",
    "    '1.2.826.0.1.3680043.32071'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d97a37",
   "metadata": {
    "papermill": {
     "duration": 0.010014,
     "end_time": "2024-09-05T05:52:49.260579",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.250565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b50d2163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.282215Z",
     "iopub.status.busy": "2024-09-05T05:52:49.281943Z",
     "iopub.status.idle": "2024-09-05T05:52:49.299312Z",
     "shell.execute_reply": "2024-09-05T05:52:49.298448Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.030536,
     "end_time": "2024-09-05T05:52:49.301257",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.270721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_dicom_scan(folder_path):\n",
    "    \"\"\" Read CT scan (dicom files) and stack the slices\"\"\"\n",
    "    slices = []\n",
    "    for filename in sorted(os.listdir(folder_path), key=lambda x: int(x.split(\".\")[0])):\n",
    "        if filename.endswith('.dcm'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            ds = pydicom.dcmread(filepath)\n",
    "            slices.append(ds.pixel_array)\n",
    "    scan = np.stack(slices, -1).astype('float64')\n",
    "    return scan\n",
    "\n",
    "\n",
    "def read_nifti_file(file_path, revert_mask=revert_mask):\n",
    "    \"\"\" Read nifit file segmentation\"\"\"    \n",
    "    data = nib.load(file_path).get_fdata()\n",
    "    shape = data.shape\n",
    "    # Reorient because segmentations are done over the sagittal plane\n",
    "    data = data.transpose(1, 0, 2)[::-1, :, ::-1]\n",
    "    # Revert the files that have inverted sequence (from bottom to top)\n",
    "    if file_path in revert_mask:\n",
    "        data[:, :, ::-1]\n",
    "    return data\n",
    "\n",
    "\n",
    "def zoom_volume(vol, spatial_size):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    \"\"\" NON UTILIZZATA SOSTITUITA DA RESIZE\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_width, desired_height, desired_depth = spatial_size\n",
    "    # Get current depth\n",
    "    current_depth = vol.shape[-1]\n",
    "    current_width = vol.shape[0]\n",
    "    current_height = vol.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Resize across z-axis\n",
    "    #vol = ndi.zoom(vol, (width_factor, height_factor, depth_factor), order=0, mode='constant')\n",
    "    zoom_transform = Zoom(zoom=(width_factor, height_factor, depth_factor), keep_size=False)\n",
    "    zoom_vol = zoom_transform(vol)\n",
    "    return zoom_vol\n",
    "\n",
    "def one_hot_encoding_multiclass_mask(mask):\n",
    "    \"\"\" Binary OneHot Encoding of Multi-class masks\"\"\"\n",
    "    labels = list(range(8))\n",
    "    num_labels = len(labels)\n",
    "    c, h, w, d = mask.shape\n",
    "    enc_mask = np.zeros((num_labels, h, w, d))\n",
    "    for c in range(1, num_labels):  # this loop starts from label 1 to ignore background 0\n",
    "        enc_mask[c, :, :, :] = (mask == labels[c]) # 1 for the pixel belonging to that class, 0 for the rest of the pixel\n",
    "        \n",
    "    return enc_mask\n",
    "\n",
    "def expand_dims(arr):\n",
    "    return np.expand_dims(arr, axis=0)\n",
    "\n",
    "def training_plot(file, output_path, config):\n",
    "    train_bce_dl_loss = file['Train_bce_dl_loss']\n",
    "    val_bce_dl_loss = file['Val_bce_dl_loss']\n",
    "    epochs = range(1, len(train_bce_dl_loss) + 1)\n",
    "    plt.plot(epochs, train_bce_dl_loss, label='Training BCE-DiceLoss', color='darkblue')\n",
    "    plt.plot(epochs, val_bce_dl_loss, label='Val BCE-DiceLoss', color='darkorange')\n",
    "    plt.title('Training & Val Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.savefig(os.path.join(output_path, f'{config[\"ID\"]}_Training_Losses_plot.png'))\n",
    "    plt.show()\n",
    "    \n",
    "def validation_metric_plot(file, output_path, config):\n",
    "    val_metric = file['Val_metric']\n",
    "    epochs = range(1, len(val_metric)+1)\n",
    "    plt.plot(epochs, val_metric, label='Validation DiceMetric', color='red')\n",
    "    plt.title('Validation Metric')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.savefig(os.path.join(output_path, f'{config[\"ID\"]}_Validation_Metric_plot.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20735c12",
   "metadata": {
    "papermill": {
     "duration": 0.010319,
     "end_time": "2024-09-05T05:52:49.321941",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.311622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b6dc0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.344874Z",
     "iopub.status.busy": "2024-09-05T05:52:49.344612Z",
     "iopub.status.idle": "2024-09-05T05:52:49.419942Z",
     "shell.execute_reply": "2024-09-05T05:52:49.419056Z"
    },
    "papermill": {
     "duration": 0.088988,
     "end_time": "2024-09-05T05:52:49.421841",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.332853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label_path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.780</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.21321</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.6125</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.826.0.1.3680043.30067</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.826.0.1.3680043.12833</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ID  \\\n",
       "0    1.2.826.0.1.3680043.780   \n",
       "1  1.2.826.0.1.3680043.21321   \n",
       "2   1.2.826.0.1.3680043.6125   \n",
       "3  1.2.826.0.1.3680043.30067   \n",
       "4  1.2.826.0.1.3680043.12833   \n",
       "\n",
       "                                          label_path  \\\n",
       "0  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
       "1  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
       "2  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
       "3  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
       "4  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
       "\n",
       "                                          image_path  fold  \n",
       "0  ../input/rsna-2022-cervical-spine-fracture-det...     0  \n",
       "1  ../input/rsna-2022-cervical-spine-fracture-det...     4  \n",
       "2  ../input/rsna-2022-cervical-spine-fracture-det...     4  \n",
       "3  ../input/rsna-2022-cervical-spine-fracture-det...     2  \n",
       "4  ../input/rsna-2022-cervical-spine-fracture-det...     0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataset\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Store all the nifti files in the segmentation folder\n",
    "df['ID'] = os.listdir(SEGMENTATIONS_PATH)\n",
    "\n",
    "# Remove the extension '.nii'\n",
    "df['ID'] = df['ID'].apply(lambda x: x[:-4])\n",
    "\n",
    "# Add complete path to reach segmentation file (nifti)\n",
    "df['label_path'] = df['ID'].apply(lambda x: os.path.join(SEGMENTATIONS_PATH, x + '.nii'))\n",
    "\n",
    "# Add complete path to reach CT scan folder in train_images\n",
    "df['image_path'] = df['ID'].apply(lambda x: os.path.join(TRAIN_IMAGES_PATH, x))\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=config['k'], shuffle=True, random_state=42)\n",
    "\n",
    "# Create a new column for fold indices\n",
    "df['fold'] = -1\n",
    "\n",
    "# Assign fold indices\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(df)):\n",
    "    df.loc[val_index, 'fold'] = fold\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(df.shape)\n",
    "\n",
    "# Show the head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f7d53cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.444577Z",
     "iopub.status.busy": "2024-09-05T05:52:49.444066Z",
     "iopub.status.idle": "2024-09-05T05:52:49.456890Z",
     "shell.execute_reply": "2024-09-05T05:52:49.455874Z"
    },
    "papermill": {
     "duration": 0.026326,
     "end_time": "2024-09-05T05:52:49.458978",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.432652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69, 5)\n",
      "(18, 5)\n"
     ]
    }
   ],
   "source": [
    "# Define training and validation dataset\n",
    "\n",
    "val_fold_idx = config['val_fold_idx']\n",
    "\n",
    "df_train = df[df.fold != val_fold_idx].reset_index()\n",
    "print(df_train.shape)\n",
    "\n",
    "df_val = df[df.fold == val_fold_idx].reset_index()\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780b2d3",
   "metadata": {
    "papermill": {
     "duration": 0.010889,
     "end_time": "2024-09-05T05:52:49.480907",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.470018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2daba301",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.503698Z",
     "iopub.status.busy": "2024-09-05T05:52:49.503397Z",
     "iopub.status.idle": "2024-09-05T05:52:49.528265Z",
     "shell.execute_reply": "2024-09-05T05:52:49.527435Z"
    },
    "papermill": {
     "duration": 0.03854,
     "end_time": "2024-09-05T05:52:49.530201",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.491661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define training transforms for image and label\n",
    "train_image_trans = Compose(\n",
    "    [   \n",
    "        # Load Data\n",
    "        read_dicom_scan,\n",
    "        # Data Preparation\n",
    "        expand_dims, \n",
    "        Resize(spatial_size=config['spatial_size'], mode=\"area\"), # Resize the volume to target spatial_size\n",
    "        ScaleIntensity(), # scale between (0,1)\n",
    "        # Data Augmentation \n",
    "        RandFlip(prob=config['prob'], spatial_axis=0), # width\n",
    "        RandFlip(prob=config['prob'], spatial_axis=1), # height\n",
    "        RandGridDistortion(num_cells=5, distort_limit=(-0.03, 0.03), prob=config['prob']),\n",
    "        RandAffine(prob=config['prob'], \n",
    "                   translate_range=[int(x*y) for x, y in zip(config['spatial_size'], [0.3, 0.3, 0.3])], padding_mode='zeros')\n",
    "    ]\n",
    ")\n",
    "train_label_trans = Compose(\n",
    "    [   \n",
    "        # Load data\n",
    "        read_nifti_file,\n",
    "        # Data Preparation\n",
    "        expand_dims,\n",
    "        Resize(spatial_size=config['spatial_size'], mode=\"area\"),\n",
    "        one_hot_encoding_multiclass_mask, \n",
    "        # Data Augmentation\n",
    "        RandFlip(prob=config['prob'], spatial_axis=0), # width\n",
    "        RandFlip(prob=config['prob'], spatial_axis=1), # height\n",
    "        RandGridDistortion(num_cells=5, distort_limit=(-0.03, 0.03), prob=config['prob']),\n",
    "        RandAffine(prob=config['prob'], \n",
    "                   translate_range=[int(x*y) for x, y in zip(config['spatial_size'], [0.3, 0.3, 0.3])], padding_mode='zeros')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define validation transforms for image and label (no augmentation)\n",
    "val_image_trans = Compose(\n",
    "    [\n",
    "        # Load data\n",
    "        read_dicom_scan,\n",
    "        # Data Preparation\n",
    "        expand_dims,\n",
    "        Resize(spatial_size=config['spatial_size'], mode=\"area\"), \n",
    "        ScaleIntensity()\n",
    "    ]\n",
    ")\n",
    "val_label_trans = Compose(\n",
    "    [\n",
    "        # Load data\n",
    "        read_nifti_file,\n",
    "        # Data preparation\n",
    "        expand_dims,\n",
    "        Resize(spatial_size=config['spatial_size'], mode=\"area\"),\n",
    "        one_hot_encoding_multiclass_mask\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acda239d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.553579Z",
     "iopub.status.busy": "2024-09-05T05:52:49.552836Z",
     "iopub.status.idle": "2024-09-05T05:52:49.559548Z",
     "shell.execute_reply": "2024-09-05T05:52:49.558668Z"
    },
    "papermill": {
     "duration": 0.020237,
     "end_time": "2024-09-05T05:52:49.561478",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.541241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define train dataset and dataloader\n",
    "train_ds = ArrayDataset(df_train.image_path, train_image_trans, df_train.label_path, train_label_trans)\n",
    "train_loader = DataLoader(train_ds, batch_size=config['batch_size'], num_workers=2)\n",
    "\n",
    "# Define validation dataset and dataloader\n",
    "val_ds = ArrayDataset(df_val.image_path, val_image_trans, df_val.label_path, val_label_trans)\n",
    "val_loader = DataLoader(val_ds, batch_size=config['batch_size'], num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3acc9359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:52:49.584542Z",
     "iopub.status.busy": "2024-09-05T05:52:49.584247Z",
     "iopub.status.idle": "2024-09-05T05:53:35.467457Z",
     "shell.execute_reply": "2024-09-05T05:53:35.466118Z"
    },
    "papermill": {
     "duration": 45.908829,
     "end_time": "2024-09-05T05:53:35.481205",
     "exception": false,
     "start_time": "2024-09-05T05:52:49.572376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 128, 128, 128]) torch.Size([4, 8, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Take the first processed train batch and print the shape\n",
    "first_train_image, first_train_label = first(train_loader)\n",
    "print(first_train_image.shape, first_train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76e057",
   "metadata": {
    "papermill": {
     "duration": 0.011201,
     "end_time": "2024-09-05T05:53:35.504345",
     "exception": false,
     "start_time": "2024-09-05T05:53:35.493144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resuming Model, Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2edb5bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:53:35.528593Z",
     "iopub.status.busy": "2024-09-05T05:53:35.528216Z",
     "iopub.status.idle": "2024-09-05T05:53:35.778140Z",
     "shell.execute_reply": "2024-09-05T05:53:35.777155Z"
    },
    "papermill": {
     "duration": 0.264873,
     "end_time": "2024-09-05T05:53:35.780668",
     "exception": false,
     "start_time": "2024-09-05T05:53:35.515795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the 3D Unet model\n",
    "unet_model = UNet(\n",
    "    spatial_dims = 3, # (Height, Width, Depth)\n",
    "    in_channels = 1,\n",
    "    out_channels = 8, # 8 Binary mask 7 as the vertebrae(C1->C7) + background\n",
    "    channels = config['channels'], # Channels per layer\n",
    "    strides = config['strides'], # Stride per layers\n",
    "    kernel_size = config['kernel_size'], # Size of the kernel for each layer\n",
    "    up_kernel_size = config['up_kernel_size'], # Upsampling convolution kernel size\n",
    "    num_res_units = config['num_res_units'], # Number of residual units\n",
    "    act = config['act'], # Activation function\n",
    "    dropout = config['dropout'], # Dropout rate\n",
    "    bias = config['bias'] # Presence of bias term in convolution blocks\n",
    ")\n",
    "unet_model = unet_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "461000cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:53:35.805534Z",
     "iopub.status.busy": "2024-09-05T05:53:35.805226Z",
     "iopub.status.idle": "2024-09-05T05:53:35.812479Z",
     "shell.execute_reply": "2024-09-05T05:53:35.811624Z"
    },
    "papermill": {
     "duration": 0.021888,
     "end_time": "2024-09-05T05:53:35.814424",
     "exception": false,
     "start_time": "2024-09-05T05:53:35.792536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Loss function\n",
    "\n",
    "# DiceLoss function\n",
    "#loss_function = DiceLoss(\n",
    "#    include_background = True,  # If False, channel index 0 (background) is excluded from the calculation\n",
    "#    squared_pred = False, # Use squared versions of targets and predictions in the denominator or not\n",
    "#    reduction = 'mean' # Reduction to apply to the output\n",
    "#)\n",
    "\n",
    "# BCE-DiceLoss function\n",
    "# Define DiceLoss (MONAI)\n",
    "dice_loss_fn = DiceLoss(\n",
    "    include_background=True,  # Include background class in the Dice computation\n",
    "    to_onehot_y=False,  # Assuming the labels are not one-hot encoded\n",
    "    sigmoid=False,  # Apply sigmoid to the input tensor\n",
    "    softmax=True,  # Do not apply softmax to the input tensor\n",
    "    squared_pred=True,  # Do not use squared predictions\n",
    "    reduction='mean', # Reduction to apply to the output\n",
    "    smooth_nr=1.0, # constant added to the numerator to avoid zero\n",
    "    smooth_dr=1.0 # constant added to the denominator to avoid nan\n",
    ")\n",
    "# Define BCEWithLogitsLoss (PyTorch)\n",
    "bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "# Combine BCE and Dice losses\n",
    "def bce_diceloss(input, target, loss_weights=config['loss_weights']):    # Compute the BCE loss\n",
    "    bce_loss = loss_weights[0] * bce_loss_fn(input, target)\n",
    "    # Compute the Dice loss\n",
    "    dice_loss = loss_weights[1] * dice_loss_fn(input, target)\n",
    "    # Combine the losses\n",
    "    total_loss = (bce_loss + dice_loss) / sum(loss_weights)\n",
    "    return total_loss\n",
    "# Set the combined loss function\n",
    "criterion = bce_diceloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "418634db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:53:35.838999Z",
     "iopub.status.busy": "2024-09-05T05:53:35.838529Z",
     "iopub.status.idle": "2024-09-05T05:53:35.842674Z",
     "shell.execute_reply": "2024-09-05T05:53:35.841814Z"
    },
    "papermill": {
     "duration": 0.018437,
     "end_time": "2024-09-05T05:53:35.844679",
     "exception": false,
     "start_time": "2024-09-05T05:53:35.826242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define metric\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ba77464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:53:35.868752Z",
     "iopub.status.busy": "2024-09-05T05:53:35.868504Z",
     "iopub.status.idle": "2024-09-05T05:53:35.874466Z",
     "shell.execute_reply": "2024-09-05T05:53:35.873624Z"
    },
    "papermill": {
     "duration": 0.020298,
     "end_time": "2024-09-05T05:53:35.876583",
     "exception": false,
     "start_time": "2024-09-05T05:53:35.856285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "\n",
    "# Adam optimzier\n",
    "#optimizer = torch.optim.Adam(\n",
    "#    unet_model.parameters(), # Model's params\n",
    "#    lr = 1e-3 # Learning rate\n",
    "#)\n",
    "\n",
    "# AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    unet_model.parameters(), \n",
    "    lr = config['lr'] # weight_decay = config['weight_decay'] \n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "368dab1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:53:35.901104Z",
     "iopub.status.busy": "2024-09-05T05:53:35.900854Z",
     "iopub.status.idle": "2024-09-05T05:53:35.906293Z",
     "shell.execute_reply": "2024-09-05T05:53:35.905395Z"
    },
    "papermill": {
     "duration": 0.020032,
     "end_time": "2024-09-05T05:53:35.908365",
     "exception": false,
     "start_time": "2024-09-05T05:53:35.888333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Define the transforms to apply to model prediction \n",
    "post_trans = Compose([Activations(sigmoid=False, softmax=True, dim=0), AsDiscrete(threshold=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac292d7",
   "metadata": {
    "papermill": {
     "duration": 0.011593,
     "end_time": "2024-09-05T05:53:35.932051",
     "exception": false,
     "start_time": "2024-09-05T05:53:35.920458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resuming Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd4d60bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:53:35.956034Z",
     "iopub.status.busy": "2024-09-05T05:53:35.955791Z",
     "iopub.status.idle": "2024-09-05T05:53:36.386983Z",
     "shell.execute_reply": "2024-09-05T05:53:36.386216Z"
    },
    "papermill": {
     "duration": 0.445649,
     "end_time": "2024-09-05T05:53:36.389281",
     "exception": false,
     "start_time": "2024-09-05T05:53:35.943632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resume State from checkpoint\n",
    "checkpoint = torch.load(MODEL_PATH)\n",
    "\n",
    "# Resume model, optimizer, learning rate scheduler, loss and last epoch \n",
    "unet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "#loss = checkpoint['avg_train_bce_dl_loss']\n",
    "start_epoch = checkpoint['epoch']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad654a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:53:36.415618Z",
     "iopub.status.busy": "2024-09-05T05:53:36.415283Z",
     "iopub.status.idle": "2024-09-05T05:53:36.441271Z",
     "shell.execute_reply": "2024-09-05T05:53:36.440449Z"
    },
    "papermill": {
     "duration": 0.041271,
     "end_time": "2024-09-05T05:53:36.443213",
     "exception": false,
     "start_time": "2024-09-05T05:53:36.401942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resuming_train_loop(model,\n",
    "                        train_loader,\n",
    "                        val_loader,\n",
    "                        diceloss_function,\n",
    "                        bce_function,\n",
    "                        bce_diceloss_function,\n",
    "                        metric,\n",
    "                        optimizer,\n",
    "                        lr_scheduler,\n",
    "                        start_epoch,\n",
    "                        config,\n",
    "                        output_dir,\n",
    "                        output_file,\n",
    "                        device):\n",
    "    \n",
    "    # Container to store train losses values per epoch\n",
    "    train_dl_values = []\n",
    "    train_bce_values = []\n",
    "    train_bce_dl_values = []\n",
    "    \n",
    "    # Container to store val losses values per epoch\n",
    "    val_dl_values = []\n",
    "    val_bce_values = []\n",
    "    val_bce_dl_values = []\n",
    "\n",
    "    # Container to store val metric values per epoch\n",
    "    val_metric_values = []\n",
    "\n",
    "    # Store best val metric\n",
    "    best_val_metric = -1\n",
    "\n",
    "    total_start = time.time()\n",
    "    # Iterate over the epochs\n",
    "    for epoch in range(start_epoch, config['epochs']+1):\n",
    "        epoch_start = time.time()\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"EPOCH {start_epoch}/{config['epochs']}\")\n",
    "        model.train() # Set the model in training mode\n",
    "        epoch_train_dl_loss, epoch_train_bce_loss, epoch_train_bce_dl_loss = 0, 0, 0\n",
    "        epoch_val_dl_loss, epoch_val_bce_loss, epoch_val_bce_dl_loss = 0, 0, 0\n",
    "        # Iterate over the batches\n",
    "        for step, batch_data in enumerate(train_loader):\n",
    "            step_start = time.time()\n",
    "            inputs, labels = batch_data\n",
    "            inputs, labels = (inputs.to(device), labels.to(device))\n",
    "            optimizer.zero_grad() # Clear the old gradients before computing new ones\n",
    "            # Enable automatic mixed precision (amp)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs) # Make predictions for current batch\n",
    "                train_dl_loss = diceloss_function(outputs, labels) # Compute DiceLoss (train)\n",
    "                train_bce_loss = bce_function(outputs, labels) # Compute BCE loss (train)\n",
    "                train_bce_dl_loss = bce_diceloss_function(outputs, labels) # Compute BCE-DiceLoss (train)\n",
    "            scaler.scale(train_bce_dl_loss).backward() # Compute the gradients\n",
    "            scaler.step(optimizer) # Update model weights\n",
    "            scaler.update()\n",
    "            epoch_train_dl_loss += train_dl_loss.item()\n",
    "            epoch_train_bce_loss += train_bce_loss.item()\n",
    "            epoch_train_bce_dl_loss += train_bce_dl_loss.item()\n",
    "            # REPORT PER BATCH \n",
    "            print(\n",
    "                f\"batch: {step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "                f\", train_dl_loss: {train_dl_loss.item():.4f}\"\n",
    "                f\", train_bce_loss: {train_bce_loss.item():.4f}\"\n",
    "                f\", train_bce_dl_loss: {train_bce_dl_loss.item():.4f}\"\n",
    "                f\", step time: {(time.time() - step_start):.4f}\"\n",
    "            )\n",
    "        lr_scheduler.step()\n",
    "        # Compute mean losses over the batches\n",
    "        avg_train_dl_loss = epoch_train_dl_loss / (step+1)\n",
    "        avg_train_bce_loss = epoch_train_bce_loss / (step+1)\n",
    "        avg_train_bce_dl_loss = epoch_train_bce_dl_loss / (step+1)\n",
    "\n",
    "        # EVALUATE MODEL ON VALIDATION SET\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        with torch.no_grad(): # Disable gradient computation and reduce memory consumption\n",
    "            for step, val_data in enumerate(val_loader):\n",
    "                val_inputs, val_labels = val_data\n",
    "                val_inputs, val_labels = (val_inputs.to(device), val_labels.to(device))\n",
    "                val_outputs = model(val_inputs)\n",
    "                # Compute losses\n",
    "                val_dl_loss = diceloss_function(val_outputs, val_labels) # Compute DiceLoss (val)\n",
    "                val_bce_loss = bce_function(val_outputs, val_labels) # Compute BCE (val)\n",
    "                val_bce_dl_loss = bce_diceloss_function(val_outputs, val_labels) # Compute BCE-DiceLoss (val)\n",
    "                # Apply post transforms\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                # Compute metric\n",
    "                metric(y_pred=val_outputs, y=val_labels)\n",
    "                \n",
    "                epoch_val_dl_loss += val_dl_loss.item()\n",
    "                epoch_val_bce_loss += val_bce_loss.item()\n",
    "                epoch_val_bce_dl_loss += val_bce_dl_loss.item()\n",
    "                \n",
    "            # Aggregate the final mean dice result\n",
    "            avg_val_metric = metric.aggregate().item()\n",
    "            \n",
    "        # Compute mean val losses over the batches\n",
    "        avg_val_dl_loss = epoch_val_dl_loss / (step + 1)\n",
    "        avg_val_bce_loss = epoch_val_bce_loss / (step + 1)\n",
    "        avg_val_bce_dl_loss = epoch_val_bce_dl_loss / (step + 1)\n",
    "\n",
    "        # REPORT PER EPOCH\n",
    "        print(f'LOSS train DiceLoss: {avg_train_dl_loss:.4f}, LOSS train BCE: {avg_train_bce_loss:.4f}, LOSS train BCE-DiceLoss: {avg_train_bce_dl_loss:.4f}, LOSS val DiceLoss: {avg_val_dl_loss:.4f}, LOSS val BCE: {avg_val_bce_loss:.4f}, LOSS val BCE-DiceLoss: {avg_val_bce_dl_loss:.4f}, METRIC val: {avg_val_metric:.4f}')\n",
    "\n",
    "        # Store train/val losses and val metric per epoch\n",
    "        train_dl_values.append(avg_train_dl_loss)\n",
    "        train_bce_values.append(avg_train_bce_loss)\n",
    "        train_bce_dl_values.append(avg_train_bce_dl_loss)\n",
    "        val_dl_values.append(avg_val_dl_loss)\n",
    "        val_bce_values.append(avg_val_bce_loss)\n",
    "        val_bce_dl_values.append(avg_val_bce_dl_loss)\n",
    "        val_metric_values.append(avg_val_metric)\n",
    "        \n",
    "        # Reset the metric status\n",
    "        metric.reset()\n",
    "\n",
    "        # Log the running loss averaged per batch for train and the running metric averaged per batch for val\n",
    "        with open(output_file, \"a\") as file:\n",
    "            file.write(f\"{epoch}, {avg_train_dl_loss}, {avg_train_bce_loss}, {avg_train_bce_dl_loss}, {avg_val_dl_loss}, {avg_val_bce_loss}, {avg_val_bce_dl_loss}, {avg_val_metric}\\n\")\n",
    "        \n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_val_metric > best_val_metric:\n",
    "            best_val_metric = avg_val_metric\n",
    "            best_model = {'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                          'train DiceLoss loss': avg_train_dl_loss,\n",
    "                          'train BCE loss': avg_train_bce_loss,\n",
    "                          'train BCE-DiceLoss loss': avg_train_bce_dl_loss,\n",
    "                          'val DiceLoss loss': avg_val_dl_loss,\n",
    "                          'val BCE loss': avg_val_bce_loss,\n",
    "                          'val BCE-DiceLoss loss': avg_val_bce_dl_loss\n",
    "                         }\n",
    "        print(f\"time consuming of epoch {epoch} is: {(time.time() - epoch_start):.4f}\")\n",
    "        \n",
    "        # Save last model's state\n",
    "        if epoch == config['epochs']:\n",
    "            last_model = {'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                          'train DiceLoss loss': avg_train_dl_loss,\n",
    "                          'train BCE loss': avg_train_bce_loss,\n",
    "                          'train BCE-DiceLoss loss': avg_train_bce_dl_loss,\n",
    "                          'val DiceLoss loss': avg_val_dl_loss,\n",
    "                          'val BCE loss': avg_val_bce_loss,\n",
    "                          'val BCE-DiceLoss loss': avg_val_bce_dl_loss\n",
    "                          }\n",
    "            \n",
    "    total_time = time.time() - total_start\n",
    "\n",
    "    # Save Train Losses and Val Metric\n",
    "    with open(OUTPUT_FILE, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Train_dl_loss', 'Train_bce_loss', 'Train_bce_dl_loss', 'Val_dl_loss', 'Val_bce_loss', 'Val_bce_dl_loss', 'Val_metric'])\n",
    "        csvwriter.writerows(zip(train_dl_values, train_bce_values, train_bce_dl_values, val_dl_values, val_bce_values, val_bce_dl_values, val_metric_values))    \n",
    "        \n",
    "    # Save Best Model's State\n",
    "    best_model_path = os.path.join(OUTPUT_DIR, f\"{config['ID']}_best_model\")\n",
    "    torch.save(best_model, best_model_path)\n",
    "\n",
    "    # Save Last Model's State\n",
    "    last_model_path = os.path.join(OUTPUT_DIR, f\"{config['ID']}_last_model\")\n",
    "    torch.save(last_model, last_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "114593a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T05:53:36.467284Z",
     "iopub.status.busy": "2024-09-05T05:53:36.467034Z",
     "iopub.status.idle": "2024-09-05T15:36:29.126751Z",
     "shell.execute_reply": "2024-09-05T15:36:29.125580Z"
    },
    "papermill": {
     "duration": 34972.676128,
     "end_time": "2024-09-05T15:36:29.131015",
     "exception": false,
     "start_time": "2024-09-05T05:53:36.454887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3651, train_bce_loss: 1.7441, train_bce_dl_loss: 0.3651, step time: 5.3051\n",
      "batch: 1/17, train_dl_loss: 0.3568, train_bce_loss: 1.7667, train_bce_dl_loss: 0.3568, step time: 0.3877\n",
      "batch: 2/17, train_dl_loss: 0.3970, train_bce_loss: 1.7513, train_bce_dl_loss: 0.3970, step time: 0.4308\n",
      "batch: 3/17, train_dl_loss: 0.5065, train_bce_loss: 1.7585, train_bce_dl_loss: 0.5065, step time: 0.3691\n",
      "batch: 4/17, train_dl_loss: 0.3676, train_bce_loss: 1.7580, train_bce_dl_loss: 0.3676, step time: 0.4146\n",
      "batch: 5/17, train_dl_loss: 0.3591, train_bce_loss: 1.7710, train_bce_dl_loss: 0.3591, step time: 0.3629\n",
      "batch: 6/17, train_dl_loss: 0.4088, train_bce_loss: 1.7533, train_bce_dl_loss: 0.4088, step time: 0.4070\n",
      "batch: 7/17, train_dl_loss: 0.3572, train_bce_loss: 1.7447, train_bce_dl_loss: 0.3572, step time: 0.3713\n",
      "batch: 8/17, train_dl_loss: 0.3690, train_bce_loss: 1.7606, train_bce_dl_loss: 0.3690, step time: 0.4239\n",
      "batch: 9/17, train_dl_loss: 0.3982, train_bce_loss: 1.7652, train_bce_dl_loss: 0.3982, step time: 0.3688\n",
      "batch: 10/17, train_dl_loss: 0.5201, train_bce_loss: 1.7578, train_bce_dl_loss: 0.5201, step time: 0.4314\n",
      "batch: 11/17, train_dl_loss: 0.3673, train_bce_loss: 1.7203, train_bce_dl_loss: 0.3673, step time: 0.4571\n",
      "batch: 12/17, train_dl_loss: 0.4005, train_bce_loss: 1.7542, train_bce_dl_loss: 0.4005, step time: 0.4344\n",
      "batch: 13/17, train_dl_loss: 0.4702, train_bce_loss: 1.7498, train_bce_dl_loss: 0.4702, step time: 0.3659\n",
      "batch: 14/17, train_dl_loss: 0.3989, train_bce_loss: 1.7637, train_bce_dl_loss: 0.3989, step time: 0.4271\n",
      "batch: 15/17, train_dl_loss: 0.3638, train_bce_loss: 1.7659, train_bce_dl_loss: 0.3638, step time: 0.3759\n",
      "batch: 16/17, train_dl_loss: 0.3600, train_bce_loss: 1.7529, train_bce_dl_loss: 0.3600, step time: 0.4256\n",
      "batch: 17/17, train_dl_loss: 0.3235, train_bce_loss: 1.7453, train_bce_dl_loss: 0.3235, step time: 1.2179\n",
      "LOSS train DiceLoss: 0.3939, LOSS train BCE: 1.7546, LOSS train BCE-DiceLoss: 0.3939, LOSS val DiceLoss: 0.5090, LOSS val BCE: 1.7434, LOSS val BCE-DiceLoss: 0.5090, METRIC val: 0.5263\n",
      "time consuming of epoch 481 is: 460.8338\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3674, train_bce_loss: 1.7489, train_bce_dl_loss: 0.3674, step time: 0.4236\n",
      "batch: 1/17, train_dl_loss: 0.3884, train_bce_loss: 1.7489, train_bce_dl_loss: 0.3884, step time: 0.3768\n",
      "batch: 2/17, train_dl_loss: 0.3945, train_bce_loss: 1.7436, train_bce_dl_loss: 0.3945, step time: 0.4303\n",
      "batch: 3/17, train_dl_loss: 0.4307, train_bce_loss: 1.7451, train_bce_dl_loss: 0.4307, step time: 0.3723\n",
      "batch: 4/17, train_dl_loss: 0.3690, train_bce_loss: 1.7322, train_bce_dl_loss: 0.3690, step time: 0.4236\n",
      "batch: 5/17, train_dl_loss: 0.3933, train_bce_loss: 1.7339, train_bce_dl_loss: 0.3933, step time: 0.3765\n",
      "batch: 6/17, train_dl_loss: 0.4042, train_bce_loss: 1.7548, train_bce_dl_loss: 0.4042, step time: 0.4348\n",
      "batch: 7/17, train_dl_loss: 0.3612, train_bce_loss: 1.7211, train_bce_dl_loss: 0.3612, step time: 0.3820\n",
      "batch: 8/17, train_dl_loss: 0.3811, train_bce_loss: 1.7250, train_bce_dl_loss: 0.3811, step time: 0.4180\n",
      "batch: 9/17, train_dl_loss: 0.3679, train_bce_loss: 1.7582, train_bce_dl_loss: 0.3679, step time: 0.3739\n",
      "batch: 10/17, train_dl_loss: 0.4832, train_bce_loss: 1.7535, train_bce_dl_loss: 0.4832, step time: 0.4207\n",
      "batch: 11/17, train_dl_loss: 0.3568, train_bce_loss: 1.7390, train_bce_dl_loss: 0.3568, step time: 0.4338\n",
      "batch: 12/17, train_dl_loss: 0.3868, train_bce_loss: 1.7430, train_bce_dl_loss: 0.3868, step time: 0.4484\n",
      "batch: 13/17, train_dl_loss: 0.4955, train_bce_loss: 1.7558, train_bce_dl_loss: 0.4955, step time: 0.3826\n",
      "batch: 14/17, train_dl_loss: 0.4009, train_bce_loss: 1.7714, train_bce_dl_loss: 0.4009, step time: 0.4407\n",
      "batch: 15/17, train_dl_loss: 0.3841, train_bce_loss: 1.7721, train_bce_dl_loss: 0.3841, step time: 0.3769\n",
      "batch: 16/17, train_dl_loss: 0.3948, train_bce_loss: 1.7570, train_bce_dl_loss: 0.3948, step time: 0.4300\n",
      "batch: 17/17, train_dl_loss: 0.3228, train_bce_loss: 1.7497, train_bce_dl_loss: 0.3228, step time: 0.1127\n",
      "LOSS train DiceLoss: 0.3935, LOSS train BCE: 1.7474, LOSS train BCE-DiceLoss: 0.3935, LOSS val DiceLoss: 0.5106, LOSS val BCE: 1.7466, LOSS val BCE-DiceLoss: 0.5106, METRIC val: 0.5249\n",
      "time consuming of epoch 482 is: 424.7696\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.4055, train_bce_loss: 1.7520, train_bce_dl_loss: 0.4055, step time: 0.4320\n",
      "batch: 1/17, train_dl_loss: 0.3707, train_bce_loss: 1.7509, train_bce_dl_loss: 0.3707, step time: 0.3816\n",
      "batch: 2/17, train_dl_loss: 0.3517, train_bce_loss: 1.7311, train_bce_dl_loss: 0.3517, step time: 0.4163\n",
      "batch: 3/17, train_dl_loss: 0.4593, train_bce_loss: 1.7496, train_bce_dl_loss: 0.4593, step time: 0.3730\n",
      "batch: 4/17, train_dl_loss: 0.3931, train_bce_loss: 1.7547, train_bce_dl_loss: 0.3931, step time: 0.4246\n",
      "batch: 5/17, train_dl_loss: 0.3752, train_bce_loss: 1.7537, train_bce_dl_loss: 0.3752, step time: 0.3842\n",
      "batch: 6/17, train_dl_loss: 0.4544, train_bce_loss: 1.7644, train_bce_dl_loss: 0.4544, step time: 0.4319\n",
      "batch: 7/17, train_dl_loss: 0.4021, train_bce_loss: 1.7482, train_bce_dl_loss: 0.4021, step time: 0.3756\n",
      "batch: 8/17, train_dl_loss: 0.3799, train_bce_loss: 1.7334, train_bce_dl_loss: 0.3799, step time: 0.4275\n",
      "batch: 9/17, train_dl_loss: 0.3891, train_bce_loss: 1.7462, train_bce_dl_loss: 0.3891, step time: 0.3783\n",
      "batch: 10/17, train_dl_loss: 0.4002, train_bce_loss: 1.7576, train_bce_dl_loss: 0.4002, step time: 0.4387\n",
      "batch: 11/17, train_dl_loss: 0.3668, train_bce_loss: 1.7410, train_bce_dl_loss: 0.3668, step time: 0.3816\n",
      "batch: 12/17, train_dl_loss: 0.3706, train_bce_loss: 1.7496, train_bce_dl_loss: 0.3706, step time: 0.4291\n",
      "batch: 13/17, train_dl_loss: 0.4621, train_bce_loss: 1.7540, train_bce_dl_loss: 0.4621, step time: 0.3769\n",
      "batch: 14/17, train_dl_loss: 0.4264, train_bce_loss: 1.7649, train_bce_dl_loss: 0.4264, step time: 0.4399\n",
      "batch: 15/17, train_dl_loss: 0.4381, train_bce_loss: 1.7503, train_bce_dl_loss: 0.4381, step time: 0.3776\n",
      "batch: 16/17, train_dl_loss: 0.4203, train_bce_loss: 1.7501, train_bce_dl_loss: 0.4203, step time: 0.4174\n",
      "batch: 17/17, train_dl_loss: 0.5417, train_bce_loss: 1.7763, train_bce_dl_loss: 0.5417, step time: 0.1124\n",
      "LOSS train DiceLoss: 0.4115, LOSS train BCE: 1.7515, LOSS train BCE-DiceLoss: 0.4115, LOSS val DiceLoss: 0.5089, LOSS val BCE: 1.7463, LOSS val BCE-DiceLoss: 0.5089, METRIC val: 0.5262\n",
      "time consuming of epoch 483 is: 408.8574\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.4528, train_bce_loss: 1.7482, train_bce_dl_loss: 0.4528, step time: 0.4328\n",
      "batch: 1/17, train_dl_loss: 0.4606, train_bce_loss: 1.7826, train_bce_dl_loss: 0.4606, step time: 0.3782\n",
      "batch: 2/17, train_dl_loss: 0.4169, train_bce_loss: 1.7719, train_bce_dl_loss: 0.4169, step time: 0.4414\n",
      "batch: 3/17, train_dl_loss: 0.4282, train_bce_loss: 1.7614, train_bce_dl_loss: 0.4282, step time: 0.3788\n",
      "batch: 4/17, train_dl_loss: 0.3943, train_bce_loss: 1.7659, train_bce_dl_loss: 0.3943, step time: 0.4350\n",
      "batch: 5/17, train_dl_loss: 0.4172, train_bce_loss: 1.7520, train_bce_dl_loss: 0.4172, step time: 0.3809\n",
      "batch: 6/17, train_dl_loss: 0.4560, train_bce_loss: 1.7550, train_bce_dl_loss: 0.4560, step time: 0.4348\n",
      "batch: 7/17, train_dl_loss: 0.3726, train_bce_loss: 1.7477, train_bce_dl_loss: 0.3726, step time: 0.4328\n",
      "batch: 8/17, train_dl_loss: 0.3731, train_bce_loss: 1.7524, train_bce_dl_loss: 0.3731, step time: 0.4348\n",
      "batch: 9/17, train_dl_loss: 0.3735, train_bce_loss: 1.7646, train_bce_dl_loss: 0.3735, step time: 0.4393\n",
      "batch: 10/17, train_dl_loss: 0.4507, train_bce_loss: 1.7570, train_bce_dl_loss: 0.4507, step time: 0.4367\n",
      "batch: 11/17, train_dl_loss: 0.3517, train_bce_loss: 1.7366, train_bce_dl_loss: 0.3517, step time: 0.4370\n",
      "batch: 12/17, train_dl_loss: 0.4337, train_bce_loss: 1.7503, train_bce_dl_loss: 0.4337, step time: 0.4388\n",
      "batch: 13/17, train_dl_loss: 0.4441, train_bce_loss: 1.7560, train_bce_dl_loss: 0.4441, step time: 0.4320\n",
      "batch: 14/17, train_dl_loss: 0.3884, train_bce_loss: 1.7736, train_bce_dl_loss: 0.3884, step time: 0.4373\n",
      "batch: 15/17, train_dl_loss: 0.3970, train_bce_loss: 1.7634, train_bce_dl_loss: 0.3970, step time: 0.4277\n",
      "batch: 16/17, train_dl_loss: 0.3760, train_bce_loss: 1.7567, train_bce_dl_loss: 0.3760, step time: 0.4304\n",
      "batch: 17/17, train_dl_loss: 0.3224, train_bce_loss: 1.7495, train_bce_dl_loss: 0.3224, step time: 0.1149\n",
      "LOSS train DiceLoss: 0.4061, LOSS train BCE: 1.7580, LOSS train BCE-DiceLoss: 0.4061, LOSS val DiceLoss: 0.4982, LOSS val BCE: 1.7524, LOSS val BCE-DiceLoss: 0.4982, METRIC val: 0.5395\n",
      "time consuming of epoch 484 is: 378.1730\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.4091, train_bce_loss: 1.7554, train_bce_dl_loss: 0.4091, step time: 0.4344\n",
      "batch: 1/17, train_dl_loss: 0.4457, train_bce_loss: 1.7801, train_bce_dl_loss: 0.4457, step time: 0.3790\n",
      "batch: 2/17, train_dl_loss: 0.3877, train_bce_loss: 1.7460, train_bce_dl_loss: 0.3877, step time: 0.4286\n",
      "batch: 3/17, train_dl_loss: 0.4122, train_bce_loss: 1.7545, train_bce_dl_loss: 0.4122, step time: 0.3797\n",
      "batch: 4/17, train_dl_loss: 0.3408, train_bce_loss: 1.7384, train_bce_dl_loss: 0.3408, step time: 0.4261\n",
      "batch: 5/17, train_dl_loss: 0.4086, train_bce_loss: 1.7576, train_bce_dl_loss: 0.4086, step time: 0.3822\n",
      "batch: 6/17, train_dl_loss: 0.4715, train_bce_loss: 1.7709, train_bce_dl_loss: 0.4715, step time: 0.4353\n",
      "batch: 7/17, train_dl_loss: 0.3973, train_bce_loss: 1.7403, train_bce_dl_loss: 0.3973, step time: 0.3894\n",
      "batch: 8/17, train_dl_loss: 0.3934, train_bce_loss: 1.7400, train_bce_dl_loss: 0.3934, step time: 0.4414\n",
      "batch: 9/17, train_dl_loss: 0.3700, train_bce_loss: 1.7649, train_bce_dl_loss: 0.3700, step time: 0.3828\n",
      "batch: 10/17, train_dl_loss: 0.4910, train_bce_loss: 1.7585, train_bce_dl_loss: 0.4910, step time: 0.4303\n",
      "batch: 11/17, train_dl_loss: 0.3512, train_bce_loss: 1.7391, train_bce_dl_loss: 0.3512, step time: 0.3849\n",
      "batch: 12/17, train_dl_loss: 0.3773, train_bce_loss: 1.7485, train_bce_dl_loss: 0.3773, step time: 0.4386\n",
      "batch: 13/17, train_dl_loss: 0.4776, train_bce_loss: 1.7687, train_bce_dl_loss: 0.4776, step time: 0.3802\n",
      "batch: 14/17, train_dl_loss: 0.4203, train_bce_loss: 1.7658, train_bce_dl_loss: 0.4203, step time: 0.4410\n",
      "batch: 15/17, train_dl_loss: 0.4379, train_bce_loss: 1.7580, train_bce_dl_loss: 0.4379, step time: 0.3823\n",
      "batch: 16/17, train_dl_loss: 0.3639, train_bce_loss: 1.7710, train_bce_dl_loss: 0.3639, step time: 0.4253\n",
      "batch: 17/17, train_dl_loss: 0.4677, train_bce_loss: 1.7732, train_bce_dl_loss: 0.4677, step time: 0.1150\n",
      "LOSS train DiceLoss: 0.4124, LOSS train BCE: 1.7573, LOSS train BCE-DiceLoss: 0.4124, LOSS val DiceLoss: 0.5117, LOSS val BCE: 1.7500, LOSS val BCE-DiceLoss: 0.5117, METRIC val: 0.5256\n",
      "time consuming of epoch 485 is: 416.7266\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.4331, train_bce_loss: 1.7684, train_bce_dl_loss: 0.4331, step time: 0.4475\n",
      "batch: 1/17, train_dl_loss: 0.4056, train_bce_loss: 1.7517, train_bce_dl_loss: 0.4056, step time: 0.3845\n",
      "batch: 2/17, train_dl_loss: 0.4466, train_bce_loss: 1.7588, train_bce_dl_loss: 0.4466, step time: 0.4262\n",
      "batch: 3/17, train_dl_loss: 0.4333, train_bce_loss: 1.7520, train_bce_dl_loss: 0.4333, step time: 0.3771\n",
      "batch: 4/17, train_dl_loss: 0.3914, train_bce_loss: 1.7524, train_bce_dl_loss: 0.3914, step time: 0.4337\n",
      "batch: 5/17, train_dl_loss: 0.3775, train_bce_loss: 1.7495, train_bce_dl_loss: 0.3775, step time: 0.3799\n",
      "batch: 6/17, train_dl_loss: 0.4199, train_bce_loss: 1.7570, train_bce_dl_loss: 0.4199, step time: 0.4236\n",
      "batch: 7/17, train_dl_loss: 0.3754, train_bce_loss: 1.7302, train_bce_dl_loss: 0.3754, step time: 0.3900\n",
      "batch: 8/17, train_dl_loss: 0.5453, train_bce_loss: 1.7626, train_bce_dl_loss: 0.5453, step time: 0.4214\n",
      "batch: 9/17, train_dl_loss: 0.4931, train_bce_loss: 1.7543, train_bce_dl_loss: 0.4931, step time: 0.4337\n",
      "batch: 10/17, train_dl_loss: 0.5335, train_bce_loss: 1.7696, train_bce_dl_loss: 0.5335, step time: 0.4293\n",
      "batch: 11/17, train_dl_loss: 0.3955, train_bce_loss: 1.7268, train_bce_dl_loss: 0.3955, step time: 0.4234\n",
      "batch: 12/17, train_dl_loss: 0.4555, train_bce_loss: 1.7553, train_bce_dl_loss: 0.4555, step time: 0.4393\n",
      "batch: 13/17, train_dl_loss: 0.4703, train_bce_loss: 1.7580, train_bce_dl_loss: 0.4703, step time: 0.4354\n",
      "batch: 14/17, train_dl_loss: 0.3912, train_bce_loss: 1.7534, train_bce_dl_loss: 0.3912, step time: 0.4428\n",
      "batch: 15/17, train_dl_loss: 0.3816, train_bce_loss: 1.7532, train_bce_dl_loss: 0.3816, step time: 0.3901\n",
      "batch: 16/17, train_dl_loss: 0.3664, train_bce_loss: 1.7723, train_bce_dl_loss: 0.3664, step time: 0.4173\n",
      "batch: 17/17, train_dl_loss: 0.3552, train_bce_loss: 1.7752, train_bce_dl_loss: 0.3552, step time: 0.1125\n",
      "LOSS train DiceLoss: 0.4261, LOSS train BCE: 1.7556, LOSS train BCE-DiceLoss: 0.4261, LOSS val DiceLoss: 0.5183, LOSS val BCE: 1.7505, LOSS val BCE-DiceLoss: 0.5183, METRIC val: 0.5157\n",
      "time consuming of epoch 486 is: 377.6445\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.4790, train_bce_loss: 1.7829, train_bce_dl_loss: 0.4790, step time: 0.4448\n",
      "batch: 1/17, train_dl_loss: 0.3889, train_bce_loss: 1.7636, train_bce_dl_loss: 0.3889, step time: 0.3799\n",
      "batch: 2/17, train_dl_loss: 0.3875, train_bce_loss: 1.7694, train_bce_dl_loss: 0.3875, step time: 0.4384\n",
      "batch: 3/17, train_dl_loss: 0.4705, train_bce_loss: 1.7702, train_bce_dl_loss: 0.4705, step time: 0.3785\n",
      "batch: 4/17, train_dl_loss: 0.3646, train_bce_loss: 1.7348, train_bce_dl_loss: 0.3646, step time: 0.4389\n",
      "batch: 5/17, train_dl_loss: 0.3601, train_bce_loss: 1.7702, train_bce_dl_loss: 0.3601, step time: 0.3856\n",
      "batch: 6/17, train_dl_loss: 0.4454, train_bce_loss: 1.7567, train_bce_dl_loss: 0.4454, step time: 0.4435\n",
      "batch: 7/17, train_dl_loss: 0.3583, train_bce_loss: 1.7340, train_bce_dl_loss: 0.3583, step time: 0.3789\n",
      "batch: 8/17, train_dl_loss: 0.3766, train_bce_loss: 1.7403, train_bce_dl_loss: 0.3766, step time: 0.4290\n",
      "batch: 9/17, train_dl_loss: 0.4063, train_bce_loss: 1.7670, train_bce_dl_loss: 0.4063, step time: 0.3878\n",
      "batch: 10/17, train_dl_loss: 0.4399, train_bce_loss: 1.7742, train_bce_dl_loss: 0.4399, step time: 0.4414\n",
      "batch: 11/17, train_dl_loss: 0.3597, train_bce_loss: 1.7259, train_bce_dl_loss: 0.3597, step time: 0.3812\n",
      "batch: 12/17, train_dl_loss: 0.4056, train_bce_loss: 1.7466, train_bce_dl_loss: 0.4056, step time: 0.4300\n",
      "batch: 13/17, train_dl_loss: 0.4875, train_bce_loss: 1.7675, train_bce_dl_loss: 0.4875, step time: 0.3781\n",
      "batch: 14/17, train_dl_loss: 0.3839, train_bce_loss: 1.7745, train_bce_dl_loss: 0.3839, step time: 0.4231\n",
      "batch: 15/17, train_dl_loss: 0.3590, train_bce_loss: 1.7776, train_bce_dl_loss: 0.3590, step time: 0.3782\n",
      "batch: 16/17, train_dl_loss: 0.3630, train_bce_loss: 1.7596, train_bce_dl_loss: 0.3630, step time: 0.4320\n",
      "batch: 17/17, train_dl_loss: 0.3625, train_bce_loss: 1.7548, train_bce_dl_loss: 0.3625, step time: 0.1121\n",
      "LOSS train DiceLoss: 0.3999, LOSS train BCE: 1.7594, LOSS train BCE-DiceLoss: 0.3999, LOSS val DiceLoss: 0.5048, LOSS val BCE: 1.7564, LOSS val BCE-DiceLoss: 0.5048, METRIC val: 0.5330\n",
      "time consuming of epoch 487 is: 418.8391\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3693, train_bce_loss: 1.7674, train_bce_dl_loss: 0.3693, step time: 0.4500\n",
      "batch: 1/17, train_dl_loss: 0.3664, train_bce_loss: 1.7639, train_bce_dl_loss: 0.3664, step time: 0.4334\n",
      "batch: 2/17, train_dl_loss: 0.3937, train_bce_loss: 1.7656, train_bce_dl_loss: 0.3937, step time: 0.4431\n",
      "batch: 3/17, train_dl_loss: 0.4423, train_bce_loss: 1.7513, train_bce_dl_loss: 0.4423, step time: 0.3774\n",
      "batch: 4/17, train_dl_loss: 0.3892, train_bce_loss: 1.7537, train_bce_dl_loss: 0.3892, step time: 0.4788\n",
      "batch: 5/17, train_dl_loss: 0.4237, train_bce_loss: 1.7582, train_bce_dl_loss: 0.4237, step time: 0.3787\n",
      "batch: 6/17, train_dl_loss: 0.4125, train_bce_loss: 1.7575, train_bce_dl_loss: 0.4125, step time: 0.4386\n",
      "batch: 7/17, train_dl_loss: 0.3641, train_bce_loss: 1.7586, train_bce_dl_loss: 0.3641, step time: 0.3789\n",
      "batch: 8/17, train_dl_loss: 0.3792, train_bce_loss: 1.7322, train_bce_dl_loss: 0.3792, step time: 0.4263\n",
      "batch: 9/17, train_dl_loss: 0.3967, train_bce_loss: 1.7575, train_bce_dl_loss: 0.3967, step time: 0.3803\n",
      "batch: 10/17, train_dl_loss: 0.4791, train_bce_loss: 1.7630, train_bce_dl_loss: 0.4791, step time: 0.4430\n",
      "batch: 11/17, train_dl_loss: 0.3652, train_bce_loss: 1.7552, train_bce_dl_loss: 0.3652, step time: 0.4479\n",
      "batch: 12/17, train_dl_loss: 0.4190, train_bce_loss: 1.7415, train_bce_dl_loss: 0.4190, step time: 0.4397\n",
      "batch: 13/17, train_dl_loss: 0.4828, train_bce_loss: 1.7631, train_bce_dl_loss: 0.4828, step time: 0.3918\n",
      "batch: 14/17, train_dl_loss: 0.4093, train_bce_loss: 1.7574, train_bce_dl_loss: 0.4093, step time: 0.4191\n",
      "batch: 15/17, train_dl_loss: 0.4040, train_bce_loss: 1.7486, train_bce_dl_loss: 0.4040, step time: 0.3773\n",
      "batch: 16/17, train_dl_loss: 0.3959, train_bce_loss: 1.7619, train_bce_dl_loss: 0.3959, step time: 0.4173\n",
      "batch: 17/17, train_dl_loss: 0.3138, train_bce_loss: 1.7727, train_bce_dl_loss: 0.3138, step time: 0.1144\n",
      "LOSS train DiceLoss: 0.4003, LOSS train BCE: 1.7572, LOSS train BCE-DiceLoss: 0.4003, LOSS val DiceLoss: 0.4925, LOSS val BCE: 1.7520, LOSS val BCE-DiceLoss: 0.4925, METRIC val: 0.5472\n",
      "time consuming of epoch 488 is: 391.4987\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3881, train_bce_loss: 1.7557, train_bce_dl_loss: 0.3881, step time: 0.4260\n",
      "batch: 1/17, train_dl_loss: 0.4110, train_bce_loss: 1.7596, train_bce_dl_loss: 0.4110, step time: 0.3864\n",
      "batch: 2/17, train_dl_loss: 0.3823, train_bce_loss: 1.7624, train_bce_dl_loss: 0.3823, step time: 0.4263\n",
      "batch: 3/17, train_dl_loss: 0.4580, train_bce_loss: 1.7531, train_bce_dl_loss: 0.4580, step time: 0.3774\n",
      "batch: 4/17, train_dl_loss: 0.4021, train_bce_loss: 1.7667, train_bce_dl_loss: 0.4021, step time: 0.4318\n",
      "batch: 5/17, train_dl_loss: 0.3549, train_bce_loss: 1.7479, train_bce_dl_loss: 0.3549, step time: 0.3831\n",
      "batch: 6/17, train_dl_loss: 0.4291, train_bce_loss: 1.7676, train_bce_dl_loss: 0.4291, step time: 0.4336\n",
      "batch: 7/17, train_dl_loss: 0.3583, train_bce_loss: 1.7671, train_bce_dl_loss: 0.3583, step time: 0.3863\n",
      "batch: 8/17, train_dl_loss: 0.3692, train_bce_loss: 1.7638, train_bce_dl_loss: 0.3692, step time: 0.4326\n",
      "batch: 9/17, train_dl_loss: 0.3891, train_bce_loss: 1.7687, train_bce_dl_loss: 0.3891, step time: 0.3805\n",
      "batch: 10/17, train_dl_loss: 0.4402, train_bce_loss: 1.7531, train_bce_dl_loss: 0.4402, step time: 0.4419\n",
      "batch: 11/17, train_dl_loss: 0.3652, train_bce_loss: 1.7445, train_bce_dl_loss: 0.3652, step time: 0.3847\n",
      "batch: 12/17, train_dl_loss: 0.3657, train_bce_loss: 1.7422, train_bce_dl_loss: 0.3657, step time: 0.4308\n",
      "batch: 13/17, train_dl_loss: 0.4329, train_bce_loss: 1.7616, train_bce_dl_loss: 0.4329, step time: 0.3982\n",
      "batch: 14/17, train_dl_loss: 0.4040, train_bce_loss: 1.7745, train_bce_dl_loss: 0.4040, step time: 0.4258\n",
      "batch: 15/17, train_dl_loss: 0.3550, train_bce_loss: 1.7512, train_bce_dl_loss: 0.3550, step time: 0.4285\n",
      "batch: 16/17, train_dl_loss: 0.3387, train_bce_loss: 1.7589, train_bce_dl_loss: 0.3387, step time: 0.4200\n",
      "batch: 17/17, train_dl_loss: 0.3113, train_bce_loss: 1.7767, train_bce_dl_loss: 0.3113, step time: 0.1134\n",
      "LOSS train DiceLoss: 0.3864, LOSS train BCE: 1.7597, LOSS train BCE-DiceLoss: 0.3864, LOSS val DiceLoss: 0.4884, LOSS val BCE: 1.7529, LOSS val BCE-DiceLoss: 0.4884, METRIC val: 0.5520\n",
      "time consuming of epoch 489 is: 431.5499\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3717, train_bce_loss: 1.7484, train_bce_dl_loss: 0.3717, step time: 0.4314\n",
      "batch: 1/17, train_dl_loss: 0.3637, train_bce_loss: 1.7632, train_bce_dl_loss: 0.3637, step time: 0.3792\n",
      "batch: 2/17, train_dl_loss: 0.3737, train_bce_loss: 1.7507, train_bce_dl_loss: 0.3737, step time: 0.4302\n",
      "batch: 3/17, train_dl_loss: 0.4054, train_bce_loss: 1.7541, train_bce_dl_loss: 0.4054, step time: 0.3766\n",
      "batch: 4/17, train_dl_loss: 0.3521, train_bce_loss: 1.7577, train_bce_dl_loss: 0.3521, step time: 0.4263\n",
      "batch: 5/17, train_dl_loss: 0.3706, train_bce_loss: 1.7580, train_bce_dl_loss: 0.3706, step time: 0.3838\n",
      "batch: 6/17, train_dl_loss: 0.4069, train_bce_loss: 1.7611, train_bce_dl_loss: 0.4069, step time: 0.4365\n",
      "batch: 7/17, train_dl_loss: 0.3634, train_bce_loss: 1.7373, train_bce_dl_loss: 0.3634, step time: 0.3761\n",
      "batch: 8/17, train_dl_loss: 0.3515, train_bce_loss: 1.7701, train_bce_dl_loss: 0.3515, step time: 0.4371\n",
      "batch: 9/17, train_dl_loss: 0.3931, train_bce_loss: 1.7686, train_bce_dl_loss: 0.3931, step time: 0.3765\n",
      "batch: 10/17, train_dl_loss: 0.4024, train_bce_loss: 1.7564, train_bce_dl_loss: 0.4024, step time: 0.4438\n",
      "batch: 11/17, train_dl_loss: 0.3457, train_bce_loss: 1.7439, train_bce_dl_loss: 0.3457, step time: 0.4290\n",
      "batch: 12/17, train_dl_loss: 0.3669, train_bce_loss: 1.7539, train_bce_dl_loss: 0.3669, step time: 0.4513\n",
      "batch: 13/17, train_dl_loss: 0.4076, train_bce_loss: 1.7702, train_bce_dl_loss: 0.4076, step time: 0.3852\n",
      "batch: 14/17, train_dl_loss: 0.3937, train_bce_loss: 1.7646, train_bce_dl_loss: 0.3937, step time: 0.4188\n",
      "batch: 15/17, train_dl_loss: 0.3452, train_bce_loss: 1.7700, train_bce_dl_loss: 0.3452, step time: 0.3798\n",
      "batch: 16/17, train_dl_loss: 0.3687, train_bce_loss: 1.7754, train_bce_dl_loss: 0.3687, step time: 0.4347\n",
      "batch: 17/17, train_dl_loss: 0.3602, train_bce_loss: 1.7556, train_bce_dl_loss: 0.3602, step time: 0.1133\n",
      "LOSS train DiceLoss: 0.3746, LOSS train BCE: 1.7588, LOSS train BCE-DiceLoss: 0.3746, LOSS val DiceLoss: 0.4867, LOSS val BCE: 1.7550, LOSS val BCE-DiceLoss: 0.4867, METRIC val: 0.5556\n",
      "time consuming of epoch 490 is: 461.9154\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3634, train_bce_loss: 1.7740, train_bce_dl_loss: 0.3634, step time: 0.4409\n",
      "batch: 1/17, train_dl_loss: 0.3744, train_bce_loss: 1.7670, train_bce_dl_loss: 0.3744, step time: 0.3850\n",
      "batch: 2/17, train_dl_loss: 0.3806, train_bce_loss: 1.7651, train_bce_dl_loss: 0.3806, step time: 0.4247\n",
      "batch: 3/17, train_dl_loss: 0.4133, train_bce_loss: 1.7650, train_bce_dl_loss: 0.4133, step time: 0.3844\n",
      "batch: 4/17, train_dl_loss: 0.3956, train_bce_loss: 1.7619, train_bce_dl_loss: 0.3956, step time: 0.4388\n",
      "batch: 5/17, train_dl_loss: 0.3713, train_bce_loss: 1.7719, train_bce_dl_loss: 0.3713, step time: 0.3832\n",
      "batch: 6/17, train_dl_loss: 0.4569, train_bce_loss: 1.7627, train_bce_dl_loss: 0.4569, step time: 0.4407\n",
      "batch: 7/17, train_dl_loss: 0.3813, train_bce_loss: 1.7652, train_bce_dl_loss: 0.3813, step time: 0.3817\n",
      "batch: 8/17, train_dl_loss: 0.3833, train_bce_loss: 1.7532, train_bce_dl_loss: 0.3833, step time: 0.4198\n",
      "batch: 9/17, train_dl_loss: 0.4193, train_bce_loss: 1.7656, train_bce_dl_loss: 0.4193, step time: 0.3829\n",
      "batch: 10/17, train_dl_loss: 0.4267, train_bce_loss: 1.7524, train_bce_dl_loss: 0.4267, step time: 0.4241\n",
      "batch: 11/17, train_dl_loss: 0.3504, train_bce_loss: 1.7233, train_bce_dl_loss: 0.3504, step time: 0.4463\n",
      "batch: 12/17, train_dl_loss: 0.3500, train_bce_loss: 1.7550, train_bce_dl_loss: 0.3500, step time: 0.4447\n",
      "batch: 13/17, train_dl_loss: 0.4306, train_bce_loss: 1.7633, train_bce_dl_loss: 0.4306, step time: 0.3840\n",
      "batch: 14/17, train_dl_loss: 0.3806, train_bce_loss: 1.7670, train_bce_dl_loss: 0.3806, step time: 0.4205\n",
      "batch: 15/17, train_dl_loss: 0.3678, train_bce_loss: 1.7479, train_bce_dl_loss: 0.3678, step time: 0.3796\n",
      "batch: 16/17, train_dl_loss: 0.3655, train_bce_loss: 1.7517, train_bce_dl_loss: 0.3655, step time: 0.4316\n",
      "batch: 17/17, train_dl_loss: 0.3653, train_bce_loss: 1.7517, train_bce_dl_loss: 0.3653, step time: 0.1138\n",
      "LOSS train DiceLoss: 0.3876, LOSS train BCE: 1.7591, LOSS train BCE-DiceLoss: 0.3876, LOSS val DiceLoss: 0.4892, LOSS val BCE: 1.7505, LOSS val BCE-DiceLoss: 0.4892, METRIC val: 0.5525\n",
      "time consuming of epoch 491 is: 433.3291\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3947, train_bce_loss: 1.7533, train_bce_dl_loss: 0.3947, step time: 0.4288\n",
      "batch: 1/17, train_dl_loss: 0.4010, train_bce_loss: 1.7761, train_bce_dl_loss: 0.4010, step time: 0.3796\n",
      "batch: 2/17, train_dl_loss: 0.3502, train_bce_loss: 1.7501, train_bce_dl_loss: 0.3502, step time: 0.5016\n",
      "batch: 3/17, train_dl_loss: 0.4332, train_bce_loss: 1.7461, train_bce_dl_loss: 0.4332, step time: 0.3821\n",
      "batch: 4/17, train_dl_loss: 0.3634, train_bce_loss: 1.7418, train_bce_dl_loss: 0.3634, step time: 0.4426\n",
      "batch: 5/17, train_dl_loss: 0.3606, train_bce_loss: 1.7501, train_bce_dl_loss: 0.3606, step time: 0.3857\n",
      "batch: 6/17, train_dl_loss: 0.4330, train_bce_loss: 1.7648, train_bce_dl_loss: 0.4330, step time: 0.4361\n",
      "batch: 7/17, train_dl_loss: 0.3522, train_bce_loss: 1.7430, train_bce_dl_loss: 0.3522, step time: 0.3811\n",
      "batch: 8/17, train_dl_loss: 0.3666, train_bce_loss: 1.7511, train_bce_dl_loss: 0.3666, step time: 0.4659\n",
      "batch: 9/17, train_dl_loss: 0.3834, train_bce_loss: 1.7626, train_bce_dl_loss: 0.3834, step time: 0.3913\n",
      "batch: 10/17, train_dl_loss: 0.4441, train_bce_loss: 1.7614, train_bce_dl_loss: 0.4441, step time: 0.4605\n",
      "batch: 11/17, train_dl_loss: 0.3731, train_bce_loss: 1.7364, train_bce_dl_loss: 0.3731, step time: 0.4319\n",
      "batch: 12/17, train_dl_loss: 0.3494, train_bce_loss: 1.7420, train_bce_dl_loss: 0.3494, step time: 0.4238\n",
      "batch: 13/17, train_dl_loss: 0.4357, train_bce_loss: 1.7615, train_bce_dl_loss: 0.4357, step time: 0.3797\n",
      "batch: 14/17, train_dl_loss: 0.3726, train_bce_loss: 1.7806, train_bce_dl_loss: 0.3726, step time: 0.4266\n",
      "batch: 15/17, train_dl_loss: 0.3583, train_bce_loss: 1.7745, train_bce_dl_loss: 0.3583, step time: 0.3804\n",
      "batch: 16/17, train_dl_loss: 0.3484, train_bce_loss: 1.7660, train_bce_dl_loss: 0.3484, step time: 0.4170\n",
      "batch: 17/17, train_dl_loss: 0.3581, train_bce_loss: 1.7548, train_bce_dl_loss: 0.3581, step time: 0.1132\n",
      "LOSS train DiceLoss: 0.3821, LOSS train BCE: 1.7564, LOSS train BCE-DiceLoss: 0.3821, LOSS val DiceLoss: 0.4891, LOSS val BCE: 1.7561, LOSS val BCE-DiceLoss: 0.4891, METRIC val: 0.5533\n",
      "time consuming of epoch 492 is: 418.6738\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3419, train_bce_loss: 1.7438, train_bce_dl_loss: 0.3419, step time: 0.4340\n",
      "batch: 1/17, train_dl_loss: 0.3618, train_bce_loss: 1.7625, train_bce_dl_loss: 0.3618, step time: 0.3812\n",
      "batch: 2/17, train_dl_loss: 0.3914, train_bce_loss: 1.7465, train_bce_dl_loss: 0.3914, step time: 0.4208\n",
      "batch: 3/17, train_dl_loss: 0.4182, train_bce_loss: 1.7620, train_bce_dl_loss: 0.4182, step time: 0.3991\n",
      "batch: 4/17, train_dl_loss: 0.3410, train_bce_loss: 1.7584, train_bce_dl_loss: 0.3410, step time: 0.4254\n",
      "batch: 5/17, train_dl_loss: 0.3656, train_bce_loss: 1.7451, train_bce_dl_loss: 0.3656, step time: 0.3821\n",
      "batch: 6/17, train_dl_loss: 0.4061, train_bce_loss: 1.7677, train_bce_dl_loss: 0.4061, step time: 0.4338\n",
      "batch: 7/17, train_dl_loss: 0.3402, train_bce_loss: 1.7446, train_bce_dl_loss: 0.3402, step time: 0.4444\n",
      "batch: 8/17, train_dl_loss: 0.3596, train_bce_loss: 1.7553, train_bce_dl_loss: 0.3596, step time: 0.4401\n",
      "batch: 9/17, train_dl_loss: 0.3684, train_bce_loss: 1.7715, train_bce_dl_loss: 0.3684, step time: 0.4333\n",
      "batch: 10/17, train_dl_loss: 0.4068, train_bce_loss: 1.7655, train_bce_dl_loss: 0.4068, step time: 0.4321\n",
      "batch: 11/17, train_dl_loss: 0.3272, train_bce_loss: 1.7260, train_bce_dl_loss: 0.3272, step time: 0.4233\n",
      "batch: 12/17, train_dl_loss: 0.3733, train_bce_loss: 1.7640, train_bce_dl_loss: 0.3733, step time: 0.4229\n",
      "batch: 13/17, train_dl_loss: 0.4339, train_bce_loss: 1.7645, train_bce_dl_loss: 0.4339, step time: 0.4278\n",
      "batch: 14/17, train_dl_loss: 0.4333, train_bce_loss: 1.7664, train_bce_dl_loss: 0.4333, step time: 0.4188\n",
      "batch: 15/17, train_dl_loss: 0.3544, train_bce_loss: 1.7639, train_bce_dl_loss: 0.3544, step time: 0.4246\n",
      "batch: 16/17, train_dl_loss: 0.3340, train_bce_loss: 1.7691, train_bce_dl_loss: 0.3340, step time: 0.4187\n",
      "batch: 17/17, train_dl_loss: 0.4651, train_bce_loss: 1.7795, train_bce_dl_loss: 0.4651, step time: 0.1134\n",
      "LOSS train DiceLoss: 0.3790, LOSS train BCE: 1.7587, LOSS train BCE-DiceLoss: 0.3790, LOSS val DiceLoss: 0.4855, LOSS val BCE: 1.7528, LOSS val BCE-DiceLoss: 0.4855, METRIC val: 0.5569\n",
      "time consuming of epoch 493 is: 408.2409\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3435, train_bce_loss: 1.7473, train_bce_dl_loss: 0.3435, step time: 0.4282\n",
      "batch: 1/17, train_dl_loss: 0.3710, train_bce_loss: 1.7659, train_bce_dl_loss: 0.3710, step time: 0.3764\n",
      "batch: 2/17, train_dl_loss: 0.3774, train_bce_loss: 1.7477, train_bce_dl_loss: 0.3774, step time: 0.4324\n",
      "batch: 3/17, train_dl_loss: 0.4257, train_bce_loss: 1.7476, train_bce_dl_loss: 0.4257, step time: 0.3750\n",
      "batch: 4/17, train_dl_loss: 0.3355, train_bce_loss: 1.7514, train_bce_dl_loss: 0.3355, step time: 0.4340\n",
      "batch: 5/17, train_dl_loss: 0.3371, train_bce_loss: 1.7491, train_bce_dl_loss: 0.3371, step time: 0.3827\n",
      "batch: 6/17, train_dl_loss: 0.3928, train_bce_loss: 1.7650, train_bce_dl_loss: 0.3928, step time: 0.4257\n",
      "batch: 7/17, train_dl_loss: 0.3412, train_bce_loss: 1.7417, train_bce_dl_loss: 0.3412, step time: 0.3866\n",
      "batch: 8/17, train_dl_loss: 0.3331, train_bce_loss: 1.7580, train_bce_dl_loss: 0.3331, step time: 0.4407\n",
      "batch: 9/17, train_dl_loss: 0.3947, train_bce_loss: 1.7579, train_bce_dl_loss: 0.3947, step time: 0.3796\n",
      "batch: 10/17, train_dl_loss: 0.4000, train_bce_loss: 1.7637, train_bce_dl_loss: 0.4000, step time: 0.4405\n",
      "batch: 11/17, train_dl_loss: 0.3504, train_bce_loss: 1.7451, train_bce_dl_loss: 0.3504, step time: 0.3902\n",
      "batch: 12/17, train_dl_loss: 0.3728, train_bce_loss: 1.7350, train_bce_dl_loss: 0.3728, step time: 0.4305\n",
      "batch: 13/17, train_dl_loss: 0.4526, train_bce_loss: 1.7606, train_bce_dl_loss: 0.4526, step time: 0.3855\n",
      "batch: 14/17, train_dl_loss: 0.3527, train_bce_loss: 1.7728, train_bce_dl_loss: 0.3527, step time: 0.4317\n",
      "batch: 15/17, train_dl_loss: 0.3400, train_bce_loss: 1.7542, train_bce_dl_loss: 0.3400, step time: 0.3787\n",
      "batch: 16/17, train_dl_loss: 0.3628, train_bce_loss: 1.7632, train_bce_dl_loss: 0.3628, step time: 0.4241\n",
      "batch: 17/17, train_dl_loss: 0.3303, train_bce_loss: 1.7735, train_bce_dl_loss: 0.3303, step time: 0.1120\n",
      "LOSS train DiceLoss: 0.3674, LOSS train BCE: 1.7555, LOSS train BCE-DiceLoss: 0.3674, LOSS val DiceLoss: 0.4993, LOSS val BCE: 1.7513, LOSS val BCE-DiceLoss: 0.4993, METRIC val: 0.5388\n",
      "time consuming of epoch 494 is: 407.6383\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3647, train_bce_loss: 1.7490, train_bce_dl_loss: 0.3647, step time: 0.4235\n",
      "batch: 1/17, train_dl_loss: 0.3799, train_bce_loss: 1.7696, train_bce_dl_loss: 0.3799, step time: 0.3787\n",
      "batch: 2/17, train_dl_loss: 0.3805, train_bce_loss: 1.7619, train_bce_dl_loss: 0.3805, step time: 0.4215\n",
      "batch: 3/17, train_dl_loss: 0.4832, train_bce_loss: 1.7674, train_bce_dl_loss: 0.4832, step time: 0.3743\n",
      "batch: 4/17, train_dl_loss: 0.3513, train_bce_loss: 1.7524, train_bce_dl_loss: 0.3513, step time: 0.4309\n",
      "batch: 5/17, train_dl_loss: 0.3543, train_bce_loss: 1.7671, train_bce_dl_loss: 0.3543, step time: 0.3951\n",
      "batch: 6/17, train_dl_loss: 0.4102, train_bce_loss: 1.7732, train_bce_dl_loss: 0.4102, step time: 0.4459\n",
      "batch: 7/17, train_dl_loss: 0.3690, train_bce_loss: 1.7470, train_bce_dl_loss: 0.3690, step time: 0.3987\n",
      "batch: 8/17, train_dl_loss: 0.3641, train_bce_loss: 1.7372, train_bce_dl_loss: 0.3641, step time: 0.4284\n",
      "batch: 9/17, train_dl_loss: 0.4348, train_bce_loss: 1.7749, train_bce_dl_loss: 0.4348, step time: 0.3815\n",
      "batch: 10/17, train_dl_loss: 0.4784, train_bce_loss: 1.7546, train_bce_dl_loss: 0.4784, step time: 0.4293\n",
      "batch: 11/17, train_dl_loss: 0.3510, train_bce_loss: 1.7373, train_bce_dl_loss: 0.3510, step time: 0.4472\n",
      "batch: 12/17, train_dl_loss: 0.3544, train_bce_loss: 1.7532, train_bce_dl_loss: 0.3544, step time: 0.4224\n",
      "batch: 13/17, train_dl_loss: 0.4236, train_bce_loss: 1.7608, train_bce_dl_loss: 0.4236, step time: 0.3831\n",
      "batch: 14/17, train_dl_loss: 0.3622, train_bce_loss: 1.7672, train_bce_dl_loss: 0.3622, step time: 0.4242\n",
      "batch: 15/17, train_dl_loss: 0.3618, train_bce_loss: 1.7685, train_bce_dl_loss: 0.3618, step time: 0.3828\n",
      "batch: 16/17, train_dl_loss: 0.3422, train_bce_loss: 1.7602, train_bce_dl_loss: 0.3422, step time: 0.4164\n",
      "batch: 17/17, train_dl_loss: 0.3251, train_bce_loss: 1.7569, train_bce_dl_loss: 0.3251, step time: 0.1129\n",
      "LOSS train DiceLoss: 0.3828, LOSS train BCE: 1.7588, LOSS train BCE-DiceLoss: 0.3828, LOSS val DiceLoss: 0.4882, LOSS val BCE: 1.7549, LOSS val BCE-DiceLoss: 0.4882, METRIC val: 0.5531\n",
      "time consuming of epoch 495 is: 424.7155\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3648, train_bce_loss: 1.7437, train_bce_dl_loss: 0.3648, step time: 0.4368\n",
      "batch: 1/17, train_dl_loss: 0.3586, train_bce_loss: 1.7667, train_bce_dl_loss: 0.3586, step time: 0.3803\n",
      "batch: 2/17, train_dl_loss: 0.3628, train_bce_loss: 1.7481, train_bce_dl_loss: 0.3628, step time: 0.4293\n",
      "batch: 3/17, train_dl_loss: 0.4094, train_bce_loss: 1.7549, train_bce_dl_loss: 0.4094, step time: 0.3793\n",
      "batch: 4/17, train_dl_loss: 0.3459, train_bce_loss: 1.7523, train_bce_dl_loss: 0.3459, step time: 0.4350\n",
      "batch: 5/17, train_dl_loss: 0.3327, train_bce_loss: 1.7553, train_bce_dl_loss: 0.3327, step time: 0.3852\n",
      "batch: 6/17, train_dl_loss: 0.3909, train_bce_loss: 1.7601, train_bce_dl_loss: 0.3909, step time: 0.4334\n",
      "batch: 7/17, train_dl_loss: 0.3574, train_bce_loss: 1.7576, train_bce_dl_loss: 0.3574, step time: 0.3968\n",
      "batch: 8/17, train_dl_loss: 0.3505, train_bce_loss: 1.7624, train_bce_dl_loss: 0.3505, step time: 0.4307\n",
      "batch: 9/17, train_dl_loss: 0.4086, train_bce_loss: 1.7777, train_bce_dl_loss: 0.4086, step time: 0.4431\n",
      "batch: 10/17, train_dl_loss: 0.3898, train_bce_loss: 1.7632, train_bce_dl_loss: 0.3898, step time: 0.4342\n",
      "batch: 11/17, train_dl_loss: 0.3625, train_bce_loss: 1.7528, train_bce_dl_loss: 0.3625, step time: 0.4312\n",
      "batch: 12/17, train_dl_loss: 0.3568, train_bce_loss: 1.7545, train_bce_dl_loss: 0.3568, step time: 0.4326\n",
      "batch: 13/17, train_dl_loss: 0.4572, train_bce_loss: 1.7630, train_bce_dl_loss: 0.4572, step time: 0.4427\n",
      "batch: 14/17, train_dl_loss: 0.3653, train_bce_loss: 1.7726, train_bce_dl_loss: 0.3653, step time: 0.4447\n",
      "batch: 15/17, train_dl_loss: 0.3410, train_bce_loss: 1.7625, train_bce_dl_loss: 0.3410, step time: 0.3858\n",
      "batch: 16/17, train_dl_loss: 0.3262, train_bce_loss: 1.7698, train_bce_dl_loss: 0.3262, step time: 0.4162\n",
      "batch: 17/17, train_dl_loss: 0.3090, train_bce_loss: 1.7796, train_bce_dl_loss: 0.3090, step time: 0.1125\n",
      "LOSS train DiceLoss: 0.3661, LOSS train BCE: 1.7609, LOSS train BCE-DiceLoss: 0.3661, LOSS val DiceLoss: 0.4970, LOSS val BCE: 1.7532, LOSS val BCE-DiceLoss: 0.4970, METRIC val: 0.5411\n",
      "time consuming of epoch 496 is: 426.6906\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3498, train_bce_loss: 1.7453, train_bce_dl_loss: 0.3498, step time: 0.4443\n",
      "batch: 1/17, train_dl_loss: 0.4342, train_bce_loss: 1.7576, train_bce_dl_loss: 0.4342, step time: 0.3836\n",
      "batch: 2/17, train_dl_loss: 0.3475, train_bce_loss: 1.7667, train_bce_dl_loss: 0.3475, step time: 0.4269\n",
      "batch: 3/17, train_dl_loss: 0.3918, train_bce_loss: 1.7589, train_bce_dl_loss: 0.3918, step time: 0.3765\n",
      "batch: 4/17, train_dl_loss: 0.3656, train_bce_loss: 1.7703, train_bce_dl_loss: 0.3656, step time: 0.4450\n",
      "batch: 5/17, train_dl_loss: 0.3362, train_bce_loss: 1.7529, train_bce_dl_loss: 0.3362, step time: 0.4192\n",
      "batch: 6/17, train_dl_loss: 0.4466, train_bce_loss: 1.7742, train_bce_dl_loss: 0.4466, step time: 0.4318\n",
      "batch: 7/17, train_dl_loss: 0.3720, train_bce_loss: 1.7609, train_bce_dl_loss: 0.3720, step time: 0.3863\n",
      "batch: 8/17, train_dl_loss: 0.3503, train_bce_loss: 1.7420, train_bce_dl_loss: 0.3503, step time: 0.4400\n",
      "batch: 9/17, train_dl_loss: 0.3798, train_bce_loss: 1.7548, train_bce_dl_loss: 0.3798, step time: 0.3778\n",
      "batch: 10/17, train_dl_loss: 0.4077, train_bce_loss: 1.7665, train_bce_dl_loss: 0.4077, step time: 0.4263\n",
      "batch: 11/17, train_dl_loss: 0.3493, train_bce_loss: 1.7364, train_bce_dl_loss: 0.3493, step time: 0.3869\n",
      "batch: 12/17, train_dl_loss: 0.3810, train_bce_loss: 1.7516, train_bce_dl_loss: 0.3810, step time: 0.4270\n",
      "batch: 13/17, train_dl_loss: 0.4438, train_bce_loss: 1.7645, train_bce_dl_loss: 0.4438, step time: 0.3744\n",
      "batch: 14/17, train_dl_loss: 0.3910, train_bce_loss: 1.7674, train_bce_dl_loss: 0.3910, step time: 0.4485\n",
      "batch: 15/17, train_dl_loss: 0.3409, train_bce_loss: 1.7685, train_bce_dl_loss: 0.3409, step time: 0.3848\n",
      "batch: 16/17, train_dl_loss: 0.3500, train_bce_loss: 1.7550, train_bce_dl_loss: 0.3500, step time: 0.4188\n",
      "batch: 17/17, train_dl_loss: 0.3184, train_bce_loss: 1.7756, train_bce_dl_loss: 0.3184, step time: 0.1132\n",
      "LOSS train DiceLoss: 0.3753, LOSS train BCE: 1.7594, LOSS train BCE-DiceLoss: 0.3753, LOSS val DiceLoss: 0.4887, LOSS val BCE: 1.7487, LOSS val BCE-DiceLoss: 0.4887, METRIC val: 0.5524\n",
      "time consuming of epoch 497 is: 434.6138\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3555, train_bce_loss: 1.7400, train_bce_dl_loss: 0.3555, step time: 0.4314\n",
      "batch: 1/17, train_dl_loss: 0.3568, train_bce_loss: 1.7736, train_bce_dl_loss: 0.3568, step time: 0.3823\n",
      "batch: 2/17, train_dl_loss: 0.4010, train_bce_loss: 1.7511, train_bce_dl_loss: 0.4010, step time: 0.4345\n",
      "batch: 3/17, train_dl_loss: 0.4057, train_bce_loss: 1.7572, train_bce_dl_loss: 0.4057, step time: 0.3820\n",
      "batch: 4/17, train_dl_loss: 0.3616, train_bce_loss: 1.7749, train_bce_dl_loss: 0.3616, step time: 0.4265\n",
      "batch: 5/17, train_dl_loss: 0.3594, train_bce_loss: 1.7381, train_bce_dl_loss: 0.3594, step time: 0.3843\n",
      "batch: 6/17, train_dl_loss: 0.3995, train_bce_loss: 1.7686, train_bce_dl_loss: 0.3995, step time: 0.4267\n",
      "batch: 7/17, train_dl_loss: 0.3326, train_bce_loss: 1.7548, train_bce_dl_loss: 0.3326, step time: 0.3795\n",
      "batch: 8/17, train_dl_loss: 0.3768, train_bce_loss: 1.7473, train_bce_dl_loss: 0.3768, step time: 0.4329\n",
      "batch: 9/17, train_dl_loss: 0.3527, train_bce_loss: 1.7709, train_bce_dl_loss: 0.3527, step time: 0.3867\n",
      "batch: 10/17, train_dl_loss: 0.4044, train_bce_loss: 1.7555, train_bce_dl_loss: 0.4044, step time: 0.4345\n",
      "batch: 11/17, train_dl_loss: 0.4004, train_bce_loss: 1.7442, train_bce_dl_loss: 0.4004, step time: 0.4411\n",
      "batch: 12/17, train_dl_loss: 0.3620, train_bce_loss: 1.7514, train_bce_dl_loss: 0.3620, step time: 0.4256\n",
      "batch: 13/17, train_dl_loss: 0.4333, train_bce_loss: 1.7627, train_bce_dl_loss: 0.4333, step time: 0.3975\n",
      "batch: 14/17, train_dl_loss: 0.3692, train_bce_loss: 1.7708, train_bce_dl_loss: 0.3692, step time: 0.4262\n",
      "batch: 15/17, train_dl_loss: 0.3759, train_bce_loss: 1.7617, train_bce_dl_loss: 0.3759, step time: 0.3747\n",
      "batch: 16/17, train_dl_loss: 0.3395, train_bce_loss: 1.7645, train_bce_dl_loss: 0.3395, step time: 0.4128\n",
      "batch: 17/17, train_dl_loss: 0.3626, train_bce_loss: 1.7553, train_bce_dl_loss: 0.3626, step time: 0.1132\n",
      "LOSS train DiceLoss: 0.3750, LOSS train BCE: 1.7579, LOSS train BCE-DiceLoss: 0.3750, LOSS val DiceLoss: 0.4934, LOSS val BCE: 1.7550, LOSS val BCE-DiceLoss: 0.4934, METRIC val: 0.5473\n",
      "time consuming of epoch 498 is: 421.7928\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3416, train_bce_loss: 1.7780, train_bce_dl_loss: 0.3416, step time: 0.4354\n",
      "batch: 1/17, train_dl_loss: 0.3507, train_bce_loss: 1.7768, train_bce_dl_loss: 0.3507, step time: 0.3854\n",
      "batch: 2/17, train_dl_loss: 0.3476, train_bce_loss: 1.7574, train_bce_dl_loss: 0.3476, step time: 0.4448\n",
      "batch: 3/17, train_dl_loss: 0.4127, train_bce_loss: 1.7470, train_bce_dl_loss: 0.4127, step time: 0.3894\n",
      "batch: 4/17, train_dl_loss: 0.3511, train_bce_loss: 1.7700, train_bce_dl_loss: 0.3511, step time: 0.4346\n",
      "batch: 5/17, train_dl_loss: 0.3532, train_bce_loss: 1.7541, train_bce_dl_loss: 0.3532, step time: 0.3784\n",
      "batch: 6/17, train_dl_loss: 0.4171, train_bce_loss: 1.7650, train_bce_dl_loss: 0.4171, step time: 0.4348\n",
      "batch: 7/17, train_dl_loss: 0.3455, train_bce_loss: 1.7493, train_bce_dl_loss: 0.3455, step time: 0.3808\n",
      "batch: 8/17, train_dl_loss: 0.3622, train_bce_loss: 1.7375, train_bce_dl_loss: 0.3622, step time: 0.4236\n",
      "batch: 9/17, train_dl_loss: 0.3705, train_bce_loss: 1.7596, train_bce_dl_loss: 0.3705, step time: 0.3828\n",
      "batch: 10/17, train_dl_loss: 0.4479, train_bce_loss: 1.7620, train_bce_dl_loss: 0.4479, step time: 0.4313\n",
      "batch: 11/17, train_dl_loss: 0.3376, train_bce_loss: 1.7354, train_bce_dl_loss: 0.3376, step time: 0.4422\n",
      "batch: 12/17, train_dl_loss: 0.4341, train_bce_loss: 1.7497, train_bce_dl_loss: 0.4341, step time: 0.4390\n",
      "batch: 13/17, train_dl_loss: 0.4310, train_bce_loss: 1.7602, train_bce_dl_loss: 0.4310, step time: 0.4341\n",
      "batch: 14/17, train_dl_loss: 0.3812, train_bce_loss: 1.7631, train_bce_dl_loss: 0.3812, step time: 0.4386\n",
      "batch: 15/17, train_dl_loss: 0.3457, train_bce_loss: 1.7669, train_bce_dl_loss: 0.3457, step time: 0.3851\n",
      "batch: 16/17, train_dl_loss: 0.3479, train_bce_loss: 1.7633, train_bce_dl_loss: 0.3479, step time: 0.4123\n",
      "batch: 17/17, train_dl_loss: 0.3148, train_bce_loss: 1.7500, train_bce_dl_loss: 0.3148, step time: 0.1152\n",
      "LOSS train DiceLoss: 0.3718, LOSS train BCE: 1.7581, LOSS train BCE-DiceLoss: 0.3718, LOSS val DiceLoss: 0.5015, LOSS val BCE: 1.7519, LOSS val BCE-DiceLoss: 0.5015, METRIC val: 0.5350\n",
      "time consuming of epoch 499 is: 502.1423\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.4337, train_bce_loss: 1.7687, train_bce_dl_loss: 0.4337, step time: 0.4356\n",
      "batch: 1/17, train_dl_loss: 0.3568, train_bce_loss: 1.7592, train_bce_dl_loss: 0.3568, step time: 0.3829\n",
      "batch: 2/17, train_dl_loss: 0.3531, train_bce_loss: 1.7596, train_bce_dl_loss: 0.3531, step time: 0.4346\n",
      "batch: 3/17, train_dl_loss: 0.4207, train_bce_loss: 1.7603, train_bce_dl_loss: 0.4207, step time: 0.3795\n",
      "batch: 4/17, train_dl_loss: 0.3487, train_bce_loss: 1.7470, train_bce_dl_loss: 0.3487, step time: 0.4255\n",
      "batch: 5/17, train_dl_loss: 0.3624, train_bce_loss: 1.7429, train_bce_dl_loss: 0.3624, step time: 0.3885\n",
      "batch: 6/17, train_dl_loss: 0.4147, train_bce_loss: 1.7580, train_bce_dl_loss: 0.4147, step time: 0.4304\n",
      "batch: 7/17, train_dl_loss: 0.3514, train_bce_loss: 1.7337, train_bce_dl_loss: 0.3514, step time: 0.3858\n",
      "batch: 8/17, train_dl_loss: 0.3539, train_bce_loss: 1.7539, train_bce_dl_loss: 0.3539, step time: 0.4278\n",
      "batch: 9/17, train_dl_loss: 0.3758, train_bce_loss: 1.7642, train_bce_dl_loss: 0.3758, step time: 0.3794\n",
      "batch: 10/17, train_dl_loss: 0.4419, train_bce_loss: 1.7531, train_bce_dl_loss: 0.4419, step time: 0.4302\n",
      "batch: 11/17, train_dl_loss: 0.3566, train_bce_loss: 1.7339, train_bce_dl_loss: 0.3566, step time: 0.3862\n",
      "batch: 12/17, train_dl_loss: 0.3754, train_bce_loss: 1.7409, train_bce_dl_loss: 0.3754, step time: 0.4262\n",
      "batch: 13/17, train_dl_loss: 0.4477, train_bce_loss: 1.7624, train_bce_dl_loss: 0.4477, step time: 0.3821\n",
      "batch: 14/17, train_dl_loss: 0.3820, train_bce_loss: 1.7592, train_bce_dl_loss: 0.3820, step time: 0.4386\n",
      "batch: 15/17, train_dl_loss: 0.3537, train_bce_loss: 1.7609, train_bce_dl_loss: 0.3537, step time: 0.3783\n",
      "batch: 16/17, train_dl_loss: 0.3408, train_bce_loss: 1.7700, train_bce_dl_loss: 0.3408, step time: 0.4294\n",
      "batch: 17/17, train_dl_loss: 0.3528, train_bce_loss: 1.7718, train_bce_dl_loss: 0.3528, step time: 0.1131\n",
      "LOSS train DiceLoss: 0.3790, LOSS train BCE: 1.7555, LOSS train BCE-DiceLoss: 0.3790, LOSS val DiceLoss: 0.4889, LOSS val BCE: 1.7544, LOSS val BCE-DiceLoss: 0.4889, METRIC val: 0.5512\n",
      "time consuming of epoch 500 is: 442.2910\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3497, train_bce_loss: 1.7507, train_bce_dl_loss: 0.3497, step time: 0.4423\n",
      "batch: 1/17, train_dl_loss: 0.3539, train_bce_loss: 1.7780, train_bce_dl_loss: 0.3539, step time: 0.3780\n",
      "batch: 2/17, train_dl_loss: 0.3638, train_bce_loss: 1.7534, train_bce_dl_loss: 0.3638, step time: 0.4348\n",
      "batch: 3/17, train_dl_loss: 0.4095, train_bce_loss: 1.7568, train_bce_dl_loss: 0.4095, step time: 0.3816\n",
      "batch: 4/17, train_dl_loss: 0.3595, train_bce_loss: 1.7429, train_bce_dl_loss: 0.3595, step time: 0.4342\n",
      "batch: 5/17, train_dl_loss: 0.3367, train_bce_loss: 1.7489, train_bce_dl_loss: 0.3367, step time: 0.4451\n",
      "batch: 6/17, train_dl_loss: 0.3891, train_bce_loss: 1.7716, train_bce_dl_loss: 0.3891, step time: 0.4535\n",
      "batch: 7/17, train_dl_loss: 0.3400, train_bce_loss: 1.7472, train_bce_dl_loss: 0.3400, step time: 0.4307\n",
      "batch: 8/17, train_dl_loss: 0.3603, train_bce_loss: 1.7599, train_bce_dl_loss: 0.3603, step time: 0.4332\n",
      "batch: 9/17, train_dl_loss: 0.3700, train_bce_loss: 1.7539, train_bce_dl_loss: 0.3700, step time: 0.3911\n",
      "batch: 10/17, train_dl_loss: 0.3996, train_bce_loss: 1.7684, train_bce_dl_loss: 0.3996, step time: 0.4400\n",
      "batch: 11/17, train_dl_loss: 0.3525, train_bce_loss: 1.7452, train_bce_dl_loss: 0.3525, step time: 0.4404\n",
      "batch: 12/17, train_dl_loss: 0.3446, train_bce_loss: 1.7390, train_bce_dl_loss: 0.3446, step time: 0.4344\n",
      "batch: 13/17, train_dl_loss: 0.4711, train_bce_loss: 1.7715, train_bce_dl_loss: 0.4711, step time: 0.3849\n",
      "batch: 14/17, train_dl_loss: 0.3514, train_bce_loss: 1.7741, train_bce_dl_loss: 0.3514, step time: 0.4265\n",
      "batch: 15/17, train_dl_loss: 0.3682, train_bce_loss: 1.7638, train_bce_dl_loss: 0.3682, step time: 0.3855\n",
      "batch: 16/17, train_dl_loss: 0.3407, train_bce_loss: 1.7583, train_bce_dl_loss: 0.3407, step time: 0.4302\n",
      "batch: 17/17, train_dl_loss: 0.3577, train_bce_loss: 1.7558, train_bce_dl_loss: 0.3577, step time: 0.1136\n",
      "LOSS train DiceLoss: 0.3677, LOSS train BCE: 1.7578, LOSS train BCE-DiceLoss: 0.3677, LOSS val DiceLoss: 0.4921, LOSS val BCE: 1.7555, LOSS val BCE-DiceLoss: 0.4921, METRIC val: 0.5484\n",
      "time consuming of epoch 501 is: 432.7732\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3634, train_bce_loss: 1.7400, train_bce_dl_loss: 0.3634, step time: 0.4232\n",
      "batch: 1/17, train_dl_loss: 0.3841, train_bce_loss: 1.7702, train_bce_dl_loss: 0.3841, step time: 0.3804\n",
      "batch: 2/17, train_dl_loss: 0.3705, train_bce_loss: 1.7740, train_bce_dl_loss: 0.3705, step time: 0.4436\n",
      "batch: 3/17, train_dl_loss: 0.4553, train_bce_loss: 1.7662, train_bce_dl_loss: 0.4553, step time: 0.3856\n",
      "batch: 4/17, train_dl_loss: 0.3541, train_bce_loss: 1.7579, train_bce_dl_loss: 0.3541, step time: 0.4201\n",
      "batch: 5/17, train_dl_loss: 0.3377, train_bce_loss: 1.7640, train_bce_dl_loss: 0.3377, step time: 0.4239\n",
      "batch: 6/17, train_dl_loss: 0.4150, train_bce_loss: 1.7749, train_bce_dl_loss: 0.4150, step time: 0.4262\n",
      "batch: 7/17, train_dl_loss: 0.3235, train_bce_loss: 1.7411, train_bce_dl_loss: 0.3235, step time: 0.3818\n",
      "batch: 8/17, train_dl_loss: 0.3665, train_bce_loss: 1.7344, train_bce_dl_loss: 0.3665, step time: 0.4372\n",
      "batch: 9/17, train_dl_loss: 0.3515, train_bce_loss: 1.7624, train_bce_dl_loss: 0.3515, step time: 0.4406\n",
      "batch: 10/17, train_dl_loss: 0.4659, train_bce_loss: 1.7726, train_bce_dl_loss: 0.4659, step time: 0.4273\n",
      "batch: 11/17, train_dl_loss: 0.3817, train_bce_loss: 1.7328, train_bce_dl_loss: 0.3817, step time: 0.4321\n",
      "batch: 12/17, train_dl_loss: 0.3638, train_bce_loss: 1.7767, train_bce_dl_loss: 0.3638, step time: 0.4280\n",
      "batch: 13/17, train_dl_loss: 0.4196, train_bce_loss: 1.7648, train_bce_dl_loss: 0.4196, step time: 0.3798\n",
      "batch: 14/17, train_dl_loss: 0.3709, train_bce_loss: 1.7768, train_bce_dl_loss: 0.3709, step time: 0.4397\n",
      "batch: 15/17, train_dl_loss: 0.3721, train_bce_loss: 1.7489, train_bce_dl_loss: 0.3721, step time: 0.3762\n",
      "batch: 16/17, train_dl_loss: 0.3712, train_bce_loss: 1.7763, train_bce_dl_loss: 0.3712, step time: 0.4283\n",
      "batch: 17/17, train_dl_loss: 0.3248, train_bce_loss: 1.7744, train_bce_dl_loss: 0.3248, step time: 0.1142\n",
      "LOSS train DiceLoss: 0.3773, LOSS train BCE: 1.7616, LOSS train BCE-DiceLoss: 0.3773, LOSS val DiceLoss: 0.4855, LOSS val BCE: 1.7518, LOSS val BCE-DiceLoss: 0.4855, METRIC val: 0.5553\n",
      "time consuming of epoch 502 is: 393.0723\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3633, train_bce_loss: 1.7678, train_bce_dl_loss: 0.3633, step time: 0.4404\n",
      "batch: 1/17, train_dl_loss: 0.3646, train_bce_loss: 1.7589, train_bce_dl_loss: 0.3646, step time: 0.3868\n",
      "batch: 2/17, train_dl_loss: 0.4014, train_bce_loss: 1.7644, train_bce_dl_loss: 0.4014, step time: 0.4352\n",
      "batch: 3/17, train_dl_loss: 0.4079, train_bce_loss: 1.7669, train_bce_dl_loss: 0.4079, step time: 0.3810\n",
      "batch: 4/17, train_dl_loss: 0.3402, train_bce_loss: 1.7699, train_bce_dl_loss: 0.3402, step time: 0.4348\n",
      "batch: 5/17, train_dl_loss: 0.3438, train_bce_loss: 1.7638, train_bce_dl_loss: 0.3438, step time: 0.3837\n",
      "batch: 6/17, train_dl_loss: 0.4033, train_bce_loss: 1.7656, train_bce_dl_loss: 0.4033, step time: 0.4274\n",
      "batch: 7/17, train_dl_loss: 0.3652, train_bce_loss: 1.7400, train_bce_dl_loss: 0.3652, step time: 0.3804\n",
      "batch: 8/17, train_dl_loss: 0.3747, train_bce_loss: 1.7303, train_bce_dl_loss: 0.3747, step time: 0.4373\n",
      "batch: 9/17, train_dl_loss: 0.3678, train_bce_loss: 1.7774, train_bce_dl_loss: 0.3678, step time: 0.3862\n",
      "batch: 10/17, train_dl_loss: 0.3969, train_bce_loss: 1.7579, train_bce_dl_loss: 0.3969, step time: 0.4215\n",
      "batch: 11/17, train_dl_loss: 0.3578, train_bce_loss: 1.7404, train_bce_dl_loss: 0.3578, step time: 0.4637\n",
      "batch: 12/17, train_dl_loss: 0.3355, train_bce_loss: 1.7411, train_bce_dl_loss: 0.3355, step time: 0.4317\n",
      "batch: 13/17, train_dl_loss: 0.4430, train_bce_loss: 1.7638, train_bce_dl_loss: 0.4430, step time: 0.3902\n",
      "batch: 14/17, train_dl_loss: 0.3947, train_bce_loss: 1.7639, train_bce_dl_loss: 0.3947, step time: 0.4249\n",
      "batch: 15/17, train_dl_loss: 0.3503, train_bce_loss: 1.7536, train_bce_dl_loss: 0.3503, step time: 0.3819\n",
      "batch: 16/17, train_dl_loss: 0.3432, train_bce_loss: 1.7689, train_bce_dl_loss: 0.3432, step time: 0.4269\n",
      "batch: 17/17, train_dl_loss: 0.3161, train_bce_loss: 1.7699, train_bce_dl_loss: 0.3161, step time: 0.1141\n",
      "LOSS train DiceLoss: 0.3705, LOSS train BCE: 1.7591, LOSS train BCE-DiceLoss: 0.3705, LOSS val DiceLoss: 0.4905, LOSS val BCE: 1.7530, LOSS val BCE-DiceLoss: 0.4905, METRIC val: 0.5509\n",
      "time consuming of epoch 503 is: 450.1468\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3472, train_bce_loss: 1.7491, train_bce_dl_loss: 0.3472, step time: 0.4274\n",
      "batch: 1/17, train_dl_loss: 0.3655, train_bce_loss: 1.7588, train_bce_dl_loss: 0.3655, step time: 0.4324\n",
      "batch: 2/17, train_dl_loss: 0.3952, train_bce_loss: 1.7707, train_bce_dl_loss: 0.3952, step time: 0.4327\n",
      "batch: 3/17, train_dl_loss: 0.4082, train_bce_loss: 1.7644, train_bce_dl_loss: 0.4082, step time: 0.3792\n",
      "batch: 4/17, train_dl_loss: 0.3212, train_bce_loss: 1.7632, train_bce_dl_loss: 0.3212, step time: 0.4255\n",
      "batch: 5/17, train_dl_loss: 0.3264, train_bce_loss: 1.7405, train_bce_dl_loss: 0.3264, step time: 0.3876\n",
      "batch: 6/17, train_dl_loss: 0.4148, train_bce_loss: 1.7801, train_bce_dl_loss: 0.4148, step time: 0.4278\n",
      "batch: 7/17, train_dl_loss: 0.3486, train_bce_loss: 1.7745, train_bce_dl_loss: 0.3486, step time: 0.3855\n",
      "batch: 8/17, train_dl_loss: 0.3732, train_bce_loss: 1.7509, train_bce_dl_loss: 0.3732, step time: 0.4262\n",
      "batch: 9/17, train_dl_loss: 0.3961, train_bce_loss: 1.7671, train_bce_dl_loss: 0.3961, step time: 0.3841\n",
      "batch: 10/17, train_dl_loss: 0.3862, train_bce_loss: 1.7485, train_bce_dl_loss: 0.3862, step time: 0.4306\n",
      "batch: 11/17, train_dl_loss: 0.3397, train_bce_loss: 1.7556, train_bce_dl_loss: 0.3397, step time: 0.4410\n",
      "batch: 12/17, train_dl_loss: 0.3637, train_bce_loss: 1.7605, train_bce_dl_loss: 0.3637, step time: 0.4281\n",
      "batch: 13/17, train_dl_loss: 0.4168, train_bce_loss: 1.7603, train_bce_dl_loss: 0.4168, step time: 0.3777\n",
      "batch: 14/17, train_dl_loss: 0.3894, train_bce_loss: 1.7669, train_bce_dl_loss: 0.3894, step time: 0.4257\n",
      "batch: 15/17, train_dl_loss: 0.3496, train_bce_loss: 1.7643, train_bce_dl_loss: 0.3496, step time: 0.3822\n",
      "batch: 16/17, train_dl_loss: 0.3593, train_bce_loss: 1.7613, train_bce_dl_loss: 0.3593, step time: 0.4385\n",
      "batch: 17/17, train_dl_loss: 0.3595, train_bce_loss: 1.7553, train_bce_dl_loss: 0.3595, step time: 0.1119\n",
      "LOSS train DiceLoss: 0.3700, LOSS train BCE: 1.7607, LOSS train BCE-DiceLoss: 0.3700, LOSS val DiceLoss: 0.4811, LOSS val BCE: 1.7553, LOSS val BCE-DiceLoss: 0.4811, METRIC val: 0.5618\n",
      "time consuming of epoch 504 is: 440.7169\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3730, train_bce_loss: 1.7595, train_bce_dl_loss: 0.3730, step time: 0.4361\n",
      "batch: 1/17, train_dl_loss: 0.3986, train_bce_loss: 1.7728, train_bce_dl_loss: 0.3986, step time: 0.3744\n",
      "batch: 2/17, train_dl_loss: 0.3613, train_bce_loss: 1.7612, train_bce_dl_loss: 0.3613, step time: 0.4381\n",
      "batch: 3/17, train_dl_loss: 0.4245, train_bce_loss: 1.7803, train_bce_dl_loss: 0.4245, step time: 0.3831\n",
      "batch: 4/17, train_dl_loss: 0.3579, train_bce_loss: 1.7742, train_bce_dl_loss: 0.3579, step time: 0.4306\n",
      "batch: 5/17, train_dl_loss: 0.3481, train_bce_loss: 1.7616, train_bce_dl_loss: 0.3481, step time: 0.3819\n",
      "batch: 6/17, train_dl_loss: 0.4095, train_bce_loss: 1.7775, train_bce_dl_loss: 0.4095, step time: 0.4270\n",
      "batch: 7/17, train_dl_loss: 0.3252, train_bce_loss: 1.7459, train_bce_dl_loss: 0.3252, step time: 0.3761\n",
      "batch: 8/17, train_dl_loss: 0.3536, train_bce_loss: 1.7445, train_bce_dl_loss: 0.3536, step time: 0.4433\n",
      "batch: 9/17, train_dl_loss: 0.3769, train_bce_loss: 1.7765, train_bce_dl_loss: 0.3769, step time: 0.3801\n",
      "batch: 10/17, train_dl_loss: 0.4105, train_bce_loss: 1.7576, train_bce_dl_loss: 0.4105, step time: 0.4255\n",
      "batch: 11/17, train_dl_loss: 0.3451, train_bce_loss: 1.7507, train_bce_dl_loss: 0.3451, step time: 0.4430\n",
      "batch: 12/17, train_dl_loss: 0.3658, train_bce_loss: 1.7526, train_bce_dl_loss: 0.3658, step time: 0.4228\n",
      "batch: 13/17, train_dl_loss: 0.4403, train_bce_loss: 1.7783, train_bce_dl_loss: 0.4403, step time: 0.3797\n",
      "batch: 14/17, train_dl_loss: 0.3574, train_bce_loss: 1.7821, train_bce_dl_loss: 0.3574, step time: 0.4278\n",
      "batch: 15/17, train_dl_loss: 0.3394, train_bce_loss: 1.7544, train_bce_dl_loss: 0.3394, step time: 0.3841\n",
      "batch: 16/17, train_dl_loss: 0.3511, train_bce_loss: 1.7717, train_bce_dl_loss: 0.3511, step time: 0.4286\n",
      "batch: 17/17, train_dl_loss: 0.3562, train_bce_loss: 1.7586, train_bce_dl_loss: 0.3562, step time: 0.1117\n",
      "LOSS train DiceLoss: 0.3719, LOSS train BCE: 1.7644, LOSS train BCE-DiceLoss: 0.3719, LOSS val DiceLoss: 0.4865, LOSS val BCE: 1.7562, LOSS val BCE-DiceLoss: 0.4865, METRIC val: 0.5556\n",
      "time consuming of epoch 505 is: 453.6310\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3723, train_bce_loss: 1.7475, train_bce_dl_loss: 0.3723, step time: 0.4395\n",
      "batch: 1/17, train_dl_loss: 0.3561, train_bce_loss: 1.7711, train_bce_dl_loss: 0.3561, step time: 0.3795\n",
      "batch: 2/17, train_dl_loss: 0.3601, train_bce_loss: 1.7701, train_bce_dl_loss: 0.3601, step time: 0.4320\n",
      "batch: 3/17, train_dl_loss: 0.3869, train_bce_loss: 1.7603, train_bce_dl_loss: 0.3869, step time: 0.3743\n",
      "batch: 4/17, train_dl_loss: 0.3356, train_bce_loss: 1.7510, train_bce_dl_loss: 0.3356, step time: 0.4474\n",
      "batch: 5/17, train_dl_loss: 0.3315, train_bce_loss: 1.7791, train_bce_dl_loss: 0.3315, step time: 0.4303\n",
      "batch: 6/17, train_dl_loss: 0.3785, train_bce_loss: 1.7802, train_bce_dl_loss: 0.3785, step time: 0.4443\n",
      "batch: 7/17, train_dl_loss: 0.3485, train_bce_loss: 1.7390, train_bce_dl_loss: 0.3485, step time: 0.4430\n",
      "batch: 8/17, train_dl_loss: 0.3312, train_bce_loss: 1.7534, train_bce_dl_loss: 0.3312, step time: 0.4358\n",
      "batch: 9/17, train_dl_loss: 0.3545, train_bce_loss: 1.7703, train_bce_dl_loss: 0.3545, step time: 0.4334\n",
      "batch: 10/17, train_dl_loss: 0.4590, train_bce_loss: 1.7595, train_bce_dl_loss: 0.4590, step time: 0.4202\n",
      "batch: 11/17, train_dl_loss: 0.3496, train_bce_loss: 1.7607, train_bce_dl_loss: 0.3496, step time: 0.5006\n",
      "batch: 12/17, train_dl_loss: 0.4033, train_bce_loss: 1.7864, train_bce_dl_loss: 0.4033, step time: 0.4238\n",
      "batch: 13/17, train_dl_loss: 0.4635, train_bce_loss: 1.7656, train_bce_dl_loss: 0.4635, step time: 0.3866\n",
      "batch: 14/17, train_dl_loss: 0.3530, train_bce_loss: 1.7683, train_bce_dl_loss: 0.3530, step time: 0.4356\n",
      "batch: 15/17, train_dl_loss: 0.3462, train_bce_loss: 1.7731, train_bce_dl_loss: 0.3462, step time: 0.3918\n",
      "batch: 16/17, train_dl_loss: 0.3426, train_bce_loss: 1.7746, train_bce_dl_loss: 0.3426, step time: 0.4176\n",
      "batch: 17/17, train_dl_loss: 0.3107, train_bce_loss: 1.7585, train_bce_dl_loss: 0.3107, step time: 0.1127\n",
      "LOSS train DiceLoss: 0.3657, LOSS train BCE: 1.7649, LOSS train BCE-DiceLoss: 0.3657, LOSS val DiceLoss: 0.4879, LOSS val BCE: 1.7583, LOSS val BCE-DiceLoss: 0.4879, METRIC val: 0.5552\n",
      "time consuming of epoch 506 is: 448.5664\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3674, train_bce_loss: 1.7601, train_bce_dl_loss: 0.3674, step time: 0.4333\n",
      "batch: 1/17, train_dl_loss: 0.3631, train_bce_loss: 1.7705, train_bce_dl_loss: 0.3631, step time: 0.4423\n",
      "batch: 2/17, train_dl_loss: 0.3549, train_bce_loss: 1.7648, train_bce_dl_loss: 0.3549, step time: 0.4262\n",
      "batch: 3/17, train_dl_loss: 0.4015, train_bce_loss: 1.7699, train_bce_dl_loss: 0.4015, step time: 0.3796\n",
      "batch: 4/17, train_dl_loss: 0.3537, train_bce_loss: 1.7642, train_bce_dl_loss: 0.3537, step time: 0.4299\n",
      "batch: 5/17, train_dl_loss: 0.3492, train_bce_loss: 1.7587, train_bce_dl_loss: 0.3492, step time: 0.3915\n",
      "batch: 6/17, train_dl_loss: 0.4270, train_bce_loss: 1.7588, train_bce_dl_loss: 0.4270, step time: 0.4360\n",
      "batch: 7/17, train_dl_loss: 0.3315, train_bce_loss: 1.7730, train_bce_dl_loss: 0.3315, step time: 0.3887\n",
      "batch: 8/17, train_dl_loss: 0.3793, train_bce_loss: 1.7550, train_bce_dl_loss: 0.3793, step time: 0.4264\n",
      "batch: 9/17, train_dl_loss: 0.3687, train_bce_loss: 1.7733, train_bce_dl_loss: 0.3687, step time: 0.4401\n",
      "batch: 10/17, train_dl_loss: 0.4080, train_bce_loss: 1.7758, train_bce_dl_loss: 0.4080, step time: 0.4393\n",
      "batch: 11/17, train_dl_loss: 0.3817, train_bce_loss: 1.7626, train_bce_dl_loss: 0.3817, step time: 0.4483\n",
      "batch: 12/17, train_dl_loss: 0.3407, train_bce_loss: 1.7672, train_bce_dl_loss: 0.3407, step time: 0.4367\n",
      "batch: 13/17, train_dl_loss: 0.4083, train_bce_loss: 1.7769, train_bce_dl_loss: 0.4083, step time: 0.3799\n",
      "batch: 14/17, train_dl_loss: 0.3981, train_bce_loss: 1.7720, train_bce_dl_loss: 0.3981, step time: 0.4228\n",
      "batch: 15/17, train_dl_loss: 0.3607, train_bce_loss: 1.7759, train_bce_dl_loss: 0.3607, step time: 0.3818\n",
      "batch: 16/17, train_dl_loss: 0.3447, train_bce_loss: 1.7654, train_bce_dl_loss: 0.3447, step time: 0.4165\n",
      "batch: 17/17, train_dl_loss: 0.3577, train_bce_loss: 1.7590, train_bce_dl_loss: 0.3577, step time: 0.1138\n",
      "LOSS train DiceLoss: 0.3720, LOSS train BCE: 1.7668, LOSS train BCE-DiceLoss: 0.3720, LOSS val DiceLoss: 0.4828, LOSS val BCE: 1.7578, LOSS val BCE-DiceLoss: 0.4828, METRIC val: 0.5598\n",
      "time consuming of epoch 507 is: 438.9791\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3931, train_bce_loss: 1.7659, train_bce_dl_loss: 0.3931, step time: 0.4358\n",
      "batch: 1/17, train_dl_loss: 0.3299, train_bce_loss: 1.7728, train_bce_dl_loss: 0.3299, step time: 0.3781\n",
      "batch: 2/17, train_dl_loss: 0.3640, train_bce_loss: 1.7599, train_bce_dl_loss: 0.3640, step time: 0.4493\n",
      "batch: 3/17, train_dl_loss: 0.4074, train_bce_loss: 1.7699, train_bce_dl_loss: 0.4074, step time: 0.3861\n",
      "batch: 4/17, train_dl_loss: 0.3388, train_bce_loss: 1.7506, train_bce_dl_loss: 0.3388, step time: 0.4315\n",
      "batch: 5/17, train_dl_loss: 0.3484, train_bce_loss: 1.7609, train_bce_dl_loss: 0.3484, step time: 0.3881\n",
      "batch: 6/17, train_dl_loss: 0.4020, train_bce_loss: 1.7661, train_bce_dl_loss: 0.4020, step time: 0.4417\n",
      "batch: 7/17, train_dl_loss: 0.3492, train_bce_loss: 1.7734, train_bce_dl_loss: 0.3492, step time: 0.3840\n",
      "batch: 8/17, train_dl_loss: 0.3395, train_bce_loss: 1.7523, train_bce_dl_loss: 0.3395, step time: 0.4380\n",
      "batch: 9/17, train_dl_loss: 0.3503, train_bce_loss: 1.7567, train_bce_dl_loss: 0.3503, step time: 0.3814\n",
      "batch: 10/17, train_dl_loss: 0.4018, train_bce_loss: 1.7747, train_bce_dl_loss: 0.4018, step time: 0.4294\n",
      "batch: 11/17, train_dl_loss: 0.3601, train_bce_loss: 1.7260, train_bce_dl_loss: 0.3601, step time: 0.4509\n",
      "batch: 12/17, train_dl_loss: 0.3715, train_bce_loss: 1.7466, train_bce_dl_loss: 0.3715, step time: 0.4357\n",
      "batch: 13/17, train_dl_loss: 0.4561, train_bce_loss: 1.7682, train_bce_dl_loss: 0.4561, step time: 0.3819\n",
      "batch: 14/17, train_dl_loss: 0.3831, train_bce_loss: 1.7717, train_bce_dl_loss: 0.3831, step time: 0.4250\n",
      "batch: 15/17, train_dl_loss: 0.3597, train_bce_loss: 1.7565, train_bce_dl_loss: 0.3597, step time: 0.3801\n",
      "batch: 16/17, train_dl_loss: 0.3378, train_bce_loss: 1.7682, train_bce_dl_loss: 0.3378, step time: 0.4153\n",
      "batch: 17/17, train_dl_loss: 0.3202, train_bce_loss: 1.7774, train_bce_dl_loss: 0.3202, step time: 0.1141\n",
      "LOSS train DiceLoss: 0.3674, LOSS train BCE: 1.7621, LOSS train BCE-DiceLoss: 0.3674, LOSS val DiceLoss: 0.4851, LOSS val BCE: 1.7564, LOSS val BCE-DiceLoss: 0.4851, METRIC val: 0.5573\n",
      "time consuming of epoch 508 is: 510.9344\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3458, train_bce_loss: 1.7564, train_bce_dl_loss: 0.3458, step time: 0.4322\n",
      "batch: 1/17, train_dl_loss: 0.3466, train_bce_loss: 1.7582, train_bce_dl_loss: 0.3466, step time: 0.3859\n",
      "batch: 2/17, train_dl_loss: 0.3664, train_bce_loss: 1.7632, train_bce_dl_loss: 0.3664, step time: 0.4186\n",
      "batch: 3/17, train_dl_loss: 0.4047, train_bce_loss: 1.7792, train_bce_dl_loss: 0.4047, step time: 0.3769\n",
      "batch: 4/17, train_dl_loss: 0.3502, train_bce_loss: 1.7695, train_bce_dl_loss: 0.3502, step time: 0.4425\n",
      "batch: 5/17, train_dl_loss: 0.3394, train_bce_loss: 1.7604, train_bce_dl_loss: 0.3394, step time: 0.3893\n",
      "batch: 6/17, train_dl_loss: 0.4074, train_bce_loss: 1.7670, train_bce_dl_loss: 0.4074, step time: 0.4299\n",
      "batch: 7/17, train_dl_loss: 0.3481, train_bce_loss: 1.7524, train_bce_dl_loss: 0.3481, step time: 0.3765\n",
      "batch: 8/17, train_dl_loss: 0.3587, train_bce_loss: 1.7302, train_bce_dl_loss: 0.3587, step time: 0.4267\n",
      "batch: 9/17, train_dl_loss: 0.3515, train_bce_loss: 1.7582, train_bce_dl_loss: 0.3515, step time: 0.4295\n",
      "batch: 10/17, train_dl_loss: 0.3922, train_bce_loss: 1.7517, train_bce_dl_loss: 0.3922, step time: 0.4344\n",
      "batch: 11/17, train_dl_loss: 0.3570, train_bce_loss: 1.7549, train_bce_dl_loss: 0.3570, step time: 0.4252\n",
      "batch: 12/17, train_dl_loss: 0.3417, train_bce_loss: 1.7498, train_bce_dl_loss: 0.3417, step time: 0.4205\n",
      "batch: 13/17, train_dl_loss: 0.4175, train_bce_loss: 1.7598, train_bce_dl_loss: 0.4175, step time: 0.3896\n",
      "batch: 14/17, train_dl_loss: 0.3822, train_bce_loss: 1.7677, train_bce_dl_loss: 0.3822, step time: 0.4253\n",
      "batch: 15/17, train_dl_loss: 0.3390, train_bce_loss: 1.7613, train_bce_dl_loss: 0.3390, step time: 0.3775\n",
      "batch: 16/17, train_dl_loss: 0.3424, train_bce_loss: 1.7792, train_bce_dl_loss: 0.3424, step time: 0.4181\n",
      "batch: 17/17, train_dl_loss: 0.3037, train_bce_loss: 1.7763, train_bce_dl_loss: 0.3037, step time: 0.1125\n",
      "LOSS train DiceLoss: 0.3608, LOSS train BCE: 1.7609, LOSS train BCE-DiceLoss: 0.3608, LOSS val DiceLoss: 0.4836, LOSS val BCE: 1.7570, LOSS val BCE-DiceLoss: 0.4836, METRIC val: 0.5586\n",
      "time consuming of epoch 509 is: 439.3920\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3803, train_bce_loss: 1.7507, train_bce_dl_loss: 0.3803, step time: 0.4299\n",
      "batch: 1/17, train_dl_loss: 0.3612, train_bce_loss: 1.7537, train_bce_dl_loss: 0.3612, step time: 0.3820\n",
      "batch: 2/17, train_dl_loss: 0.3801, train_bce_loss: 1.7627, train_bce_dl_loss: 0.3801, step time: 0.4385\n",
      "batch: 3/17, train_dl_loss: 0.3942, train_bce_loss: 1.7445, train_bce_dl_loss: 0.3942, step time: 0.3780\n",
      "batch: 4/17, train_dl_loss: 0.3456, train_bce_loss: 1.7452, train_bce_dl_loss: 0.3456, step time: 0.4354\n",
      "batch: 5/17, train_dl_loss: 0.3323, train_bce_loss: 1.7757, train_bce_dl_loss: 0.3323, step time: 0.4478\n",
      "batch: 6/17, train_dl_loss: 0.4135, train_bce_loss: 1.7718, train_bce_dl_loss: 0.4135, step time: 0.4382\n",
      "batch: 7/17, train_dl_loss: 0.3520, train_bce_loss: 1.7603, train_bce_dl_loss: 0.3520, step time: 0.4565\n",
      "batch: 8/17, train_dl_loss: 0.3402, train_bce_loss: 1.7666, train_bce_dl_loss: 0.3402, step time: 0.4412\n",
      "batch: 9/17, train_dl_loss: 0.3385, train_bce_loss: 1.7818, train_bce_dl_loss: 0.3385, step time: 0.4289\n",
      "batch: 10/17, train_dl_loss: 0.4085, train_bce_loss: 1.7635, train_bce_dl_loss: 0.4085, step time: 0.4404\n",
      "batch: 11/17, train_dl_loss: 0.3440, train_bce_loss: 1.7673, train_bce_dl_loss: 0.3440, step time: 0.4189\n",
      "batch: 12/17, train_dl_loss: 0.3322, train_bce_loss: 1.7660, train_bce_dl_loss: 0.3322, step time: 0.4366\n",
      "batch: 13/17, train_dl_loss: 0.4134, train_bce_loss: 1.7758, train_bce_dl_loss: 0.4134, step time: 0.4352\n",
      "batch: 14/17, train_dl_loss: 0.3807, train_bce_loss: 1.7751, train_bce_dl_loss: 0.3807, step time: 0.4487\n",
      "batch: 15/17, train_dl_loss: 0.3460, train_bce_loss: 1.7690, train_bce_dl_loss: 0.3460, step time: 0.4386\n",
      "batch: 16/17, train_dl_loss: 0.3569, train_bce_loss: 1.7653, train_bce_dl_loss: 0.3569, step time: 0.4230\n",
      "batch: 17/17, train_dl_loss: 0.3136, train_bce_loss: 1.7620, train_bce_dl_loss: 0.3136, step time: 0.1163\n",
      "LOSS train DiceLoss: 0.3629, LOSS train BCE: 1.7643, LOSS train BCE-DiceLoss: 0.3629, LOSS val DiceLoss: 0.4887, LOSS val BCE: 1.7611, LOSS val BCE-DiceLoss: 0.4887, METRIC val: 0.5528\n",
      "time consuming of epoch 510 is: 418.2718\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3449, train_bce_loss: 1.7538, train_bce_dl_loss: 0.3449, step time: 0.4702\n",
      "batch: 1/17, train_dl_loss: 0.3826, train_bce_loss: 1.7627, train_bce_dl_loss: 0.3826, step time: 0.3862\n",
      "batch: 2/17, train_dl_loss: 0.3541, train_bce_loss: 1.7539, train_bce_dl_loss: 0.3541, step time: 0.4397\n",
      "batch: 3/17, train_dl_loss: 0.3915, train_bce_loss: 1.7722, train_bce_dl_loss: 0.3915, step time: 0.3783\n",
      "batch: 4/17, train_dl_loss: 0.3500, train_bce_loss: 1.7482, train_bce_dl_loss: 0.3500, step time: 0.4515\n",
      "batch: 5/17, train_dl_loss: 0.3531, train_bce_loss: 1.7558, train_bce_dl_loss: 0.3531, step time: 0.3894\n",
      "batch: 6/17, train_dl_loss: 0.4263, train_bce_loss: 1.7821, train_bce_dl_loss: 0.4263, step time: 0.4344\n",
      "batch: 7/17, train_dl_loss: 0.3366, train_bce_loss: 1.7538, train_bce_dl_loss: 0.3366, step time: 0.3866\n",
      "batch: 8/17, train_dl_loss: 0.3533, train_bce_loss: 1.7460, train_bce_dl_loss: 0.3533, step time: 0.4232\n",
      "batch: 9/17, train_dl_loss: 0.3670, train_bce_loss: 1.7758, train_bce_dl_loss: 0.3670, step time: 0.3827\n",
      "batch: 10/17, train_dl_loss: 0.3878, train_bce_loss: 1.7505, train_bce_dl_loss: 0.3878, step time: 0.4490\n",
      "batch: 11/17, train_dl_loss: 0.3489, train_bce_loss: 1.7409, train_bce_dl_loss: 0.3489, step time: 0.4473\n",
      "batch: 12/17, train_dl_loss: 0.3304, train_bce_loss: 1.7555, train_bce_dl_loss: 0.3304, step time: 0.4366\n",
      "batch: 13/17, train_dl_loss: 0.4303, train_bce_loss: 1.7629, train_bce_dl_loss: 0.4303, step time: 0.3904\n",
      "batch: 14/17, train_dl_loss: 0.3553, train_bce_loss: 1.7885, train_bce_dl_loss: 0.3553, step time: 0.4231\n",
      "batch: 15/17, train_dl_loss: 0.3369, train_bce_loss: 1.7636, train_bce_dl_loss: 0.3369, step time: 0.3845\n",
      "batch: 16/17, train_dl_loss: 0.3445, train_bce_loss: 1.7688, train_bce_dl_loss: 0.3445, step time: 0.4151\n",
      "batch: 17/17, train_dl_loss: 0.3191, train_bce_loss: 1.7785, train_bce_dl_loss: 0.3191, step time: 0.1124\n",
      "LOSS train DiceLoss: 0.3618, LOSS train BCE: 1.7619, LOSS train BCE-DiceLoss: 0.3618, LOSS val DiceLoss: 0.4809, LOSS val BCE: 1.7566, LOSS val BCE-DiceLoss: 0.4809, METRIC val: 0.5611\n",
      "time consuming of epoch 511 is: 405.6444\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3576, train_bce_loss: 1.7444, train_bce_dl_loss: 0.3576, step time: 0.4542\n",
      "batch: 1/17, train_dl_loss: 0.3418, train_bce_loss: 1.7843, train_bce_dl_loss: 0.3418, step time: 0.3804\n",
      "batch: 2/17, train_dl_loss: 0.3617, train_bce_loss: 1.7622, train_bce_dl_loss: 0.3617, step time: 0.4377\n",
      "batch: 3/17, train_dl_loss: 0.4066, train_bce_loss: 1.7770, train_bce_dl_loss: 0.4066, step time: 0.3823\n",
      "batch: 4/17, train_dl_loss: 0.3482, train_bce_loss: 1.7772, train_bce_dl_loss: 0.3482, step time: 0.4380\n",
      "batch: 5/17, train_dl_loss: 0.3526, train_bce_loss: 1.7616, train_bce_dl_loss: 0.3526, step time: 0.4447\n",
      "batch: 6/17, train_dl_loss: 0.4028, train_bce_loss: 1.7649, train_bce_dl_loss: 0.4028, step time: 0.4409\n",
      "batch: 7/17, train_dl_loss: 0.3382, train_bce_loss: 1.7610, train_bce_dl_loss: 0.3382, step time: 0.4753\n",
      "batch: 8/17, train_dl_loss: 0.3392, train_bce_loss: 1.7556, train_bce_dl_loss: 0.3392, step time: 0.4405\n",
      "batch: 9/17, train_dl_loss: 0.4578, train_bce_loss: 1.7755, train_bce_dl_loss: 0.4578, step time: 0.4431\n",
      "batch: 10/17, train_dl_loss: 0.3807, train_bce_loss: 1.7571, train_bce_dl_loss: 0.3807, step time: 0.4234\n",
      "batch: 11/17, train_dl_loss: 0.3314, train_bce_loss: 1.7567, train_bce_dl_loss: 0.3314, step time: 0.4320\n",
      "batch: 12/17, train_dl_loss: 0.3478, train_bce_loss: 1.7580, train_bce_dl_loss: 0.3478, step time: 0.4299\n",
      "batch: 13/17, train_dl_loss: 0.4201, train_bce_loss: 1.7717, train_bce_dl_loss: 0.4201, step time: 0.4399\n",
      "batch: 14/17, train_dl_loss: 0.3964, train_bce_loss: 1.7683, train_bce_dl_loss: 0.3964, step time: 0.4353\n",
      "batch: 15/17, train_dl_loss: 0.3566, train_bce_loss: 1.7593, train_bce_dl_loss: 0.3566, step time: 0.4381\n",
      "batch: 16/17, train_dl_loss: 0.3672, train_bce_loss: 1.7830, train_bce_dl_loss: 0.3672, step time: 0.4222\n",
      "batch: 17/17, train_dl_loss: 0.3085, train_bce_loss: 1.7745, train_bce_dl_loss: 0.3085, step time: 0.1125\n",
      "LOSS train DiceLoss: 0.3675, LOSS train BCE: 1.7662, LOSS train BCE-DiceLoss: 0.3675, LOSS val DiceLoss: 0.4872, LOSS val BCE: 1.7603, LOSS val BCE-DiceLoss: 0.4872, METRIC val: 0.5545\n",
      "time consuming of epoch 512 is: 424.0039\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3434, train_bce_loss: 1.7569, train_bce_dl_loss: 0.3434, step time: 0.4383\n",
      "batch: 1/17, train_dl_loss: 0.3447, train_bce_loss: 1.7666, train_bce_dl_loss: 0.3447, step time: 0.3853\n",
      "batch: 2/17, train_dl_loss: 0.3533, train_bce_loss: 1.7622, train_bce_dl_loss: 0.3533, step time: 0.4267\n",
      "batch: 3/17, train_dl_loss: 0.4742, train_bce_loss: 1.7564, train_bce_dl_loss: 0.4742, step time: 0.3791\n",
      "batch: 4/17, train_dl_loss: 0.3361, train_bce_loss: 1.7729, train_bce_dl_loss: 0.3361, step time: 0.5152\n",
      "batch: 5/17, train_dl_loss: 0.3416, train_bce_loss: 1.7738, train_bce_dl_loss: 0.3416, step time: 0.4432\n",
      "batch: 6/17, train_dl_loss: 0.4268, train_bce_loss: 1.7698, train_bce_dl_loss: 0.4268, step time: 0.4539\n",
      "batch: 7/17, train_dl_loss: 0.3375, train_bce_loss: 1.7477, train_bce_dl_loss: 0.3375, step time: 0.4385\n",
      "batch: 8/17, train_dl_loss: 0.3431, train_bce_loss: 1.7463, train_bce_dl_loss: 0.3431, step time: 0.4350\n",
      "batch: 9/17, train_dl_loss: 0.3519, train_bce_loss: 1.7625, train_bce_dl_loss: 0.3519, step time: 0.4276\n",
      "batch: 10/17, train_dl_loss: 0.3947, train_bce_loss: 1.7544, train_bce_dl_loss: 0.3947, step time: 0.4441\n",
      "batch: 11/17, train_dl_loss: 0.3307, train_bce_loss: 1.7584, train_bce_dl_loss: 0.3307, step time: 0.4412\n",
      "batch: 12/17, train_dl_loss: 0.3327, train_bce_loss: 1.7459, train_bce_dl_loss: 0.3327, step time: 0.4338\n",
      "batch: 13/17, train_dl_loss: 0.4169, train_bce_loss: 1.7712, train_bce_dl_loss: 0.4169, step time: 0.4239\n",
      "batch: 14/17, train_dl_loss: 0.3920, train_bce_loss: 1.7840, train_bce_dl_loss: 0.3920, step time: 0.4405\n",
      "batch: 15/17, train_dl_loss: 0.3521, train_bce_loss: 1.7834, train_bce_dl_loss: 0.3521, step time: 0.4389\n",
      "batch: 16/17, train_dl_loss: 0.3378, train_bce_loss: 1.7711, train_bce_dl_loss: 0.3378, step time: 0.4357\n",
      "batch: 17/17, train_dl_loss: 0.3105, train_bce_loss: 1.7561, train_bce_dl_loss: 0.3105, step time: 0.1132\n",
      "LOSS train DiceLoss: 0.3622, LOSS train BCE: 1.7633, LOSS train BCE-DiceLoss: 0.3622, LOSS val DiceLoss: 0.4841, LOSS val BCE: 1.7556, LOSS val BCE-DiceLoss: 0.4841, METRIC val: 0.5568\n",
      "time consuming of epoch 513 is: 419.9056\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3572, train_bce_loss: 1.7678, train_bce_dl_loss: 0.3572, step time: 0.4444\n",
      "batch: 1/17, train_dl_loss: 0.3579, train_bce_loss: 1.7683, train_bce_dl_loss: 0.3579, step time: 0.3802\n",
      "batch: 2/17, train_dl_loss: 0.3560, train_bce_loss: 1.7659, train_bce_dl_loss: 0.3560, step time: 0.4431\n",
      "batch: 3/17, train_dl_loss: 0.4250, train_bce_loss: 1.7722, train_bce_dl_loss: 0.4250, step time: 0.3886\n",
      "batch: 4/17, train_dl_loss: 0.3383, train_bce_loss: 1.7584, train_bce_dl_loss: 0.3383, step time: 0.4328\n",
      "batch: 5/17, train_dl_loss: 0.3450, train_bce_loss: 1.7604, train_bce_dl_loss: 0.3450, step time: 0.3816\n",
      "batch: 6/17, train_dl_loss: 0.3936, train_bce_loss: 1.7572, train_bce_dl_loss: 0.3936, step time: 0.4534\n",
      "batch: 7/17, train_dl_loss: 0.3561, train_bce_loss: 1.7656, train_bce_dl_loss: 0.3561, step time: 0.3971\n",
      "batch: 8/17, train_dl_loss: 0.3610, train_bce_loss: 1.7583, train_bce_dl_loss: 0.3610, step time: 0.4366\n",
      "batch: 9/17, train_dl_loss: 0.3485, train_bce_loss: 1.7621, train_bce_dl_loss: 0.3485, step time: 0.3823\n",
      "batch: 10/17, train_dl_loss: 0.3797, train_bce_loss: 1.7567, train_bce_dl_loss: 0.3797, step time: 0.5135\n",
      "batch: 11/17, train_dl_loss: 0.3317, train_bce_loss: 1.7593, train_bce_dl_loss: 0.3317, step time: 0.4243\n",
      "batch: 12/17, train_dl_loss: 0.3376, train_bce_loss: 1.7558, train_bce_dl_loss: 0.3376, step time: 0.4287\n",
      "batch: 13/17, train_dl_loss: 0.4270, train_bce_loss: 1.7770, train_bce_dl_loss: 0.4270, step time: 0.3951\n",
      "batch: 14/17, train_dl_loss: 0.3714, train_bce_loss: 1.7870, train_bce_dl_loss: 0.3714, step time: 0.4331\n",
      "batch: 15/17, train_dl_loss: 0.3522, train_bce_loss: 1.7721, train_bce_dl_loss: 0.3522, step time: 0.3715\n",
      "batch: 16/17, train_dl_loss: 0.3446, train_bce_loss: 1.7677, train_bce_dl_loss: 0.3446, step time: 0.4156\n",
      "batch: 17/17, train_dl_loss: 0.3153, train_bce_loss: 1.7568, train_bce_dl_loss: 0.3153, step time: 0.1126\n",
      "LOSS train DiceLoss: 0.3610, LOSS train BCE: 1.7649, LOSS train BCE-DiceLoss: 0.3610, LOSS val DiceLoss: 0.4798, LOSS val BCE: 1.7583, LOSS val BCE-DiceLoss: 0.4798, METRIC val: 0.5626\n",
      "time consuming of epoch 514 is: 425.7792\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3501, train_bce_loss: 1.7619, train_bce_dl_loss: 0.3501, step time: 0.4284\n",
      "batch: 1/17, train_dl_loss: 0.3396, train_bce_loss: 1.7645, train_bce_dl_loss: 0.3396, step time: 0.3828\n",
      "batch: 2/17, train_dl_loss: 0.3861, train_bce_loss: 1.7539, train_bce_dl_loss: 0.3861, step time: 0.4410\n",
      "batch: 3/17, train_dl_loss: 0.3795, train_bce_loss: 1.7756, train_bce_dl_loss: 0.3795, step time: 0.3819\n",
      "batch: 4/17, train_dl_loss: 0.3225, train_bce_loss: 1.7722, train_bce_dl_loss: 0.3225, step time: 0.4301\n",
      "batch: 5/17, train_dl_loss: 0.3512, train_bce_loss: 1.7542, train_bce_dl_loss: 0.3512, step time: 0.4336\n",
      "batch: 6/17, train_dl_loss: 0.3930, train_bce_loss: 1.7768, train_bce_dl_loss: 0.3930, step time: 0.4375\n",
      "batch: 7/17, train_dl_loss: 0.3572, train_bce_loss: 1.7573, train_bce_dl_loss: 0.3572, step time: 0.3942\n",
      "batch: 8/17, train_dl_loss: 0.3707, train_bce_loss: 1.7513, train_bce_dl_loss: 0.3707, step time: 0.4270\n",
      "batch: 9/17, train_dl_loss: 0.3425, train_bce_loss: 1.7778, train_bce_dl_loss: 0.3425, step time: 0.4301\n",
      "batch: 10/17, train_dl_loss: 0.4293, train_bce_loss: 1.7747, train_bce_dl_loss: 0.4293, step time: 0.4383\n",
      "batch: 11/17, train_dl_loss: 0.3296, train_bce_loss: 1.7657, train_bce_dl_loss: 0.3296, step time: 0.4467\n",
      "batch: 12/17, train_dl_loss: 0.3720, train_bce_loss: 1.7585, train_bce_dl_loss: 0.3720, step time: 0.4391\n",
      "batch: 13/17, train_dl_loss: 0.4215, train_bce_loss: 1.7819, train_bce_dl_loss: 0.4215, step time: 0.4331\n",
      "batch: 14/17, train_dl_loss: 0.3725, train_bce_loss: 1.7775, train_bce_dl_loss: 0.3725, step time: 0.4258\n",
      "batch: 15/17, train_dl_loss: 0.3651, train_bce_loss: 1.7598, train_bce_dl_loss: 0.3651, step time: 0.3764\n",
      "batch: 16/17, train_dl_loss: 0.3439, train_bce_loss: 1.7656, train_bce_dl_loss: 0.3439, step time: 0.4226\n",
      "batch: 17/17, train_dl_loss: 0.3581, train_bce_loss: 1.7600, train_bce_dl_loss: 0.3581, step time: 0.1147\n",
      "LOSS train DiceLoss: 0.3658, LOSS train BCE: 1.7661, LOSS train BCE-DiceLoss: 0.3658, LOSS val DiceLoss: 0.4807, LOSS val BCE: 1.7597, LOSS val BCE-DiceLoss: 0.4807, METRIC val: 0.5622\n",
      "time consuming of epoch 515 is: 438.4204\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3558, train_bce_loss: 1.7482, train_bce_dl_loss: 0.3558, step time: 0.4444\n",
      "batch: 1/17, train_dl_loss: 0.3858, train_bce_loss: 1.7725, train_bce_dl_loss: 0.3858, step time: 0.3837\n",
      "batch: 2/17, train_dl_loss: 0.3429, train_bce_loss: 1.7651, train_bce_dl_loss: 0.3429, step time: 0.4215\n",
      "batch: 3/17, train_dl_loss: 0.3900, train_bce_loss: 1.7721, train_bce_dl_loss: 0.3900, step time: 0.3767\n",
      "batch: 4/17, train_dl_loss: 0.3374, train_bce_loss: 1.7678, train_bce_dl_loss: 0.3374, step time: 0.4307\n",
      "batch: 5/17, train_dl_loss: 0.3548, train_bce_loss: 1.7555, train_bce_dl_loss: 0.3548, step time: 0.3876\n",
      "batch: 6/17, train_dl_loss: 0.3941, train_bce_loss: 1.7886, train_bce_dl_loss: 0.3941, step time: 0.4467\n",
      "batch: 7/17, train_dl_loss: 0.3334, train_bce_loss: 1.7605, train_bce_dl_loss: 0.3334, step time: 0.3827\n",
      "batch: 8/17, train_dl_loss: 0.3560, train_bce_loss: 1.7401, train_bce_dl_loss: 0.3560, step time: 0.4254\n",
      "batch: 9/17, train_dl_loss: 0.3485, train_bce_loss: 1.7748, train_bce_dl_loss: 0.3485, step time: 0.3908\n",
      "batch: 10/17, train_dl_loss: 0.4071, train_bce_loss: 1.7736, train_bce_dl_loss: 0.4071, step time: 0.4429\n",
      "batch: 11/17, train_dl_loss: 0.3507, train_bce_loss: 1.7443, train_bce_dl_loss: 0.3507, step time: 0.4645\n",
      "batch: 12/17, train_dl_loss: 0.3465, train_bce_loss: 1.7663, train_bce_dl_loss: 0.3465, step time: 0.4434\n",
      "batch: 13/17, train_dl_loss: 0.4583, train_bce_loss: 1.7638, train_bce_dl_loss: 0.4583, step time: 0.3836\n",
      "batch: 14/17, train_dl_loss: 0.3753, train_bce_loss: 1.7722, train_bce_dl_loss: 0.3753, step time: 0.4323\n",
      "batch: 15/17, train_dl_loss: 0.3672, train_bce_loss: 1.7676, train_bce_dl_loss: 0.3672, step time: 0.3796\n",
      "batch: 16/17, train_dl_loss: 0.3433, train_bce_loss: 1.7719, train_bce_dl_loss: 0.3433, step time: 0.4334\n",
      "batch: 17/17, train_dl_loss: 0.3137, train_bce_loss: 1.7567, train_bce_dl_loss: 0.3137, step time: 0.1130\n",
      "LOSS train DiceLoss: 0.3645, LOSS train BCE: 1.7645, LOSS train BCE-DiceLoss: 0.3645, LOSS val DiceLoss: 0.4792, LOSS val BCE: 1.7579, LOSS val BCE-DiceLoss: 0.4792, METRIC val: 0.5627\n",
      "time consuming of epoch 516 is: 436.1777\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3351, train_bce_loss: 1.7658, train_bce_dl_loss: 0.3351, step time: 0.4297\n",
      "batch: 1/17, train_dl_loss: 0.3962, train_bce_loss: 1.7678, train_bce_dl_loss: 0.3962, step time: 0.3742\n",
      "batch: 2/17, train_dl_loss: 0.3727, train_bce_loss: 1.7717, train_bce_dl_loss: 0.3727, step time: 0.4259\n",
      "batch: 3/17, train_dl_loss: 0.4238, train_bce_loss: 1.7584, train_bce_dl_loss: 0.4238, step time: 0.3805\n",
      "batch: 4/17, train_dl_loss: 0.3189, train_bce_loss: 1.7543, train_bce_dl_loss: 0.3189, step time: 0.4428\n",
      "batch: 5/17, train_dl_loss: 0.3363, train_bce_loss: 1.7760, train_bce_dl_loss: 0.3363, step time: 0.3824\n",
      "batch: 6/17, train_dl_loss: 0.4088, train_bce_loss: 1.7743, train_bce_dl_loss: 0.4088, step time: 0.4357\n",
      "batch: 7/17, train_dl_loss: 0.3693, train_bce_loss: 1.7524, train_bce_dl_loss: 0.3693, step time: 0.3852\n",
      "batch: 8/17, train_dl_loss: 0.3472, train_bce_loss: 1.7477, train_bce_dl_loss: 0.3472, step time: 0.4307\n",
      "batch: 9/17, train_dl_loss: 0.3497, train_bce_loss: 1.7651, train_bce_dl_loss: 0.3497, step time: 0.3772\n",
      "batch: 10/17, train_dl_loss: 0.4189, train_bce_loss: 1.7540, train_bce_dl_loss: 0.4189, step time: 0.4176\n",
      "batch: 11/17, train_dl_loss: 0.3370, train_bce_loss: 1.7469, train_bce_dl_loss: 0.3370, step time: 0.4385\n",
      "batch: 12/17, train_dl_loss: 0.3406, train_bce_loss: 1.7552, train_bce_dl_loss: 0.3406, step time: 0.4304\n",
      "batch: 13/17, train_dl_loss: 0.4349, train_bce_loss: 1.7709, train_bce_dl_loss: 0.4349, step time: 0.3799\n",
      "batch: 14/17, train_dl_loss: 0.3471, train_bce_loss: 1.7804, train_bce_dl_loss: 0.3471, step time: 0.4217\n",
      "batch: 15/17, train_dl_loss: 0.3392, train_bce_loss: 1.7747, train_bce_dl_loss: 0.3392, step time: 0.3831\n",
      "batch: 16/17, train_dl_loss: 0.3516, train_bce_loss: 1.7795, train_bce_dl_loss: 0.3516, step time: 0.4163\n",
      "batch: 17/17, train_dl_loss: 0.3125, train_bce_loss: 1.7581, train_bce_dl_loss: 0.3125, step time: 0.1145\n",
      "LOSS train DiceLoss: 0.3633, LOSS train BCE: 1.7641, LOSS train BCE-DiceLoss: 0.3633, LOSS val DiceLoss: 0.4804, LOSS val BCE: 1.7579, LOSS val BCE-DiceLoss: 0.4804, METRIC val: 0.5612\n",
      "time consuming of epoch 517 is: 498.5709\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3714, train_bce_loss: 1.7445, train_bce_dl_loss: 0.3714, step time: 0.4427\n",
      "batch: 1/17, train_dl_loss: 0.3659, train_bce_loss: 1.7845, train_bce_dl_loss: 0.3659, step time: 0.3833\n",
      "batch: 2/17, train_dl_loss: 0.3520, train_bce_loss: 1.7481, train_bce_dl_loss: 0.3520, step time: 0.4293\n",
      "batch: 3/17, train_dl_loss: 0.4135, train_bce_loss: 1.7752, train_bce_dl_loss: 0.4135, step time: 0.3815\n",
      "batch: 4/17, train_dl_loss: 0.3453, train_bce_loss: 1.7539, train_bce_dl_loss: 0.3453, step time: 0.4466\n",
      "batch: 5/17, train_dl_loss: 0.3387, train_bce_loss: 1.7563, train_bce_dl_loss: 0.3387, step time: 0.3870\n",
      "batch: 6/17, train_dl_loss: 0.4066, train_bce_loss: 1.7677, train_bce_dl_loss: 0.4066, step time: 0.4462\n",
      "batch: 7/17, train_dl_loss: 0.3193, train_bce_loss: 1.7580, train_bce_dl_loss: 0.3193, step time: 0.4426\n",
      "batch: 8/17, train_dl_loss: 0.3556, train_bce_loss: 1.7513, train_bce_dl_loss: 0.3556, step time: 0.4325\n",
      "batch: 9/17, train_dl_loss: 0.3370, train_bce_loss: 1.7796, train_bce_dl_loss: 0.3370, step time: 0.4360\n",
      "batch: 10/17, train_dl_loss: 0.3831, train_bce_loss: 1.7760, train_bce_dl_loss: 0.3831, step time: 0.4350\n",
      "batch: 11/17, train_dl_loss: 0.3376, train_bce_loss: 1.7671, train_bce_dl_loss: 0.3376, step time: 0.4358\n",
      "batch: 12/17, train_dl_loss: 0.3712, train_bce_loss: 1.7627, train_bce_dl_loss: 0.3712, step time: 0.4398\n",
      "batch: 13/17, train_dl_loss: 0.4457, train_bce_loss: 1.7751, train_bce_dl_loss: 0.4457, step time: 0.4316\n",
      "batch: 14/17, train_dl_loss: 0.3431, train_bce_loss: 1.7827, train_bce_dl_loss: 0.3431, step time: 0.4235\n",
      "batch: 15/17, train_dl_loss: 0.3314, train_bce_loss: 1.7667, train_bce_dl_loss: 0.3314, step time: 0.3903\n",
      "batch: 16/17, train_dl_loss: 0.3527, train_bce_loss: 1.7595, train_bce_dl_loss: 0.3527, step time: 0.4120\n",
      "batch: 17/17, train_dl_loss: 0.3071, train_bce_loss: 1.7793, train_bce_dl_loss: 0.3071, step time: 0.1137\n",
      "LOSS train DiceLoss: 0.3598, LOSS train BCE: 1.7660, LOSS train BCE-DiceLoss: 0.3598, LOSS val DiceLoss: 0.4777, LOSS val BCE: 1.7561, LOSS val BCE-DiceLoss: 0.4777, METRIC val: 0.5641\n",
      "time consuming of epoch 518 is: 443.6721\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3477, train_bce_loss: 1.7457, train_bce_dl_loss: 0.3477, step time: 0.4312\n",
      "batch: 1/17, train_dl_loss: 0.3516, train_bce_loss: 1.7672, train_bce_dl_loss: 0.3516, step time: 0.3860\n",
      "batch: 2/17, train_dl_loss: 0.3508, train_bce_loss: 1.7547, train_bce_dl_loss: 0.3508, step time: 0.4410\n",
      "batch: 3/17, train_dl_loss: 0.3988, train_bce_loss: 1.7545, train_bce_dl_loss: 0.3988, step time: 0.3862\n",
      "batch: 4/17, train_dl_loss: 0.3552, train_bce_loss: 1.7812, train_bce_dl_loss: 0.3552, step time: 0.4421\n",
      "batch: 5/17, train_dl_loss: 0.3869, train_bce_loss: 1.7618, train_bce_dl_loss: 0.3869, step time: 0.3894\n",
      "batch: 6/17, train_dl_loss: 0.3959, train_bce_loss: 1.7823, train_bce_dl_loss: 0.3959, step time: 0.4400\n",
      "batch: 7/17, train_dl_loss: 0.3330, train_bce_loss: 1.7450, train_bce_dl_loss: 0.3330, step time: 0.3739\n",
      "batch: 8/17, train_dl_loss: 0.3359, train_bce_loss: 1.7583, train_bce_dl_loss: 0.3359, step time: 0.4270\n",
      "batch: 9/17, train_dl_loss: 0.3766, train_bce_loss: 1.7745, train_bce_dl_loss: 0.3766, step time: 0.3763\n",
      "batch: 10/17, train_dl_loss: 0.3900, train_bce_loss: 1.7720, train_bce_dl_loss: 0.3900, step time: 0.4400\n",
      "batch: 11/17, train_dl_loss: 0.3599, train_bce_loss: 1.7551, train_bce_dl_loss: 0.3599, step time: 0.3964\n",
      "batch: 12/17, train_dl_loss: 0.3279, train_bce_loss: 1.7666, train_bce_dl_loss: 0.3279, step time: 0.4247\n",
      "batch: 13/17, train_dl_loss: 0.4291, train_bce_loss: 1.7807, train_bce_dl_loss: 0.4291, step time: 0.3931\n",
      "batch: 14/17, train_dl_loss: 0.3780, train_bce_loss: 1.7801, train_bce_dl_loss: 0.3780, step time: 0.4214\n",
      "batch: 15/17, train_dl_loss: 0.3349, train_bce_loss: 1.7535, train_bce_dl_loss: 0.3349, step time: 0.3764\n",
      "batch: 16/17, train_dl_loss: 0.3463, train_bce_loss: 1.7730, train_bce_dl_loss: 0.3463, step time: 0.4268\n",
      "batch: 17/17, train_dl_loss: 0.3613, train_bce_loss: 1.7558, train_bce_dl_loss: 0.3613, step time: 0.1137\n",
      "LOSS train DiceLoss: 0.3644, LOSS train BCE: 1.7645, LOSS train BCE-DiceLoss: 0.3644, LOSS val DiceLoss: 0.4779, LOSS val BCE: 1.7556, LOSS val BCE-DiceLoss: 0.4779, METRIC val: 0.5640\n",
      "time consuming of epoch 519 is: 430.1600\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3725, train_bce_loss: 1.7425, train_bce_dl_loss: 0.3725, step time: 0.4471\n",
      "batch: 1/17, train_dl_loss: 0.3441, train_bce_loss: 1.7714, train_bce_dl_loss: 0.3441, step time: 0.3779\n",
      "batch: 2/17, train_dl_loss: 0.3554, train_bce_loss: 1.7509, train_bce_dl_loss: 0.3554, step time: 0.4270\n",
      "batch: 3/17, train_dl_loss: 0.3823, train_bce_loss: 1.7579, train_bce_dl_loss: 0.3823, step time: 0.4310\n",
      "batch: 4/17, train_dl_loss: 0.3403, train_bce_loss: 1.7451, train_bce_dl_loss: 0.3403, step time: 0.4255\n",
      "batch: 5/17, train_dl_loss: 0.3333, train_bce_loss: 1.7741, train_bce_dl_loss: 0.3333, step time: 0.4506\n",
      "batch: 6/17, train_dl_loss: 0.3766, train_bce_loss: 1.7633, train_bce_dl_loss: 0.3766, step time: 0.4185\n",
      "batch: 7/17, train_dl_loss: 0.3679, train_bce_loss: 1.7750, train_bce_dl_loss: 0.3679, step time: 0.4296\n",
      "batch: 8/17, train_dl_loss: 0.3508, train_bce_loss: 1.7548, train_bce_dl_loss: 0.3508, step time: 0.4274\n",
      "batch: 9/17, train_dl_loss: 0.3522, train_bce_loss: 1.7713, train_bce_dl_loss: 0.3522, step time: 0.4291\n",
      "batch: 10/17, train_dl_loss: 0.4081, train_bce_loss: 1.7724, train_bce_dl_loss: 0.4081, step time: 0.4274\n",
      "batch: 11/17, train_dl_loss: 0.3197, train_bce_loss: 1.7373, train_bce_dl_loss: 0.3197, step time: 0.4706\n",
      "batch: 12/17, train_dl_loss: 0.3322, train_bce_loss: 1.7744, train_bce_dl_loss: 0.3322, step time: 0.4266\n",
      "batch: 13/17, train_dl_loss: 0.4044, train_bce_loss: 1.7826, train_bce_dl_loss: 0.4044, step time: 0.4240\n",
      "batch: 14/17, train_dl_loss: 0.3530, train_bce_loss: 1.7886, train_bce_dl_loss: 0.3530, step time: 0.4248\n",
      "batch: 15/17, train_dl_loss: 0.3729, train_bce_loss: 1.7638, train_bce_dl_loss: 0.3729, step time: 0.3777\n",
      "batch: 16/17, train_dl_loss: 0.3594, train_bce_loss: 1.7741, train_bce_dl_loss: 0.3594, step time: 0.4188\n",
      "batch: 17/17, train_dl_loss: 0.3142, train_bce_loss: 1.7603, train_bce_dl_loss: 0.3142, step time: 0.1126\n",
      "LOSS train DiceLoss: 0.3577, LOSS train BCE: 1.7644, LOSS train BCE-DiceLoss: 0.3577, LOSS val DiceLoss: 0.4796, LOSS val BCE: 1.7569, LOSS val BCE-DiceLoss: 0.4796, METRIC val: 0.5613\n",
      "time consuming of epoch 520 is: 443.1759\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3273, train_bce_loss: 1.7769, train_bce_dl_loss: 0.3273, step time: 0.4446\n",
      "batch: 1/17, train_dl_loss: 0.3341, train_bce_loss: 1.7601, train_bce_dl_loss: 0.3341, step time: 0.3835\n",
      "batch: 2/17, train_dl_loss: 0.3783, train_bce_loss: 1.7689, train_bce_dl_loss: 0.3783, step time: 0.4307\n",
      "batch: 3/17, train_dl_loss: 0.4282, train_bce_loss: 1.7850, train_bce_dl_loss: 0.4282, step time: 0.3820\n",
      "batch: 4/17, train_dl_loss: 0.3325, train_bce_loss: 1.7651, train_bce_dl_loss: 0.3325, step time: 0.4357\n",
      "batch: 5/17, train_dl_loss: 0.3854, train_bce_loss: 1.7556, train_bce_dl_loss: 0.3854, step time: 0.3901\n",
      "batch: 6/17, train_dl_loss: 0.4168, train_bce_loss: 1.7744, train_bce_dl_loss: 0.4168, step time: 0.4413\n",
      "batch: 7/17, train_dl_loss: 0.3325, train_bce_loss: 1.7631, train_bce_dl_loss: 0.3325, step time: 0.3950\n",
      "batch: 8/17, train_dl_loss: 0.3386, train_bce_loss: 1.7690, train_bce_dl_loss: 0.3386, step time: 0.4387\n",
      "batch: 9/17, train_dl_loss: 0.3534, train_bce_loss: 1.7856, train_bce_dl_loss: 0.3534, step time: 0.3793\n",
      "batch: 10/17, train_dl_loss: 0.4180, train_bce_loss: 1.7605, train_bce_dl_loss: 0.4180, step time: 0.4273\n",
      "batch: 11/17, train_dl_loss: 0.3466, train_bce_loss: 1.7466, train_bce_dl_loss: 0.3466, step time: 0.3867\n",
      "batch: 12/17, train_dl_loss: 0.3333, train_bce_loss: 1.7779, train_bce_dl_loss: 0.3333, step time: 0.4349\n",
      "batch: 13/17, train_dl_loss: 0.4201, train_bce_loss: 1.7679, train_bce_dl_loss: 0.4201, step time: 0.3875\n",
      "batch: 14/17, train_dl_loss: 0.3704, train_bce_loss: 1.7826, train_bce_dl_loss: 0.3704, step time: 0.4586\n",
      "batch: 15/17, train_dl_loss: 0.3415, train_bce_loss: 1.7630, train_bce_dl_loss: 0.3415, step time: 0.3794\n",
      "batch: 16/17, train_dl_loss: 0.3551, train_bce_loss: 1.7699, train_bce_dl_loss: 0.3551, step time: 0.4141\n",
      "batch: 17/17, train_dl_loss: 0.3090, train_bce_loss: 1.7576, train_bce_dl_loss: 0.3090, step time: 0.1131\n",
      "LOSS train DiceLoss: 0.3623, LOSS train BCE: 1.7683, LOSS train BCE-DiceLoss: 0.3623, LOSS val DiceLoss: 0.4814, LOSS val BCE: 1.7568, LOSS val BCE-DiceLoss: 0.4814, METRIC val: 0.5590\n",
      "time consuming of epoch 521 is: 443.1473\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3533, train_bce_loss: 1.7446, train_bce_dl_loss: 0.3533, step time: 0.4282\n",
      "batch: 1/17, train_dl_loss: 0.3425, train_bce_loss: 1.7704, train_bce_dl_loss: 0.3425, step time: 0.3843\n",
      "batch: 2/17, train_dl_loss: 0.3559, train_bce_loss: 1.7738, train_bce_dl_loss: 0.3559, step time: 0.4370\n",
      "batch: 3/17, train_dl_loss: 0.3897, train_bce_loss: 1.7645, train_bce_dl_loss: 0.3897, step time: 0.3908\n",
      "batch: 4/17, train_dl_loss: 0.3300, train_bce_loss: 1.7757, train_bce_dl_loss: 0.3300, step time: 0.4319\n",
      "batch: 5/17, train_dl_loss: 0.3728, train_bce_loss: 1.7708, train_bce_dl_loss: 0.3728, step time: 0.3896\n",
      "batch: 6/17, train_dl_loss: 0.4175, train_bce_loss: 1.7867, train_bce_dl_loss: 0.4175, step time: 0.4331\n",
      "batch: 7/17, train_dl_loss: 0.3367, train_bce_loss: 1.7559, train_bce_dl_loss: 0.3367, step time: 0.3778\n",
      "batch: 8/17, train_dl_loss: 0.3651, train_bce_loss: 1.7572, train_bce_dl_loss: 0.3651, step time: 0.4276\n",
      "batch: 9/17, train_dl_loss: 0.3714, train_bce_loss: 1.7705, train_bce_dl_loss: 0.3714, step time: 0.3964\n",
      "batch: 10/17, train_dl_loss: 0.3889, train_bce_loss: 1.7484, train_bce_dl_loss: 0.3889, step time: 0.4248\n",
      "batch: 11/17, train_dl_loss: 0.3331, train_bce_loss: 1.7565, train_bce_dl_loss: 0.3331, step time: 0.4299\n",
      "batch: 12/17, train_dl_loss: 0.3533, train_bce_loss: 1.7691, train_bce_dl_loss: 0.3533, step time: 0.4374\n",
      "batch: 13/17, train_dl_loss: 0.4179, train_bce_loss: 1.7791, train_bce_dl_loss: 0.4179, step time: 0.3810\n",
      "batch: 14/17, train_dl_loss: 0.3812, train_bce_loss: 1.7839, train_bce_dl_loss: 0.3812, step time: 0.4207\n",
      "batch: 15/17, train_dl_loss: 0.3501, train_bce_loss: 1.7664, train_bce_dl_loss: 0.3501, step time: 0.3797\n",
      "batch: 16/17, train_dl_loss: 0.3377, train_bce_loss: 1.7753, train_bce_dl_loss: 0.3377, step time: 0.4181\n",
      "batch: 17/17, train_dl_loss: 0.3058, train_bce_loss: 1.7825, train_bce_dl_loss: 0.3058, step time: 0.1143\n",
      "LOSS train DiceLoss: 0.3613, LOSS train BCE: 1.7684, LOSS train BCE-DiceLoss: 0.3613, LOSS val DiceLoss: 0.4799, LOSS val BCE: 1.7560, LOSS val BCE-DiceLoss: 0.4799, METRIC val: 0.5610\n",
      "time consuming of epoch 522 is: 446.4375\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3588, train_bce_loss: 1.7365, train_bce_dl_loss: 0.3588, step time: 0.4408\n",
      "batch: 1/17, train_dl_loss: 0.3837, train_bce_loss: 1.7662, train_bce_dl_loss: 0.3837, step time: 0.3804\n",
      "batch: 2/17, train_dl_loss: 0.3725, train_bce_loss: 1.7690, train_bce_dl_loss: 0.3725, step time: 0.4265\n",
      "batch: 3/17, train_dl_loss: 0.4014, train_bce_loss: 1.7816, train_bce_dl_loss: 0.4014, step time: 0.3783\n",
      "batch: 4/17, train_dl_loss: 0.3218, train_bce_loss: 1.7644, train_bce_dl_loss: 0.3218, step time: 0.4471\n",
      "batch: 5/17, train_dl_loss: 0.3459, train_bce_loss: 1.7415, train_bce_dl_loss: 0.3459, step time: 0.3863\n",
      "batch: 6/17, train_dl_loss: 0.3999, train_bce_loss: 1.7737, train_bce_dl_loss: 0.3999, step time: 0.4292\n",
      "batch: 7/17, train_dl_loss: 0.3460, train_bce_loss: 1.7430, train_bce_dl_loss: 0.3460, step time: 0.3871\n",
      "batch: 8/17, train_dl_loss: 0.3609, train_bce_loss: 1.7332, train_bce_dl_loss: 0.3609, step time: 0.4278\n",
      "batch: 9/17, train_dl_loss: 0.3458, train_bce_loss: 1.7779, train_bce_dl_loss: 0.3458, step time: 0.3780\n",
      "batch: 10/17, train_dl_loss: 0.3705, train_bce_loss: 1.7636, train_bce_dl_loss: 0.3705, step time: 0.4473\n",
      "batch: 11/17, train_dl_loss: 0.3473, train_bce_loss: 1.7551, train_bce_dl_loss: 0.3473, step time: 0.3862\n",
      "batch: 12/17, train_dl_loss: 0.3454, train_bce_loss: 1.7552, train_bce_dl_loss: 0.3454, step time: 0.4152\n",
      "batch: 13/17, train_dl_loss: 0.3969, train_bce_loss: 1.7705, train_bce_dl_loss: 0.3969, step time: 0.3832\n",
      "batch: 14/17, train_dl_loss: 0.3773, train_bce_loss: 1.7823, train_bce_dl_loss: 0.3773, step time: 0.4360\n",
      "batch: 15/17, train_dl_loss: 0.3479, train_bce_loss: 1.7651, train_bce_dl_loss: 0.3479, step time: 0.3830\n",
      "batch: 16/17, train_dl_loss: 0.3563, train_bce_loss: 1.7644, train_bce_dl_loss: 0.3563, step time: 0.4135\n",
      "batch: 17/17, train_dl_loss: 0.3588, train_bce_loss: 1.7573, train_bce_dl_loss: 0.3588, step time: 0.1131\n",
      "LOSS train DiceLoss: 0.3632, LOSS train BCE: 1.7611, LOSS train BCE-DiceLoss: 0.3632, LOSS val DiceLoss: 0.4799, LOSS val BCE: 1.7577, LOSS val BCE-DiceLoss: 0.4799, METRIC val: 0.5612\n",
      "time consuming of epoch 523 is: 445.6305\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3830, train_bce_loss: 1.7763, train_bce_dl_loss: 0.3830, step time: 0.4256\n",
      "batch: 1/17, train_dl_loss: 0.3592, train_bce_loss: 1.7763, train_bce_dl_loss: 0.3592, step time: 0.3825\n",
      "batch: 2/17, train_dl_loss: 0.3800, train_bce_loss: 1.7597, train_bce_dl_loss: 0.3800, step time: 0.4232\n",
      "batch: 3/17, train_dl_loss: 0.4133, train_bce_loss: 1.7455, train_bce_dl_loss: 0.4133, step time: 0.3825\n",
      "batch: 4/17, train_dl_loss: 0.3308, train_bce_loss: 1.7718, train_bce_dl_loss: 0.3308, step time: 0.4411\n",
      "batch: 5/17, train_dl_loss: 0.3481, train_bce_loss: 1.7454, train_bce_dl_loss: 0.3481, step time: 0.3851\n",
      "batch: 6/17, train_dl_loss: 0.3951, train_bce_loss: 1.7724, train_bce_dl_loss: 0.3951, step time: 0.4227\n",
      "batch: 7/17, train_dl_loss: 0.3399, train_bce_loss: 1.7447, train_bce_dl_loss: 0.3399, step time: 0.3795\n",
      "batch: 8/17, train_dl_loss: 0.3557, train_bce_loss: 1.7588, train_bce_dl_loss: 0.3557, step time: 0.4436\n",
      "batch: 9/17, train_dl_loss: 0.3406, train_bce_loss: 1.7686, train_bce_dl_loss: 0.3406, step time: 0.3853\n",
      "batch: 10/17, train_dl_loss: 0.3732, train_bce_loss: 1.7737, train_bce_dl_loss: 0.3732, step time: 0.4437\n",
      "batch: 11/17, train_dl_loss: 0.3298, train_bce_loss: 1.7522, train_bce_dl_loss: 0.3298, step time: 0.4360\n",
      "batch: 12/17, train_dl_loss: 0.3394, train_bce_loss: 1.7452, train_bce_dl_loss: 0.3394, step time: 0.4408\n",
      "batch: 13/17, train_dl_loss: 0.4199, train_bce_loss: 1.7716, train_bce_dl_loss: 0.4199, step time: 0.3880\n",
      "batch: 14/17, train_dl_loss: 0.3601, train_bce_loss: 1.7886, train_bce_dl_loss: 0.3601, step time: 0.4530\n",
      "batch: 15/17, train_dl_loss: 0.3352, train_bce_loss: 1.7667, train_bce_dl_loss: 0.3352, step time: 0.3863\n",
      "batch: 16/17, train_dl_loss: 0.3565, train_bce_loss: 1.7698, train_bce_dl_loss: 0.3565, step time: 0.4278\n",
      "batch: 17/17, train_dl_loss: 0.3087, train_bce_loss: 1.7644, train_bce_dl_loss: 0.3087, step time: 0.1141\n",
      "LOSS train DiceLoss: 0.3594, LOSS train BCE: 1.7640, LOSS train BCE-DiceLoss: 0.3594, LOSS val DiceLoss: 0.4845, LOSS val BCE: 1.7581, LOSS val BCE-DiceLoss: 0.4845, METRIC val: 0.5560\n",
      "time consuming of epoch 524 is: 426.9308\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3547, train_bce_loss: 1.7636, train_bce_dl_loss: 0.3547, step time: 0.4478\n",
      "batch: 1/17, train_dl_loss: 0.3800, train_bce_loss: 1.7641, train_bce_dl_loss: 0.3800, step time: 0.4491\n",
      "batch: 2/17, train_dl_loss: 0.3608, train_bce_loss: 1.7563, train_bce_dl_loss: 0.3608, step time: 0.4230\n",
      "batch: 3/17, train_dl_loss: 0.3908, train_bce_loss: 1.7619, train_bce_dl_loss: 0.3908, step time: 0.3764\n",
      "batch: 4/17, train_dl_loss: 0.3368, train_bce_loss: 1.7465, train_bce_dl_loss: 0.3368, step time: 0.4220\n",
      "batch: 5/17, train_dl_loss: 0.3643, train_bce_loss: 1.7537, train_bce_dl_loss: 0.3643, step time: 0.3818\n",
      "batch: 6/17, train_dl_loss: 0.3844, train_bce_loss: 1.7723, train_bce_dl_loss: 0.3844, step time: 0.4563\n",
      "batch: 7/17, train_dl_loss: 0.3531, train_bce_loss: 1.7476, train_bce_dl_loss: 0.3531, step time: 0.3898\n",
      "batch: 8/17, train_dl_loss: 0.3477, train_bce_loss: 1.7591, train_bce_dl_loss: 0.3477, step time: 0.4429\n",
      "batch: 9/17, train_dl_loss: 0.3410, train_bce_loss: 1.7684, train_bce_dl_loss: 0.3410, step time: 0.3764\n",
      "batch: 10/17, train_dl_loss: 0.4569, train_bce_loss: 1.7726, train_bce_dl_loss: 0.4569, step time: 0.4378\n",
      "batch: 11/17, train_dl_loss: 0.3796, train_bce_loss: 1.7478, train_bce_dl_loss: 0.3796, step time: 0.3913\n",
      "batch: 12/17, train_dl_loss: 0.3560, train_bce_loss: 1.7569, train_bce_dl_loss: 0.3560, step time: 0.4353\n",
      "batch: 13/17, train_dl_loss: 0.3997, train_bce_loss: 1.7836, train_bce_dl_loss: 0.3997, step time: 0.3846\n",
      "batch: 14/17, train_dl_loss: 0.3700, train_bce_loss: 1.7742, train_bce_dl_loss: 0.3700, step time: 0.4307\n",
      "batch: 15/17, train_dl_loss: 0.3282, train_bce_loss: 1.7640, train_bce_dl_loss: 0.3282, step time: 0.3788\n",
      "batch: 16/17, train_dl_loss: 0.3622, train_bce_loss: 1.7713, train_bce_dl_loss: 0.3622, step time: 0.4160\n",
      "batch: 17/17, train_dl_loss: 0.3233, train_bce_loss: 1.7553, train_bce_dl_loss: 0.3233, step time: 0.1126\n",
      "LOSS train DiceLoss: 0.3661, LOSS train BCE: 1.7622, LOSS train BCE-DiceLoss: 0.3661, LOSS val DiceLoss: 0.4874, LOSS val BCE: 1.7567, LOSS val BCE-DiceLoss: 0.4874, METRIC val: 0.5511\n",
      "time consuming of epoch 525 is: 399.7132\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3608, train_bce_loss: 1.7703, train_bce_dl_loss: 0.3608, step time: 0.4312\n",
      "batch: 1/17, train_dl_loss: 0.3431, train_bce_loss: 1.7687, train_bce_dl_loss: 0.3431, step time: 0.3847\n",
      "batch: 2/17, train_dl_loss: 0.3422, train_bce_loss: 1.7615, train_bce_dl_loss: 0.3422, step time: 0.4227\n",
      "batch: 3/17, train_dl_loss: 0.3736, train_bce_loss: 1.7626, train_bce_dl_loss: 0.3736, step time: 0.3730\n",
      "batch: 4/17, train_dl_loss: 0.3366, train_bce_loss: 1.7673, train_bce_dl_loss: 0.3366, step time: 0.4261\n",
      "batch: 5/17, train_dl_loss: 0.3275, train_bce_loss: 1.7608, train_bce_dl_loss: 0.3275, step time: 0.4270\n",
      "batch: 6/17, train_dl_loss: 0.3677, train_bce_loss: 1.7703, train_bce_dl_loss: 0.3677, step time: 0.4422\n",
      "batch: 7/17, train_dl_loss: 0.3371, train_bce_loss: 1.7531, train_bce_dl_loss: 0.3371, step time: 0.4459\n",
      "batch: 8/17, train_dl_loss: 0.3439, train_bce_loss: 1.7693, train_bce_dl_loss: 0.3439, step time: 0.4205\n",
      "batch: 9/17, train_dl_loss: 0.3653, train_bce_loss: 1.7653, train_bce_dl_loss: 0.3653, step time: 0.4314\n",
      "batch: 10/17, train_dl_loss: 0.4144, train_bce_loss: 1.7787, train_bce_dl_loss: 0.4144, step time: 0.4372\n",
      "batch: 11/17, train_dl_loss: 0.3487, train_bce_loss: 1.7551, train_bce_dl_loss: 0.3487, step time: 0.4315\n",
      "batch: 12/17, train_dl_loss: 0.3563, train_bce_loss: 1.7566, train_bce_dl_loss: 0.3563, step time: 0.4458\n",
      "batch: 13/17, train_dl_loss: 0.4088, train_bce_loss: 1.7750, train_bce_dl_loss: 0.4088, step time: 0.4457\n",
      "batch: 14/17, train_dl_loss: 0.3737, train_bce_loss: 1.7743, train_bce_dl_loss: 0.3737, step time: 0.4264\n",
      "batch: 15/17, train_dl_loss: 0.3555, train_bce_loss: 1.7818, train_bce_dl_loss: 0.3555, step time: 0.3796\n",
      "batch: 16/17, train_dl_loss: 0.3363, train_bce_loss: 1.7666, train_bce_dl_loss: 0.3363, step time: 0.4240\n",
      "batch: 17/17, train_dl_loss: 0.4446, train_bce_loss: 1.7880, train_bce_dl_loss: 0.4446, step time: 0.1125\n",
      "LOSS train DiceLoss: 0.3631, LOSS train BCE: 1.7681, LOSS train BCE-DiceLoss: 0.3631, LOSS val DiceLoss: 0.4857, LOSS val BCE: 1.7578, LOSS val BCE-DiceLoss: 0.4857, METRIC val: 0.5530\n",
      "time consuming of epoch 526 is: 473.9079\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3872, train_bce_loss: 1.7856, train_bce_dl_loss: 0.3872, step time: 0.4322\n",
      "batch: 1/17, train_dl_loss: 0.3638, train_bce_loss: 1.7586, train_bce_dl_loss: 0.3638, step time: 0.3763\n",
      "batch: 2/17, train_dl_loss: 0.3511, train_bce_loss: 1.7555, train_bce_dl_loss: 0.3511, step time: 0.4361\n",
      "batch: 3/17, train_dl_loss: 0.3926, train_bce_loss: 1.7657, train_bce_dl_loss: 0.3926, step time: 0.3862\n",
      "batch: 4/17, train_dl_loss: 0.3415, train_bce_loss: 1.7767, train_bce_dl_loss: 0.3415, step time: 0.4372\n",
      "batch: 5/17, train_dl_loss: 0.3126, train_bce_loss: 1.7590, train_bce_dl_loss: 0.3126, step time: 0.3885\n",
      "batch: 6/17, train_dl_loss: 0.3852, train_bce_loss: 1.7656, train_bce_dl_loss: 0.3852, step time: 0.4533\n",
      "batch: 7/17, train_dl_loss: 0.3267, train_bce_loss: 1.7674, train_bce_dl_loss: 0.3267, step time: 0.3900\n",
      "batch: 8/17, train_dl_loss: 0.3405, train_bce_loss: 1.7433, train_bce_dl_loss: 0.3405, step time: 0.4383\n",
      "batch: 9/17, train_dl_loss: 0.4208, train_bce_loss: 1.7813, train_bce_dl_loss: 0.4208, step time: 0.3876\n",
      "batch: 10/17, train_dl_loss: 0.4276, train_bce_loss: 1.7805, train_bce_dl_loss: 0.4276, step time: 0.4466\n",
      "batch: 11/17, train_dl_loss: 0.3474, train_bce_loss: 1.7603, train_bce_dl_loss: 0.3474, step time: 0.4246\n",
      "batch: 12/17, train_dl_loss: 0.3367, train_bce_loss: 1.7575, train_bce_dl_loss: 0.3367, step time: 0.4390\n",
      "batch: 13/17, train_dl_loss: 0.3895, train_bce_loss: 1.7664, train_bce_dl_loss: 0.3895, step time: 0.3796\n",
      "batch: 14/17, train_dl_loss: 0.3361, train_bce_loss: 1.7862, train_bce_dl_loss: 0.3361, step time: 0.4338\n",
      "batch: 15/17, train_dl_loss: 0.3563, train_bce_loss: 1.7710, train_bce_dl_loss: 0.3563, step time: 0.3733\n",
      "batch: 16/17, train_dl_loss: 0.3353, train_bce_loss: 1.7712, train_bce_dl_loss: 0.3353, step time: 0.4154\n",
      "batch: 17/17, train_dl_loss: 0.3539, train_bce_loss: 1.7579, train_bce_dl_loss: 0.3539, step time: 0.1124\n",
      "LOSS train DiceLoss: 0.3614, LOSS train BCE: 1.7672, LOSS train BCE-DiceLoss: 0.3614, LOSS val DiceLoss: 0.4828, LOSS val BCE: 1.7577, LOSS val BCE-DiceLoss: 0.4828, METRIC val: 0.5562\n",
      "time consuming of epoch 527 is: 454.6020\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3427, train_bce_loss: 1.7482, train_bce_dl_loss: 0.3427, step time: 0.4246\n",
      "batch: 1/17, train_dl_loss: 0.3676, train_bce_loss: 1.7571, train_bce_dl_loss: 0.3676, step time: 0.3766\n",
      "batch: 2/17, train_dl_loss: 0.3596, train_bce_loss: 1.7622, train_bce_dl_loss: 0.3596, step time: 0.4273\n",
      "batch: 3/17, train_dl_loss: 0.3991, train_bce_loss: 1.7546, train_bce_dl_loss: 0.3991, step time: 0.3798\n",
      "batch: 4/17, train_dl_loss: 0.3419, train_bce_loss: 1.7879, train_bce_dl_loss: 0.3419, step time: 0.4306\n",
      "batch: 5/17, train_dl_loss: 0.3547, train_bce_loss: 1.7434, train_bce_dl_loss: 0.3547, step time: 0.3850\n",
      "batch: 6/17, train_dl_loss: 0.4268, train_bce_loss: 1.7658, train_bce_dl_loss: 0.4268, step time: 0.4343\n",
      "batch: 7/17, train_dl_loss: 0.3285, train_bce_loss: 1.7477, train_bce_dl_loss: 0.3285, step time: 0.3847\n",
      "batch: 8/17, train_dl_loss: 0.3681, train_bce_loss: 1.7700, train_bce_dl_loss: 0.3681, step time: 0.4349\n",
      "batch: 9/17, train_dl_loss: 0.3312, train_bce_loss: 1.7763, train_bce_dl_loss: 0.3312, step time: 0.3844\n",
      "batch: 10/17, train_dl_loss: 0.3820, train_bce_loss: 1.7621, train_bce_dl_loss: 0.3820, step time: 0.4224\n",
      "batch: 11/17, train_dl_loss: 0.3224, train_bce_loss: 1.7573, train_bce_dl_loss: 0.3224, step time: 0.4555\n",
      "batch: 12/17, train_dl_loss: 0.3489, train_bce_loss: 1.7741, train_bce_dl_loss: 0.3489, step time: 0.4316\n",
      "batch: 13/17, train_dl_loss: 0.3978, train_bce_loss: 1.7843, train_bce_dl_loss: 0.3978, step time: 0.3908\n",
      "batch: 14/17, train_dl_loss: 0.3663, train_bce_loss: 1.7877, train_bce_dl_loss: 0.3663, step time: 0.4410\n",
      "batch: 15/17, train_dl_loss: 0.3453, train_bce_loss: 1.7798, train_bce_dl_loss: 0.3453, step time: 0.3836\n",
      "batch: 16/17, train_dl_loss: 0.3441, train_bce_loss: 1.7758, train_bce_dl_loss: 0.3441, step time: 0.4265\n",
      "batch: 17/17, train_dl_loss: 0.3162, train_bce_loss: 1.7817, train_bce_dl_loss: 0.3162, step time: 0.1149\n",
      "LOSS train DiceLoss: 0.3580, LOSS train BCE: 1.7676, LOSS train BCE-DiceLoss: 0.3580, LOSS val DiceLoss: 0.4793, LOSS val BCE: 1.7601, LOSS val BCE-DiceLoss: 0.4793, METRIC val: 0.5614\n",
      "time consuming of epoch 528 is: 426.0058\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3367, train_bce_loss: 1.7621, train_bce_dl_loss: 0.3367, step time: 0.4359\n",
      "batch: 1/17, train_dl_loss: 0.4058, train_bce_loss: 1.7673, train_bce_dl_loss: 0.4058, step time: 0.3789\n",
      "batch: 2/17, train_dl_loss: 0.3539, train_bce_loss: 1.7669, train_bce_dl_loss: 0.3539, step time: 0.4261\n",
      "batch: 3/17, train_dl_loss: 0.4432, train_bce_loss: 1.7564, train_bce_dl_loss: 0.4432, step time: 0.3824\n",
      "batch: 4/17, train_dl_loss: 0.3338, train_bce_loss: 1.7657, train_bce_dl_loss: 0.3338, step time: 0.4340\n",
      "batch: 5/17, train_dl_loss: 0.3231, train_bce_loss: 1.7711, train_bce_dl_loss: 0.3231, step time: 0.3845\n",
      "batch: 6/17, train_dl_loss: 0.3775, train_bce_loss: 1.7736, train_bce_dl_loss: 0.3775, step time: 0.4274\n",
      "batch: 7/17, train_dl_loss: 0.3424, train_bce_loss: 1.7398, train_bce_dl_loss: 0.3424, step time: 0.3885\n",
      "batch: 8/17, train_dl_loss: 0.3546, train_bce_loss: 1.7620, train_bce_dl_loss: 0.3546, step time: 0.4405\n",
      "batch: 9/17, train_dl_loss: 0.3720, train_bce_loss: 1.7601, train_bce_dl_loss: 0.3720, step time: 0.3844\n",
      "batch: 10/17, train_dl_loss: 0.4216, train_bce_loss: 1.7861, train_bce_dl_loss: 0.4216, step time: 0.4477\n",
      "batch: 11/17, train_dl_loss: 0.3426, train_bce_loss: 1.7745, train_bce_dl_loss: 0.3426, step time: 0.3945\n",
      "batch: 12/17, train_dl_loss: 0.3307, train_bce_loss: 1.7608, train_bce_dl_loss: 0.3307, step time: 0.4269\n",
      "batch: 13/17, train_dl_loss: 0.4331, train_bce_loss: 1.7818, train_bce_dl_loss: 0.4331, step time: 0.3741\n",
      "batch: 14/17, train_dl_loss: 0.3828, train_bce_loss: 1.7693, train_bce_dl_loss: 0.3828, step time: 0.4401\n",
      "batch: 15/17, train_dl_loss: 0.3422, train_bce_loss: 1.7726, train_bce_dl_loss: 0.3422, step time: 0.3879\n",
      "batch: 16/17, train_dl_loss: 0.3557, train_bce_loss: 1.7772, train_bce_dl_loss: 0.3557, step time: 0.4242\n",
      "batch: 17/17, train_dl_loss: 0.3080, train_bce_loss: 1.7599, train_bce_dl_loss: 0.3080, step time: 0.1130\n",
      "LOSS train DiceLoss: 0.3644, LOSS train BCE: 1.7671, LOSS train BCE-DiceLoss: 0.3644, LOSS val DiceLoss: 0.4808, LOSS val BCE: 1.7599, LOSS val BCE-DiceLoss: 0.4808, METRIC val: 0.5610\n",
      "time consuming of epoch 529 is: 429.4390\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3565, train_bce_loss: 1.7652, train_bce_dl_loss: 0.3565, step time: 0.4270\n",
      "batch: 1/17, train_dl_loss: 0.4132, train_bce_loss: 1.7850, train_bce_dl_loss: 0.4132, step time: 0.3855\n",
      "batch: 2/17, train_dl_loss: 0.3522, train_bce_loss: 1.7737, train_bce_dl_loss: 0.3522, step time: 0.4255\n",
      "batch: 3/17, train_dl_loss: 0.3818, train_bce_loss: 1.7640, train_bce_dl_loss: 0.3818, step time: 0.3848\n",
      "batch: 4/17, train_dl_loss: 0.3393, train_bce_loss: 1.7470, train_bce_dl_loss: 0.3393, step time: 0.4198\n",
      "batch: 5/17, train_dl_loss: 0.3449, train_bce_loss: 1.7567, train_bce_dl_loss: 0.3449, step time: 0.3851\n",
      "batch: 6/17, train_dl_loss: 0.4067, train_bce_loss: 1.7741, train_bce_dl_loss: 0.4067, step time: 0.4290\n",
      "batch: 7/17, train_dl_loss: 0.3524, train_bce_loss: 1.7714, train_bce_dl_loss: 0.3524, step time: 0.3821\n",
      "batch: 8/17, train_dl_loss: 0.3605, train_bce_loss: 1.7312, train_bce_dl_loss: 0.3605, step time: 0.4304\n",
      "batch: 9/17, train_dl_loss: 0.3440, train_bce_loss: 1.7658, train_bce_dl_loss: 0.3440, step time: 0.4279\n",
      "batch: 10/17, train_dl_loss: 0.3791, train_bce_loss: 1.7521, train_bce_dl_loss: 0.3791, step time: 0.4426\n",
      "batch: 11/17, train_dl_loss: 0.3357, train_bce_loss: 1.7421, train_bce_dl_loss: 0.3357, step time: 0.4276\n",
      "batch: 12/17, train_dl_loss: 0.3658, train_bce_loss: 1.7659, train_bce_dl_loss: 0.3658, step time: 0.4356\n",
      "batch: 13/17, train_dl_loss: 0.4580, train_bce_loss: 1.7624, train_bce_dl_loss: 0.4580, step time: 0.3821\n",
      "batch: 14/17, train_dl_loss: 0.3348, train_bce_loss: 1.7803, train_bce_dl_loss: 0.3348, step time: 0.4175\n",
      "batch: 15/17, train_dl_loss: 0.3681, train_bce_loss: 1.7571, train_bce_dl_loss: 0.3681, step time: 0.3798\n",
      "batch: 16/17, train_dl_loss: 0.3499, train_bce_loss: 1.7717, train_bce_dl_loss: 0.3499, step time: 0.4258\n",
      "batch: 17/17, train_dl_loss: 0.3083, train_bce_loss: 1.7815, train_bce_dl_loss: 0.3083, step time: 0.1132\n",
      "LOSS train DiceLoss: 0.3639, LOSS train BCE: 1.7637, LOSS train BCE-DiceLoss: 0.3639, LOSS val DiceLoss: 0.4806, LOSS val BCE: 1.7597, LOSS val BCE-DiceLoss: 0.4806, METRIC val: 0.5613\n",
      "time consuming of epoch 530 is: 432.1698\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3329, train_bce_loss: 1.7661, train_bce_dl_loss: 0.3329, step time: 0.4444\n",
      "batch: 1/17, train_dl_loss: 0.3491, train_bce_loss: 1.7666, train_bce_dl_loss: 0.3491, step time: 0.3794\n",
      "batch: 2/17, train_dl_loss: 0.3379, train_bce_loss: 1.7579, train_bce_dl_loss: 0.3379, step time: 0.4313\n",
      "batch: 3/17, train_dl_loss: 0.3773, train_bce_loss: 1.7708, train_bce_dl_loss: 0.3773, step time: 0.3947\n",
      "batch: 4/17, train_dl_loss: 0.3392, train_bce_loss: 1.7681, train_bce_dl_loss: 0.3392, step time: 0.4402\n",
      "batch: 5/17, train_dl_loss: 0.3206, train_bce_loss: 1.7632, train_bce_dl_loss: 0.3206, step time: 0.3910\n",
      "batch: 6/17, train_dl_loss: 0.3885, train_bce_loss: 1.7665, train_bce_dl_loss: 0.3885, step time: 0.4275\n",
      "batch: 7/17, train_dl_loss: 0.3256, train_bce_loss: 1.7453, train_bce_dl_loss: 0.3256, step time: 0.3827\n",
      "batch: 8/17, train_dl_loss: 0.3320, train_bce_loss: 1.7446, train_bce_dl_loss: 0.3320, step time: 0.4478\n",
      "batch: 9/17, train_dl_loss: 0.3610, train_bce_loss: 1.7695, train_bce_dl_loss: 0.3610, step time: 0.3833\n",
      "batch: 10/17, train_dl_loss: 0.3876, train_bce_loss: 1.7579, train_bce_dl_loss: 0.3876, step time: 0.4442\n",
      "batch: 11/17, train_dl_loss: 0.3367, train_bce_loss: 1.7755, train_bce_dl_loss: 0.3367, step time: 0.4485\n",
      "batch: 12/17, train_dl_loss: 0.3403, train_bce_loss: 1.7616, train_bce_dl_loss: 0.3403, step time: 0.4305\n",
      "batch: 13/17, train_dl_loss: 0.4312, train_bce_loss: 1.7799, train_bce_dl_loss: 0.4312, step time: 0.3829\n",
      "batch: 14/17, train_dl_loss: 0.3669, train_bce_loss: 1.7932, train_bce_dl_loss: 0.3669, step time: 0.4429\n",
      "batch: 15/17, train_dl_loss: 0.3571, train_bce_loss: 1.7635, train_bce_dl_loss: 0.3571, step time: 0.3855\n",
      "batch: 16/17, train_dl_loss: 0.3526, train_bce_loss: 1.7804, train_bce_dl_loss: 0.3526, step time: 0.4141\n",
      "batch: 17/17, train_dl_loss: 0.3190, train_bce_loss: 1.7746, train_bce_dl_loss: 0.3190, step time: 0.1146\n",
      "LOSS train DiceLoss: 0.3531, LOSS train BCE: 1.7670, LOSS train BCE-DiceLoss: 0.3531, LOSS val DiceLoss: 0.4797, LOSS val BCE: 1.7623, LOSS val BCE-DiceLoss: 0.4797, METRIC val: 0.5629\n",
      "time consuming of epoch 531 is: 433.3504\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3923, train_bce_loss: 1.7769, train_bce_dl_loss: 0.3923, step time: 0.4314\n",
      "batch: 1/17, train_dl_loss: 0.3562, train_bce_loss: 1.7621, train_bce_dl_loss: 0.3562, step time: 0.3791\n",
      "batch: 2/17, train_dl_loss: 0.3759, train_bce_loss: 1.7615, train_bce_dl_loss: 0.3759, step time: 0.4274\n",
      "batch: 3/17, train_dl_loss: 0.4125, train_bce_loss: 1.7512, train_bce_dl_loss: 0.4125, step time: 0.3753\n",
      "batch: 4/17, train_dl_loss: 0.3203, train_bce_loss: 1.7744, train_bce_dl_loss: 0.3203, step time: 0.4233\n",
      "batch: 5/17, train_dl_loss: 0.3441, train_bce_loss: 1.7624, train_bce_dl_loss: 0.3441, step time: 0.3832\n",
      "batch: 6/17, train_dl_loss: 0.3792, train_bce_loss: 1.7636, train_bce_dl_loss: 0.3792, step time: 0.4206\n",
      "batch: 7/17, train_dl_loss: 0.3317, train_bce_loss: 1.7449, train_bce_dl_loss: 0.3317, step time: 0.3768\n",
      "batch: 8/17, train_dl_loss: 0.3683, train_bce_loss: 1.7376, train_bce_dl_loss: 0.3683, step time: 0.4332\n",
      "batch: 9/17, train_dl_loss: 0.3295, train_bce_loss: 1.7960, train_bce_dl_loss: 0.3295, step time: 0.3862\n",
      "batch: 10/17, train_dl_loss: 0.4251, train_bce_loss: 1.7772, train_bce_dl_loss: 0.4251, step time: 0.4259\n",
      "batch: 11/17, train_dl_loss: 0.3375, train_bce_loss: 1.7774, train_bce_dl_loss: 0.3375, step time: 0.4282\n",
      "batch: 12/17, train_dl_loss: 0.3266, train_bce_loss: 1.7621, train_bce_dl_loss: 0.3266, step time: 0.4407\n",
      "batch: 13/17, train_dl_loss: 0.4331, train_bce_loss: 1.7844, train_bce_dl_loss: 0.4331, step time: 0.4488\n",
      "batch: 14/17, train_dl_loss: 0.3342, train_bce_loss: 1.7860, train_bce_dl_loss: 0.3342, step time: 0.4163\n",
      "batch: 15/17, train_dl_loss: 0.3540, train_bce_loss: 1.7666, train_bce_dl_loss: 0.3540, step time: 0.3846\n",
      "batch: 16/17, train_dl_loss: 0.3387, train_bce_loss: 1.7683, train_bce_dl_loss: 0.3387, step time: 0.4215\n",
      "batch: 17/17, train_dl_loss: 0.4330, train_bce_loss: 1.7886, train_bce_dl_loss: 0.4330, step time: 0.1123\n",
      "LOSS train DiceLoss: 0.3662, LOSS train BCE: 1.7690, LOSS train BCE-DiceLoss: 0.3662, LOSS val DiceLoss: 0.4781, LOSS val BCE: 1.7628, LOSS val BCE-DiceLoss: 0.4781, METRIC val: 0.5652\n",
      "time consuming of epoch 532 is: 419.5002\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3607, train_bce_loss: 1.7700, train_bce_dl_loss: 0.3607, step time: 0.4268\n",
      "batch: 1/17, train_dl_loss: 0.3276, train_bce_loss: 1.7882, train_bce_dl_loss: 0.3276, step time: 0.3767\n",
      "batch: 2/17, train_dl_loss: 0.3418, train_bce_loss: 1.7779, train_bce_dl_loss: 0.3418, step time: 0.4443\n",
      "batch: 3/17, train_dl_loss: 0.4159, train_bce_loss: 1.7737, train_bce_dl_loss: 0.4159, step time: 0.3971\n",
      "batch: 4/17, train_dl_loss: 0.3319, train_bce_loss: 1.7862, train_bce_dl_loss: 0.3319, step time: 0.4281\n",
      "batch: 5/17, train_dl_loss: 0.3216, train_bce_loss: 1.7608, train_bce_dl_loss: 0.3216, step time: 0.3848\n",
      "batch: 6/17, train_dl_loss: 0.4018, train_bce_loss: 1.7673, train_bce_dl_loss: 0.4018, step time: 0.4447\n",
      "batch: 7/17, train_dl_loss: 0.3265, train_bce_loss: 1.7557, train_bce_dl_loss: 0.3265, step time: 0.4487\n",
      "batch: 8/17, train_dl_loss: 0.3718, train_bce_loss: 1.7456, train_bce_dl_loss: 0.3718, step time: 0.4457\n",
      "batch: 9/17, train_dl_loss: 0.3690, train_bce_loss: 1.7602, train_bce_dl_loss: 0.3690, step time: 0.4302\n",
      "batch: 10/17, train_dl_loss: 0.3661, train_bce_loss: 1.7765, train_bce_dl_loss: 0.3661, step time: 0.4451\n",
      "batch: 11/17, train_dl_loss: 0.3472, train_bce_loss: 1.7471, train_bce_dl_loss: 0.3472, step time: 0.4368\n",
      "batch: 12/17, train_dl_loss: 0.3674, train_bce_loss: 1.7727, train_bce_dl_loss: 0.3674, step time: 0.4316\n",
      "batch: 13/17, train_dl_loss: 0.3975, train_bce_loss: 1.7721, train_bce_dl_loss: 0.3975, step time: 0.3864\n",
      "batch: 14/17, train_dl_loss: 0.3395, train_bce_loss: 1.7772, train_bce_dl_loss: 0.3395, step time: 0.4360\n",
      "batch: 15/17, train_dl_loss: 0.3743, train_bce_loss: 1.7677, train_bce_dl_loss: 0.3743, step time: 0.3780\n",
      "batch: 16/17, train_dl_loss: 0.3359, train_bce_loss: 1.7737, train_bce_dl_loss: 0.3359, step time: 0.4359\n",
      "batch: 17/17, train_dl_loss: 0.4256, train_bce_loss: 1.7888, train_bce_dl_loss: 0.4256, step time: 0.1135\n",
      "LOSS train DiceLoss: 0.3623, LOSS train BCE: 1.7701, LOSS train BCE-DiceLoss: 0.3623, LOSS val DiceLoss: 0.4774, LOSS val BCE: 1.7606, LOSS val BCE-DiceLoss: 0.4774, METRIC val: 0.5662\n",
      "time consuming of epoch 533 is: 436.1560\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3404, train_bce_loss: 1.7635, train_bce_dl_loss: 0.3404, step time: 0.4353\n",
      "batch: 1/17, train_dl_loss: 0.3710, train_bce_loss: 1.7888, train_bce_dl_loss: 0.3710, step time: 0.3807\n",
      "batch: 2/17, train_dl_loss: 0.3573, train_bce_loss: 1.7583, train_bce_dl_loss: 0.3573, step time: 0.4485\n",
      "batch: 3/17, train_dl_loss: 0.3880, train_bce_loss: 1.7668, train_bce_dl_loss: 0.3880, step time: 0.3741\n",
      "batch: 4/17, train_dl_loss: 0.3425, train_bce_loss: 1.7513, train_bce_dl_loss: 0.3425, step time: 0.4353\n",
      "batch: 5/17, train_dl_loss: 0.3402, train_bce_loss: 1.7497, train_bce_dl_loss: 0.3402, step time: 0.3873\n",
      "batch: 6/17, train_dl_loss: 0.3797, train_bce_loss: 1.7771, train_bce_dl_loss: 0.3797, step time: 0.4414\n",
      "batch: 7/17, train_dl_loss: 0.3337, train_bce_loss: 1.7546, train_bce_dl_loss: 0.3337, step time: 0.3908\n",
      "batch: 8/17, train_dl_loss: 0.3486, train_bce_loss: 1.7612, train_bce_dl_loss: 0.3486, step time: 0.4247\n",
      "batch: 9/17, train_dl_loss: 0.3672, train_bce_loss: 1.7649, train_bce_dl_loss: 0.3672, step time: 0.3903\n",
      "batch: 10/17, train_dl_loss: 0.3885, train_bce_loss: 1.7714, train_bce_dl_loss: 0.3885, step time: 0.4444\n",
      "batch: 11/17, train_dl_loss: 0.3515, train_bce_loss: 1.7367, train_bce_dl_loss: 0.3515, step time: 0.4441\n",
      "batch: 12/17, train_dl_loss: 0.3554, train_bce_loss: 1.7509, train_bce_dl_loss: 0.3554, step time: 0.4296\n",
      "batch: 13/17, train_dl_loss: 0.4416, train_bce_loss: 1.7664, train_bce_dl_loss: 0.4416, step time: 0.3879\n",
      "batch: 14/17, train_dl_loss: 0.3830, train_bce_loss: 1.7809, train_bce_dl_loss: 0.3830, step time: 0.4267\n",
      "batch: 15/17, train_dl_loss: 0.3527, train_bce_loss: 1.7578, train_bce_dl_loss: 0.3527, step time: 0.3850\n",
      "batch: 16/17, train_dl_loss: 0.3176, train_bce_loss: 1.7725, train_bce_dl_loss: 0.3176, step time: 0.4169\n",
      "batch: 17/17, train_dl_loss: 0.3511, train_bce_loss: 1.7586, train_bce_dl_loss: 0.3511, step time: 0.1119\n",
      "LOSS train DiceLoss: 0.3617, LOSS train BCE: 1.7629, LOSS train BCE-DiceLoss: 0.3617, LOSS val DiceLoss: 0.4774, LOSS val BCE: 1.7599, LOSS val BCE-DiceLoss: 0.4774, METRIC val: 0.5652\n",
      "time consuming of epoch 534 is: 408.2568\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3616, train_bce_loss: 1.7678, train_bce_dl_loss: 0.3616, step time: 0.4371\n",
      "batch: 1/17, train_dl_loss: 0.3811, train_bce_loss: 1.7627, train_bce_dl_loss: 0.3811, step time: 0.3837\n",
      "batch: 2/17, train_dl_loss: 0.3414, train_bce_loss: 1.7735, train_bce_dl_loss: 0.3414, step time: 0.4375\n",
      "batch: 3/17, train_dl_loss: 0.4010, train_bce_loss: 1.7747, train_bce_dl_loss: 0.4010, step time: 0.3800\n",
      "batch: 4/17, train_dl_loss: 0.3153, train_bce_loss: 1.7591, train_bce_dl_loss: 0.3153, step time: 0.4285\n",
      "batch: 5/17, train_dl_loss: 0.3419, train_bce_loss: 1.7478, train_bce_dl_loss: 0.3419, step time: 0.3807\n",
      "batch: 6/17, train_dl_loss: 0.3996, train_bce_loss: 1.7713, train_bce_dl_loss: 0.3996, step time: 0.4261\n",
      "batch: 7/17, train_dl_loss: 0.3375, train_bce_loss: 1.7748, train_bce_dl_loss: 0.3375, step time: 0.3857\n",
      "batch: 8/17, train_dl_loss: 0.3365, train_bce_loss: 1.7652, train_bce_dl_loss: 0.3365, step time: 0.4338\n",
      "batch: 9/17, train_dl_loss: 0.3355, train_bce_loss: 1.7797, train_bce_dl_loss: 0.3355, step time: 0.3922\n",
      "batch: 10/17, train_dl_loss: 0.4134, train_bce_loss: 1.7741, train_bce_dl_loss: 0.4134, step time: 0.4235\n",
      "batch: 11/17, train_dl_loss: 0.3954, train_bce_loss: 1.7575, train_bce_dl_loss: 0.3954, step time: 0.3821\n",
      "batch: 12/17, train_dl_loss: 0.3378, train_bce_loss: 1.7678, train_bce_dl_loss: 0.3378, step time: 0.4225\n",
      "batch: 13/17, train_dl_loss: 0.4236, train_bce_loss: 1.7643, train_bce_dl_loss: 0.4236, step time: 0.3783\n",
      "batch: 14/17, train_dl_loss: 0.3587, train_bce_loss: 1.7695, train_bce_dl_loss: 0.3587, step time: 0.4257\n",
      "batch: 15/17, train_dl_loss: 0.3615, train_bce_loss: 1.7708, train_bce_dl_loss: 0.3615, step time: 0.3870\n",
      "batch: 16/17, train_dl_loss: 0.3642, train_bce_loss: 1.7729, train_bce_dl_loss: 0.3642, step time: 0.4299\n",
      "batch: 17/17, train_dl_loss: 0.3112, train_bce_loss: 1.7635, train_bce_dl_loss: 0.3112, step time: 0.1139\n",
      "LOSS train DiceLoss: 0.3621, LOSS train BCE: 1.7676, LOSS train BCE-DiceLoss: 0.3621, LOSS val DiceLoss: 0.4786, LOSS val BCE: 1.7612, LOSS val BCE-DiceLoss: 0.4786, METRIC val: 0.5647\n",
      "time consuming of epoch 535 is: 472.7239\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3438, train_bce_loss: 1.7669, train_bce_dl_loss: 0.3438, step time: 0.4371\n",
      "batch: 1/17, train_dl_loss: 0.3527, train_bce_loss: 1.7870, train_bce_dl_loss: 0.3527, step time: 0.3820\n",
      "batch: 2/17, train_dl_loss: 0.3385, train_bce_loss: 1.7761, train_bce_dl_loss: 0.3385, step time: 0.4697\n",
      "batch: 3/17, train_dl_loss: 0.3933, train_bce_loss: 1.7597, train_bce_dl_loss: 0.3933, step time: 0.3744\n",
      "batch: 4/17, train_dl_loss: 0.3160, train_bce_loss: 1.7648, train_bce_dl_loss: 0.3160, step time: 0.4375\n",
      "batch: 5/17, train_dl_loss: 0.3301, train_bce_loss: 1.7692, train_bce_dl_loss: 0.3301, step time: 0.3960\n",
      "batch: 6/17, train_dl_loss: 0.3924, train_bce_loss: 1.7847, train_bce_dl_loss: 0.3924, step time: 0.4302\n",
      "batch: 7/17, train_dl_loss: 0.3303, train_bce_loss: 1.7669, train_bce_dl_loss: 0.3303, step time: 0.3864\n",
      "batch: 8/17, train_dl_loss: 0.3695, train_bce_loss: 1.7334, train_bce_dl_loss: 0.3695, step time: 0.4339\n",
      "batch: 9/17, train_dl_loss: 0.3430, train_bce_loss: 1.7720, train_bce_dl_loss: 0.3430, step time: 0.3849\n",
      "batch: 10/17, train_dl_loss: 0.3884, train_bce_loss: 1.7686, train_bce_dl_loss: 0.3884, step time: 0.4343\n",
      "batch: 11/17, train_dl_loss: 0.3514, train_bce_loss: 1.7465, train_bce_dl_loss: 0.3514, step time: 0.3879\n",
      "batch: 12/17, train_dl_loss: 0.3588, train_bce_loss: 1.7498, train_bce_dl_loss: 0.3588, step time: 0.4229\n",
      "batch: 13/17, train_dl_loss: 0.4282, train_bce_loss: 1.7700, train_bce_dl_loss: 0.4282, step time: 0.3849\n",
      "batch: 14/17, train_dl_loss: 0.3738, train_bce_loss: 1.7797, train_bce_dl_loss: 0.3738, step time: 0.4227\n",
      "batch: 15/17, train_dl_loss: 0.3178, train_bce_loss: 1.7677, train_bce_dl_loss: 0.3178, step time: 0.3783\n",
      "batch: 16/17, train_dl_loss: 0.3501, train_bce_loss: 1.7797, train_bce_dl_loss: 0.3501, step time: 0.4330\n",
      "batch: 17/17, train_dl_loss: 0.2944, train_bce_loss: 1.7844, train_bce_dl_loss: 0.2944, step time: 0.1130\n",
      "LOSS train DiceLoss: 0.3540, LOSS train BCE: 1.7682, LOSS train BCE-DiceLoss: 0.3540, LOSS val DiceLoss: 0.4775, LOSS val BCE: 1.7626, LOSS val BCE-DiceLoss: 0.4775, METRIC val: 0.5661\n",
      "time consuming of epoch 536 is: 454.8011\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3480, train_bce_loss: 1.7486, train_bce_dl_loss: 0.3480, step time: 0.4270\n",
      "batch: 1/17, train_dl_loss: 0.3323, train_bce_loss: 1.7723, train_bce_dl_loss: 0.3323, step time: 0.3768\n",
      "batch: 2/17, train_dl_loss: 0.3617, train_bce_loss: 1.7667, train_bce_dl_loss: 0.3617, step time: 0.4401\n",
      "batch: 3/17, train_dl_loss: 0.3886, train_bce_loss: 1.7576, train_bce_dl_loss: 0.3886, step time: 0.3837\n",
      "batch: 4/17, train_dl_loss: 0.3295, train_bce_loss: 1.7588, train_bce_dl_loss: 0.3295, step time: 0.4220\n",
      "batch: 5/17, train_dl_loss: 0.3441, train_bce_loss: 1.7593, train_bce_dl_loss: 0.3441, step time: 0.3843\n",
      "batch: 6/17, train_dl_loss: 0.3918, train_bce_loss: 1.7846, train_bce_dl_loss: 0.3918, step time: 0.4246\n",
      "batch: 7/17, train_dl_loss: 0.3262, train_bce_loss: 1.7746, train_bce_dl_loss: 0.3262, step time: 0.3953\n",
      "batch: 8/17, train_dl_loss: 0.3404, train_bce_loss: 1.7610, train_bce_dl_loss: 0.3404, step time: 0.4518\n",
      "batch: 9/17, train_dl_loss: 0.3611, train_bce_loss: 1.7644, train_bce_dl_loss: 0.3611, step time: 0.4061\n",
      "batch: 10/17, train_dl_loss: 0.4161, train_bce_loss: 1.7713, train_bce_dl_loss: 0.4161, step time: 0.4486\n",
      "batch: 11/17, train_dl_loss: 0.3234, train_bce_loss: 1.7391, train_bce_dl_loss: 0.3234, step time: 0.4281\n",
      "batch: 12/17, train_dl_loss: 0.3419, train_bce_loss: 1.7662, train_bce_dl_loss: 0.3419, step time: 0.4377\n",
      "batch: 13/17, train_dl_loss: 0.3798, train_bce_loss: 1.7685, train_bce_dl_loss: 0.3798, step time: 0.3834\n",
      "batch: 14/17, train_dl_loss: 0.3587, train_bce_loss: 1.7787, train_bce_dl_loss: 0.3587, step time: 0.4384\n",
      "batch: 15/17, train_dl_loss: 0.3363, train_bce_loss: 1.7732, train_bce_dl_loss: 0.3363, step time: 0.3851\n",
      "batch: 16/17, train_dl_loss: 0.3360, train_bce_loss: 1.7725, train_bce_dl_loss: 0.3360, step time: 0.4283\n",
      "batch: 17/17, train_dl_loss: 0.3019, train_bce_loss: 1.7855, train_bce_dl_loss: 0.3019, step time: 0.1121\n",
      "LOSS train DiceLoss: 0.3510, LOSS train BCE: 1.7668, LOSS train BCE-DiceLoss: 0.3510, LOSS val DiceLoss: 0.4791, LOSS val BCE: 1.7629, LOSS val BCE-DiceLoss: 0.4791, METRIC val: 0.5642\n",
      "time consuming of epoch 537 is: 427.4544\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3517, train_bce_loss: 1.7791, train_bce_dl_loss: 0.3517, step time: 0.4360\n",
      "batch: 1/17, train_dl_loss: 0.3805, train_bce_loss: 1.7746, train_bce_dl_loss: 0.3805, step time: 0.3847\n",
      "batch: 2/17, train_dl_loss: 0.3787, train_bce_loss: 1.7481, train_bce_dl_loss: 0.3787, step time: 0.4206\n",
      "batch: 3/17, train_dl_loss: 0.4403, train_bce_loss: 1.7662, train_bce_dl_loss: 0.4403, step time: 0.3815\n",
      "batch: 4/17, train_dl_loss: 0.3206, train_bce_loss: 1.7733, train_bce_dl_loss: 0.3206, step time: 0.4446\n",
      "batch: 5/17, train_dl_loss: 0.3140, train_bce_loss: 1.7610, train_bce_dl_loss: 0.3140, step time: 0.3870\n",
      "batch: 6/17, train_dl_loss: 0.3918, train_bce_loss: 1.7780, train_bce_dl_loss: 0.3918, step time: 0.4368\n",
      "batch: 7/17, train_dl_loss: 0.3232, train_bce_loss: 1.7522, train_bce_dl_loss: 0.3232, step time: 0.3823\n",
      "batch: 8/17, train_dl_loss: 0.3567, train_bce_loss: 1.7561, train_bce_dl_loss: 0.3567, step time: 0.4496\n",
      "batch: 9/17, train_dl_loss: 0.3439, train_bce_loss: 1.7708, train_bce_dl_loss: 0.3439, step time: 0.3854\n",
      "batch: 10/17, train_dl_loss: 0.3917, train_bce_loss: 1.7682, train_bce_dl_loss: 0.3917, step time: 0.4467\n",
      "batch: 11/17, train_dl_loss: 0.3986, train_bce_loss: 1.7567, train_bce_dl_loss: 0.3986, step time: 0.4313\n",
      "batch: 12/17, train_dl_loss: 0.3314, train_bce_loss: 1.7597, train_bce_dl_loss: 0.3314, step time: 0.4421\n",
      "batch: 13/17, train_dl_loss: 0.3943, train_bce_loss: 1.7887, train_bce_dl_loss: 0.3943, step time: 0.3773\n",
      "batch: 14/17, train_dl_loss: 0.3413, train_bce_loss: 1.7892, train_bce_dl_loss: 0.3413, step time: 0.4252\n",
      "batch: 15/17, train_dl_loss: 0.3375, train_bce_loss: 1.7609, train_bce_dl_loss: 0.3375, step time: 0.3805\n",
      "batch: 16/17, train_dl_loss: 0.3541, train_bce_loss: 1.7830, train_bce_dl_loss: 0.3541, step time: 0.4120\n",
      "batch: 17/17, train_dl_loss: 0.3100, train_bce_loss: 1.7621, train_bce_dl_loss: 0.3100, step time: 0.1123\n",
      "LOSS train DiceLoss: 0.3589, LOSS train BCE: 1.7682, LOSS train BCE-DiceLoss: 0.3589, LOSS val DiceLoss: 0.4773, LOSS val BCE: 1.7632, LOSS val BCE-DiceLoss: 0.4773, METRIC val: 0.5665\n",
      "time consuming of epoch 538 is: 438.5511\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3643, train_bce_loss: 1.7633, train_bce_dl_loss: 0.3643, step time: 0.4450\n",
      "batch: 1/17, train_dl_loss: 0.3696, train_bce_loss: 1.7605, train_bce_dl_loss: 0.3696, step time: 0.3721\n",
      "batch: 2/17, train_dl_loss: 0.3338, train_bce_loss: 1.7602, train_bce_dl_loss: 0.3338, step time: 0.4352\n",
      "batch: 3/17, train_dl_loss: 0.3847, train_bce_loss: 1.7612, train_bce_dl_loss: 0.3847, step time: 0.3757\n",
      "batch: 4/17, train_dl_loss: 0.3273, train_bce_loss: 1.7622, train_bce_dl_loss: 0.3273, step time: 0.4493\n",
      "batch: 5/17, train_dl_loss: 0.3228, train_bce_loss: 1.7741, train_bce_dl_loss: 0.3228, step time: 0.3883\n",
      "batch: 6/17, train_dl_loss: 0.4217, train_bce_loss: 1.7674, train_bce_dl_loss: 0.4217, step time: 0.4305\n",
      "batch: 7/17, train_dl_loss: 0.3516, train_bce_loss: 1.7770, train_bce_dl_loss: 0.3516, step time: 0.3873\n",
      "batch: 8/17, train_dl_loss: 0.3416, train_bce_loss: 1.7629, train_bce_dl_loss: 0.3416, step time: 0.4200\n",
      "batch: 9/17, train_dl_loss: 0.3310, train_bce_loss: 1.7790, train_bce_dl_loss: 0.3310, step time: 0.4607\n",
      "batch: 10/17, train_dl_loss: 0.3833, train_bce_loss: 1.7631, train_bce_dl_loss: 0.3833, step time: 0.4346\n",
      "batch: 11/17, train_dl_loss: 0.3519, train_bce_loss: 1.7674, train_bce_dl_loss: 0.3519, step time: 0.4476\n",
      "batch: 12/17, train_dl_loss: 0.3374, train_bce_loss: 1.7621, train_bce_dl_loss: 0.3374, step time: 0.4274\n",
      "batch: 13/17, train_dl_loss: 0.4418, train_bce_loss: 1.7743, train_bce_dl_loss: 0.4418, step time: 0.3947\n",
      "batch: 14/17, train_dl_loss: 0.3440, train_bce_loss: 1.7941, train_bce_dl_loss: 0.3440, step time: 0.4225\n",
      "batch: 15/17, train_dl_loss: 0.3523, train_bce_loss: 1.7792, train_bce_dl_loss: 0.3523, step time: 0.3816\n",
      "batch: 16/17, train_dl_loss: 0.3729, train_bce_loss: 1.7938, train_bce_dl_loss: 0.3729, step time: 0.4224\n",
      "batch: 17/17, train_dl_loss: 0.3132, train_bce_loss: 1.7887, train_bce_dl_loss: 0.3132, step time: 0.1134\n",
      "LOSS train DiceLoss: 0.3581, LOSS train BCE: 1.7717, LOSS train BCE-DiceLoss: 0.3581, LOSS val DiceLoss: 0.4760, LOSS val BCE: 1.7631, LOSS val BCE-DiceLoss: 0.4760, METRIC val: 0.5675\n",
      "time consuming of epoch 539 is: 435.7922\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3492, train_bce_loss: 1.7525, train_bce_dl_loss: 0.3492, step time: 0.4321\n",
      "batch: 1/17, train_dl_loss: 0.3480, train_bce_loss: 1.7830, train_bce_dl_loss: 0.3480, step time: 0.3774\n",
      "batch: 2/17, train_dl_loss: 0.3575, train_bce_loss: 1.7686, train_bce_dl_loss: 0.3575, step time: 0.4405\n",
      "batch: 3/17, train_dl_loss: 0.3999, train_bce_loss: 1.7614, train_bce_dl_loss: 0.3999, step time: 0.4015\n",
      "batch: 4/17, train_dl_loss: 0.3187, train_bce_loss: 1.7616, train_bce_dl_loss: 0.3187, step time: 0.4350\n",
      "batch: 5/17, train_dl_loss: 0.3370, train_bce_loss: 1.7844, train_bce_dl_loss: 0.3370, step time: 0.3896\n",
      "batch: 6/17, train_dl_loss: 0.4139, train_bce_loss: 1.7741, train_bce_dl_loss: 0.4139, step time: 0.4515\n",
      "batch: 7/17, train_dl_loss: 0.3309, train_bce_loss: 1.7596, train_bce_dl_loss: 0.3309, step time: 0.4330\n",
      "batch: 8/17, train_dl_loss: 0.3704, train_bce_loss: 1.7477, train_bce_dl_loss: 0.3704, step time: 0.4358\n",
      "batch: 9/17, train_dl_loss: 0.4060, train_bce_loss: 1.7895, train_bce_dl_loss: 0.4060, step time: 0.4252\n",
      "batch: 10/17, train_dl_loss: 0.3912, train_bce_loss: 1.7680, train_bce_dl_loss: 0.3912, step time: 0.4336\n",
      "batch: 11/17, train_dl_loss: 0.3408, train_bce_loss: 1.7411, train_bce_dl_loss: 0.3408, step time: 0.4418\n",
      "batch: 12/17, train_dl_loss: 0.3541, train_bce_loss: 1.7626, train_bce_dl_loss: 0.3541, step time: 0.4304\n",
      "batch: 13/17, train_dl_loss: 0.4054, train_bce_loss: 1.7882, train_bce_dl_loss: 0.4054, step time: 0.4393\n",
      "batch: 14/17, train_dl_loss: 0.3765, train_bce_loss: 1.8009, train_bce_dl_loss: 0.3765, step time: 0.4382\n",
      "batch: 15/17, train_dl_loss: 0.3464, train_bce_loss: 1.7902, train_bce_dl_loss: 0.3464, step time: 0.4474\n",
      "batch: 16/17, train_dl_loss: 0.3404, train_bce_loss: 1.7757, train_bce_dl_loss: 0.3404, step time: 0.4172\n",
      "batch: 17/17, train_dl_loss: 0.3569, train_bce_loss: 1.7642, train_bce_dl_loss: 0.3569, step time: 0.1125\n",
      "LOSS train DiceLoss: 0.3635, LOSS train BCE: 1.7707, LOSS train BCE-DiceLoss: 0.3635, LOSS val DiceLoss: 0.4774, LOSS val BCE: 1.7629, LOSS val BCE-DiceLoss: 0.4774, METRIC val: 0.5655\n",
      "time consuming of epoch 540 is: 434.9535\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3293, train_bce_loss: 1.7737, train_bce_dl_loss: 0.3293, step time: 0.4235\n",
      "batch: 1/17, train_dl_loss: 0.3562, train_bce_loss: 1.7586, train_bce_dl_loss: 0.3562, step time: 0.4154\n",
      "batch: 2/17, train_dl_loss: 0.3541, train_bce_loss: 1.7576, train_bce_dl_loss: 0.3541, step time: 0.4476\n",
      "batch: 3/17, train_dl_loss: 0.4000, train_bce_loss: 1.7649, train_bce_dl_loss: 0.4000, step time: 0.3816\n",
      "batch: 4/17, train_dl_loss: 0.3612, train_bce_loss: 1.7794, train_bce_dl_loss: 0.3612, step time: 0.4337\n",
      "batch: 5/17, train_dl_loss: 0.3337, train_bce_loss: 1.7696, train_bce_dl_loss: 0.3337, step time: 0.3867\n",
      "batch: 6/17, train_dl_loss: 0.3650, train_bce_loss: 1.7788, train_bce_dl_loss: 0.3650, step time: 0.4387\n",
      "batch: 7/17, train_dl_loss: 0.3453, train_bce_loss: 1.7590, train_bce_dl_loss: 0.3453, step time: 0.3786\n",
      "batch: 8/17, train_dl_loss: 0.3450, train_bce_loss: 1.7652, train_bce_dl_loss: 0.3450, step time: 0.4298\n",
      "batch: 9/17, train_dl_loss: 0.3407, train_bce_loss: 1.7765, train_bce_dl_loss: 0.3407, step time: 0.3812\n",
      "batch: 10/17, train_dl_loss: 0.3860, train_bce_loss: 1.7732, train_bce_dl_loss: 0.3860, step time: 0.4445\n",
      "batch: 11/17, train_dl_loss: 0.3353, train_bce_loss: 1.7696, train_bce_dl_loss: 0.3353, step time: 0.4406\n",
      "batch: 12/17, train_dl_loss: 0.3376, train_bce_loss: 1.7643, train_bce_dl_loss: 0.3376, step time: 0.4324\n",
      "batch: 13/17, train_dl_loss: 0.4001, train_bce_loss: 1.7730, train_bce_dl_loss: 0.4001, step time: 0.3935\n",
      "batch: 14/17, train_dl_loss: 0.3605, train_bce_loss: 1.7839, train_bce_dl_loss: 0.3605, step time: 0.4387\n",
      "batch: 15/17, train_dl_loss: 0.3697, train_bce_loss: 1.7659, train_bce_dl_loss: 0.3697, step time: 0.3921\n",
      "batch: 16/17, train_dl_loss: 0.3364, train_bce_loss: 1.7789, train_bce_dl_loss: 0.3364, step time: 0.4305\n",
      "batch: 17/17, train_dl_loss: 0.3521, train_bce_loss: 1.7624, train_bce_dl_loss: 0.3521, step time: 0.1148\n",
      "LOSS train DiceLoss: 0.3560, LOSS train BCE: 1.7697, LOSS train BCE-DiceLoss: 0.3560, LOSS val DiceLoss: 0.4773, LOSS val BCE: 1.7629, LOSS val BCE-DiceLoss: 0.4773, METRIC val: 0.5661\n",
      "time consuming of epoch 541 is: 489.6058\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3501, train_bce_loss: 1.7650, train_bce_dl_loss: 0.3501, step time: 0.4339\n",
      "batch: 1/17, train_dl_loss: 0.3475, train_bce_loss: 1.7699, train_bce_dl_loss: 0.3475, step time: 0.3806\n",
      "batch: 2/17, train_dl_loss: 0.3705, train_bce_loss: 1.7508, train_bce_dl_loss: 0.3705, step time: 0.4239\n",
      "batch: 3/17, train_dl_loss: 0.3876, train_bce_loss: 1.7681, train_bce_dl_loss: 0.3876, step time: 0.3769\n",
      "batch: 4/17, train_dl_loss: 0.3162, train_bce_loss: 1.7705, train_bce_dl_loss: 0.3162, step time: 0.4265\n",
      "batch: 5/17, train_dl_loss: 0.3439, train_bce_loss: 1.7515, train_bce_dl_loss: 0.3439, step time: 0.4259\n",
      "batch: 6/17, train_dl_loss: 0.4038, train_bce_loss: 1.7663, train_bce_dl_loss: 0.4038, step time: 0.4415\n",
      "batch: 7/17, train_dl_loss: 0.3445, train_bce_loss: 1.7473, train_bce_dl_loss: 0.3445, step time: 0.4261\n",
      "batch: 8/17, train_dl_loss: 0.3261, train_bce_loss: 1.7515, train_bce_dl_loss: 0.3261, step time: 0.4354\n",
      "batch: 9/17, train_dl_loss: 0.3401, train_bce_loss: 1.7651, train_bce_dl_loss: 0.3401, step time: 0.3863\n",
      "batch: 10/17, train_dl_loss: 0.3993, train_bce_loss: 1.7681, train_bce_dl_loss: 0.3993, step time: 0.4376\n",
      "batch: 11/17, train_dl_loss: 0.4020, train_bce_loss: 1.7768, train_bce_dl_loss: 0.4020, step time: 0.4461\n",
      "batch: 12/17, train_dl_loss: 0.3345, train_bce_loss: 1.7691, train_bce_dl_loss: 0.3345, step time: 0.4186\n",
      "batch: 13/17, train_dl_loss: 0.3851, train_bce_loss: 1.7733, train_bce_dl_loss: 0.3851, step time: 0.3773\n",
      "batch: 14/17, train_dl_loss: 0.3517, train_bce_loss: 1.7851, train_bce_dl_loss: 0.3517, step time: 0.4256\n",
      "batch: 15/17, train_dl_loss: 0.3404, train_bce_loss: 1.7803, train_bce_dl_loss: 0.3404, step time: 0.3790\n",
      "batch: 16/17, train_dl_loss: 0.3469, train_bce_loss: 1.7791, train_bce_dl_loss: 0.3469, step time: 0.4194\n",
      "batch: 17/17, train_dl_loss: 0.3027, train_bce_loss: 1.7761, train_bce_dl_loss: 0.3027, step time: 0.1125\n",
      "LOSS train DiceLoss: 0.3552, LOSS train BCE: 1.7674, LOSS train BCE-DiceLoss: 0.3552, LOSS val DiceLoss: 0.4774, LOSS val BCE: 1.7618, LOSS val BCE-DiceLoss: 0.4774, METRIC val: 0.5658\n",
      "time consuming of epoch 542 is: 559.3588\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3813, train_bce_loss: 1.7641, train_bce_dl_loss: 0.3813, step time: 0.4395\n",
      "batch: 1/17, train_dl_loss: 0.3548, train_bce_loss: 1.7786, train_bce_dl_loss: 0.3548, step time: 0.3749\n",
      "batch: 2/17, train_dl_loss: 0.3298, train_bce_loss: 1.7783, train_bce_dl_loss: 0.3298, step time: 0.4218\n",
      "batch: 3/17, train_dl_loss: 0.4361, train_bce_loss: 1.7740, train_bce_dl_loss: 0.4361, step time: 0.3747\n",
      "batch: 4/17, train_dl_loss: 0.3423, train_bce_loss: 1.7541, train_bce_dl_loss: 0.3423, step time: 0.4228\n",
      "batch: 5/17, train_dl_loss: 0.3184, train_bce_loss: 1.7660, train_bce_dl_loss: 0.3184, step time: 0.4470\n",
      "batch: 6/17, train_dl_loss: 0.3722, train_bce_loss: 1.7758, train_bce_dl_loss: 0.3722, step time: 0.4400\n",
      "batch: 7/17, train_dl_loss: 0.3299, train_bce_loss: 1.7610, train_bce_dl_loss: 0.3299, step time: 0.4304\n",
      "batch: 8/17, train_dl_loss: 0.3412, train_bce_loss: 1.7410, train_bce_dl_loss: 0.3412, step time: 0.4367\n",
      "batch: 9/17, train_dl_loss: 0.3458, train_bce_loss: 1.7775, train_bce_dl_loss: 0.3458, step time: 0.4371\n",
      "batch: 10/17, train_dl_loss: 0.3780, train_bce_loss: 1.7506, train_bce_dl_loss: 0.3780, step time: 0.4312\n",
      "batch: 11/17, train_dl_loss: 0.3533, train_bce_loss: 1.7437, train_bce_dl_loss: 0.3533, step time: 0.4312\n",
      "batch: 12/17, train_dl_loss: 0.3581, train_bce_loss: 1.7750, train_bce_dl_loss: 0.3581, step time: 0.4377\n",
      "batch: 13/17, train_dl_loss: 0.4061, train_bce_loss: 1.7746, train_bce_dl_loss: 0.4061, step time: 0.4266\n",
      "batch: 14/17, train_dl_loss: 0.3703, train_bce_loss: 1.7977, train_bce_dl_loss: 0.3703, step time: 0.4298\n",
      "batch: 15/17, train_dl_loss: 0.3307, train_bce_loss: 1.7803, train_bce_dl_loss: 0.3307, step time: 0.4269\n",
      "batch: 16/17, train_dl_loss: 0.3242, train_bce_loss: 1.7828, train_bce_dl_loss: 0.3242, step time: 0.4197\n",
      "batch: 17/17, train_dl_loss: 0.3106, train_bce_loss: 1.7786, train_bce_dl_loss: 0.3106, step time: 0.1147\n",
      "LOSS train DiceLoss: 0.3546, LOSS train BCE: 1.7697, LOSS train BCE-DiceLoss: 0.3546, LOSS val DiceLoss: 0.4775, LOSS val BCE: 1.7625, LOSS val BCE-DiceLoss: 0.4775, METRIC val: 0.5659\n",
      "time consuming of epoch 543 is: 563.4802\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3593, train_bce_loss: 1.7658, train_bce_dl_loss: 0.3593, step time: 0.4291\n",
      "batch: 1/17, train_dl_loss: 0.3449, train_bce_loss: 1.7753, train_bce_dl_loss: 0.3449, step time: 0.4325\n",
      "batch: 2/17, train_dl_loss: 0.3435, train_bce_loss: 1.7860, train_bce_dl_loss: 0.3435, step time: 0.4274\n",
      "batch: 3/17, train_dl_loss: 0.3822, train_bce_loss: 1.7668, train_bce_dl_loss: 0.3822, step time: 0.3890\n",
      "batch: 4/17, train_dl_loss: 0.3416, train_bce_loss: 1.7532, train_bce_dl_loss: 0.3416, step time: 0.4289\n",
      "batch: 5/17, train_dl_loss: 0.3150, train_bce_loss: 1.7625, train_bce_dl_loss: 0.3150, step time: 0.3863\n",
      "batch: 6/17, train_dl_loss: 0.3838, train_bce_loss: 1.7753, train_bce_dl_loss: 0.3838, step time: 0.4374\n",
      "batch: 7/17, train_dl_loss: 0.3228, train_bce_loss: 1.7718, train_bce_dl_loss: 0.3228, step time: 0.3768\n",
      "batch: 8/17, train_dl_loss: 0.3322, train_bce_loss: 1.7772, train_bce_dl_loss: 0.3322, step time: 0.4452\n",
      "batch: 9/17, train_dl_loss: 0.3596, train_bce_loss: 1.7741, train_bce_dl_loss: 0.3596, step time: 0.3815\n",
      "batch: 10/17, train_dl_loss: 0.3848, train_bce_loss: 1.7676, train_bce_dl_loss: 0.3848, step time: 0.4520\n",
      "batch: 11/17, train_dl_loss: 0.3515, train_bce_loss: 1.7390, train_bce_dl_loss: 0.3515, step time: 0.3909\n",
      "batch: 12/17, train_dl_loss: 0.3674, train_bce_loss: 1.7634, train_bce_dl_loss: 0.3674, step time: 0.4355\n",
      "batch: 13/17, train_dl_loss: 0.4587, train_bce_loss: 1.7636, train_bce_dl_loss: 0.4587, step time: 0.3749\n",
      "batch: 14/17, train_dl_loss: 0.3691, train_bce_loss: 1.7674, train_bce_dl_loss: 0.3691, step time: 0.4318\n",
      "batch: 15/17, train_dl_loss: 0.3708, train_bce_loss: 1.7758, train_bce_dl_loss: 0.3708, step time: 0.3787\n",
      "batch: 16/17, train_dl_loss: 0.3482, train_bce_loss: 1.7758, train_bce_dl_loss: 0.3482, step time: 0.4207\n",
      "batch: 17/17, train_dl_loss: 0.3023, train_bce_loss: 1.7631, train_bce_dl_loss: 0.3023, step time: 0.1193\n",
      "LOSS train DiceLoss: 0.3576, LOSS train BCE: 1.7680, LOSS train BCE-DiceLoss: 0.3576, LOSS val DiceLoss: 0.4787, LOSS val BCE: 1.7623, LOSS val BCE-DiceLoss: 0.4787, METRIC val: 0.5642\n",
      "time consuming of epoch 544 is: 423.2163\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3564, train_bce_loss: 1.7547, train_bce_dl_loss: 0.3564, step time: 0.4309\n",
      "batch: 1/17, train_dl_loss: 0.3433, train_bce_loss: 1.7646, train_bce_dl_loss: 0.3433, step time: 0.3792\n",
      "batch: 2/17, train_dl_loss: 0.3500, train_bce_loss: 1.7476, train_bce_dl_loss: 0.3500, step time: 0.4505\n",
      "batch: 3/17, train_dl_loss: 0.3779, train_bce_loss: 1.7614, train_bce_dl_loss: 0.3779, step time: 0.3851\n",
      "batch: 4/17, train_dl_loss: 0.3166, train_bce_loss: 1.7567, train_bce_dl_loss: 0.3166, step time: 0.4991\n",
      "batch: 5/17, train_dl_loss: 0.3278, train_bce_loss: 1.7601, train_bce_dl_loss: 0.3278, step time: 0.4325\n",
      "batch: 6/17, train_dl_loss: 0.4344, train_bce_loss: 1.7698, train_bce_dl_loss: 0.4344, step time: 0.4315\n",
      "batch: 7/17, train_dl_loss: 0.3276, train_bce_loss: 1.7597, train_bce_dl_loss: 0.3276, step time: 0.4418\n",
      "batch: 8/17, train_dl_loss: 0.3497, train_bce_loss: 1.7403, train_bce_dl_loss: 0.3497, step time: 0.4356\n",
      "batch: 9/17, train_dl_loss: 0.3400, train_bce_loss: 1.7668, train_bce_dl_loss: 0.3400, step time: 0.4187\n",
      "batch: 10/17, train_dl_loss: 0.4202, train_bce_loss: 1.7797, train_bce_dl_loss: 0.4202, step time: 0.4273\n",
      "batch: 11/17, train_dl_loss: 0.3331, train_bce_loss: 1.7599, train_bce_dl_loss: 0.3331, step time: 0.4350\n",
      "batch: 12/17, train_dl_loss: 0.3698, train_bce_loss: 1.7711, train_bce_dl_loss: 0.3698, step time: 0.4478\n",
      "batch: 13/17, train_dl_loss: 0.4236, train_bce_loss: 1.7812, train_bce_dl_loss: 0.4236, step time: 0.4330\n",
      "batch: 14/17, train_dl_loss: 0.3560, train_bce_loss: 1.7893, train_bce_dl_loss: 0.3560, step time: 0.4301\n",
      "batch: 15/17, train_dl_loss: 0.3566, train_bce_loss: 1.7585, train_bce_dl_loss: 0.3566, step time: 0.3852\n",
      "batch: 16/17, train_dl_loss: 0.3494, train_bce_loss: 1.7739, train_bce_dl_loss: 0.3494, step time: 0.4227\n",
      "batch: 17/17, train_dl_loss: 0.3508, train_bce_loss: 1.7620, train_bce_dl_loss: 0.3508, step time: 0.1133\n",
      "LOSS train DiceLoss: 0.3602, LOSS train BCE: 1.7643, LOSS train BCE-DiceLoss: 0.3602, LOSS val DiceLoss: 0.4791, LOSS val BCE: 1.7628, LOSS val BCE-DiceLoss: 0.4791, METRIC val: 0.5641\n",
      "time consuming of epoch 545 is: 407.7947\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3579, train_bce_loss: 1.7606, train_bce_dl_loss: 0.3579, step time: 0.4357\n",
      "batch: 1/17, train_dl_loss: 0.3399, train_bce_loss: 1.7770, train_bce_dl_loss: 0.3399, step time: 0.3746\n",
      "batch: 2/17, train_dl_loss: 0.3510, train_bce_loss: 1.7808, train_bce_dl_loss: 0.3510, step time: 0.4314\n",
      "batch: 3/17, train_dl_loss: 0.3992, train_bce_loss: 1.7691, train_bce_dl_loss: 0.3992, step time: 0.3826\n",
      "batch: 4/17, train_dl_loss: 0.3321, train_bce_loss: 1.7653, train_bce_dl_loss: 0.3321, step time: 0.4260\n",
      "batch: 5/17, train_dl_loss: 0.3319, train_bce_loss: 1.7499, train_bce_dl_loss: 0.3319, step time: 0.3805\n",
      "batch: 6/17, train_dl_loss: 0.3707, train_bce_loss: 1.7680, train_bce_dl_loss: 0.3707, step time: 0.4498\n",
      "batch: 7/17, train_dl_loss: 0.3343, train_bce_loss: 1.7471, train_bce_dl_loss: 0.3343, step time: 0.3861\n",
      "batch: 8/17, train_dl_loss: 0.3402, train_bce_loss: 1.7614, train_bce_dl_loss: 0.3402, step time: 0.4503\n",
      "batch: 9/17, train_dl_loss: 0.3463, train_bce_loss: 1.7839, train_bce_dl_loss: 0.3463, step time: 0.3965\n",
      "batch: 10/17, train_dl_loss: 0.3898, train_bce_loss: 1.7560, train_bce_dl_loss: 0.3898, step time: 0.4415\n",
      "batch: 11/17, train_dl_loss: 0.3887, train_bce_loss: 1.7536, train_bce_dl_loss: 0.3887, step time: 0.3855\n",
      "batch: 12/17, train_dl_loss: 0.3395, train_bce_loss: 1.7642, train_bce_dl_loss: 0.3395, step time: 0.4401\n",
      "batch: 13/17, train_dl_loss: 0.4277, train_bce_loss: 1.7760, train_bce_dl_loss: 0.4277, step time: 0.3820\n",
      "batch: 14/17, train_dl_loss: 0.3723, train_bce_loss: 1.7717, train_bce_dl_loss: 0.3723, step time: 0.4349\n",
      "batch: 15/17, train_dl_loss: 0.3308, train_bce_loss: 1.7747, train_bce_dl_loss: 0.3308, step time: 0.3793\n",
      "batch: 16/17, train_dl_loss: 0.3212, train_bce_loss: 1.7843, train_bce_dl_loss: 0.3212, step time: 0.4156\n",
      "batch: 17/17, train_dl_loss: 0.3299, train_bce_loss: 1.7919, train_bce_dl_loss: 0.3299, step time: 0.1126\n",
      "LOSS train DiceLoss: 0.3557, LOSS train BCE: 1.7686, LOSS train BCE-DiceLoss: 0.3557, LOSS val DiceLoss: 0.4772, LOSS val BCE: 1.7632, LOSS val BCE-DiceLoss: 0.4772, METRIC val: 0.5665\n",
      "time consuming of epoch 546 is: 440.2090\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3382, train_bce_loss: 1.7436, train_bce_dl_loss: 0.3382, step time: 0.4288\n",
      "batch: 1/17, train_dl_loss: 0.3452, train_bce_loss: 1.7646, train_bce_dl_loss: 0.3452, step time: 0.3870\n",
      "batch: 2/17, train_dl_loss: 0.3454, train_bce_loss: 1.7728, train_bce_dl_loss: 0.3454, step time: 0.4515\n",
      "batch: 3/17, train_dl_loss: 0.3857, train_bce_loss: 1.7849, train_bce_dl_loss: 0.3857, step time: 0.4434\n",
      "batch: 4/17, train_dl_loss: 0.3269, train_bce_loss: 1.7633, train_bce_dl_loss: 0.3269, step time: 0.4500\n",
      "batch: 5/17, train_dl_loss: 0.3316, train_bce_loss: 1.7810, train_bce_dl_loss: 0.3316, step time: 0.3865\n",
      "batch: 6/17, train_dl_loss: 0.3659, train_bce_loss: 1.7874, train_bce_dl_loss: 0.3659, step time: 0.4363\n",
      "batch: 7/17, train_dl_loss: 0.3630, train_bce_loss: 1.7439, train_bce_dl_loss: 0.3630, step time: 0.3808\n",
      "batch: 8/17, train_dl_loss: 0.3356, train_bce_loss: 1.7569, train_bce_dl_loss: 0.3356, step time: 0.4372\n",
      "batch: 9/17, train_dl_loss: 0.3580, train_bce_loss: 1.7720, train_bce_dl_loss: 0.3580, step time: 0.3909\n",
      "batch: 10/17, train_dl_loss: 0.3842, train_bce_loss: 1.7506, train_bce_dl_loss: 0.3842, step time: 0.4551\n",
      "batch: 11/17, train_dl_loss: 0.3223, train_bce_loss: 1.7900, train_bce_dl_loss: 0.3223, step time: 0.4344\n",
      "batch: 12/17, train_dl_loss: 0.3840, train_bce_loss: 1.7849, train_bce_dl_loss: 0.3840, step time: 0.4274\n",
      "batch: 13/17, train_dl_loss: 0.3702, train_bce_loss: 1.7843, train_bce_dl_loss: 0.3702, step time: 0.3941\n",
      "batch: 14/17, train_dl_loss: 0.3611, train_bce_loss: 1.7803, train_bce_dl_loss: 0.3611, step time: 0.4340\n",
      "batch: 15/17, train_dl_loss: 0.3293, train_bce_loss: 1.7682, train_bce_dl_loss: 0.3293, step time: 0.3817\n",
      "batch: 16/17, train_dl_loss: 0.3585, train_bce_loss: 1.7703, train_bce_dl_loss: 0.3585, step time: 0.4312\n",
      "batch: 17/17, train_dl_loss: 0.3044, train_bce_loss: 1.7626, train_bce_dl_loss: 0.3044, step time: 0.1132\n",
      "LOSS train DiceLoss: 0.3505, LOSS train BCE: 1.7701, LOSS train BCE-DiceLoss: 0.3505, LOSS val DiceLoss: 0.4776, LOSS val BCE: 1.7634, LOSS val BCE-DiceLoss: 0.4776, METRIC val: 0.5660\n",
      "time consuming of epoch 547 is: 431.5106\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3509, train_bce_loss: 1.7671, train_bce_dl_loss: 0.3509, step time: 0.4418\n",
      "batch: 1/17, train_dl_loss: 0.3577, train_bce_loss: 1.7755, train_bce_dl_loss: 0.3577, step time: 0.3843\n",
      "batch: 2/17, train_dl_loss: 0.3524, train_bce_loss: 1.7617, train_bce_dl_loss: 0.3524, step time: 0.4223\n",
      "batch: 3/17, train_dl_loss: 0.4770, train_bce_loss: 1.7896, train_bce_dl_loss: 0.4770, step time: 0.3809\n",
      "batch: 4/17, train_dl_loss: 0.3128, train_bce_loss: 1.7626, train_bce_dl_loss: 0.3128, step time: 0.4395\n",
      "batch: 5/17, train_dl_loss: 0.3731, train_bce_loss: 1.7741, train_bce_dl_loss: 0.3731, step time: 0.3929\n",
      "batch: 6/17, train_dl_loss: 0.3908, train_bce_loss: 1.7703, train_bce_dl_loss: 0.3908, step time: 0.4279\n",
      "batch: 7/17, train_dl_loss: 0.3478, train_bce_loss: 1.7539, train_bce_dl_loss: 0.3478, step time: 0.3864\n",
      "batch: 8/17, train_dl_loss: 0.4026, train_bce_loss: 1.7379, train_bce_dl_loss: 0.4026, step time: 0.4429\n",
      "batch: 9/17, train_dl_loss: 0.3473, train_bce_loss: 1.7764, train_bce_dl_loss: 0.3473, step time: 0.4361\n",
      "batch: 10/17, train_dl_loss: 0.3994, train_bce_loss: 1.7571, train_bce_dl_loss: 0.3994, step time: 0.4433\n",
      "batch: 11/17, train_dl_loss: 0.3513, train_bce_loss: 1.7354, train_bce_dl_loss: 0.3513, step time: 0.4365\n",
      "batch: 12/17, train_dl_loss: 0.3434, train_bce_loss: 1.7691, train_bce_dl_loss: 0.3434, step time: 0.4392\n",
      "batch: 13/17, train_dl_loss: 0.4039, train_bce_loss: 1.7695, train_bce_dl_loss: 0.4039, step time: 0.4598\n",
      "batch: 14/17, train_dl_loss: 0.3490, train_bce_loss: 1.7768, train_bce_dl_loss: 0.3490, step time: 0.4422\n",
      "batch: 15/17, train_dl_loss: 0.3690, train_bce_loss: 1.7775, train_bce_dl_loss: 0.3690, step time: 0.4341\n",
      "batch: 16/17, train_dl_loss: 0.3473, train_bce_loss: 1.7875, train_bce_dl_loss: 0.3473, step time: 0.4142\n",
      "batch: 17/17, train_dl_loss: 0.3085, train_bce_loss: 1.7648, train_bce_dl_loss: 0.3085, step time: 0.1122\n",
      "LOSS train DiceLoss: 0.3658, LOSS train BCE: 1.7670, LOSS train BCE-DiceLoss: 0.3658, LOSS val DiceLoss: 0.4779, LOSS val BCE: 1.7632, LOSS val BCE-DiceLoss: 0.4779, METRIC val: 0.5658\n",
      "time consuming of epoch 548 is: 438.2586\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3541, train_bce_loss: 1.7555, train_bce_dl_loss: 0.3541, step time: 0.4341\n",
      "batch: 1/17, train_dl_loss: 0.3706, train_bce_loss: 1.7691, train_bce_dl_loss: 0.3706, step time: 0.3825\n",
      "batch: 2/17, train_dl_loss: 0.3739, train_bce_loss: 1.7675, train_bce_dl_loss: 0.3739, step time: 0.4377\n",
      "batch: 3/17, train_dl_loss: 0.3908, train_bce_loss: 1.7568, train_bce_dl_loss: 0.3908, step time: 0.3890\n",
      "batch: 4/17, train_dl_loss: 0.3279, train_bce_loss: 1.7630, train_bce_dl_loss: 0.3279, step time: 0.4253\n",
      "batch: 5/17, train_dl_loss: 0.3568, train_bce_loss: 1.7592, train_bce_dl_loss: 0.3568, step time: 0.3811\n",
      "batch: 6/17, train_dl_loss: 0.3918, train_bce_loss: 1.7707, train_bce_dl_loss: 0.3918, step time: 0.4475\n",
      "batch: 7/17, train_dl_loss: 0.3368, train_bce_loss: 1.7572, train_bce_dl_loss: 0.3368, step time: 0.3841\n",
      "batch: 8/17, train_dl_loss: 0.3367, train_bce_loss: 1.7682, train_bce_dl_loss: 0.3367, step time: 0.4220\n",
      "batch: 9/17, train_dl_loss: 0.3349, train_bce_loss: 1.7785, train_bce_dl_loss: 0.3349, step time: 0.4761\n",
      "batch: 10/17, train_dl_loss: 0.4058, train_bce_loss: 1.7643, train_bce_dl_loss: 0.4058, step time: 0.4273\n",
      "batch: 11/17, train_dl_loss: 0.3717, train_bce_loss: 1.7800, train_bce_dl_loss: 0.3717, step time: 0.4284\n",
      "batch: 12/17, train_dl_loss: 0.3634, train_bce_loss: 1.7673, train_bce_dl_loss: 0.3634, step time: 0.4340\n",
      "batch: 13/17, train_dl_loss: 0.4098, train_bce_loss: 1.7785, train_bce_dl_loss: 0.4098, step time: 0.3932\n",
      "batch: 14/17, train_dl_loss: 0.3497, train_bce_loss: 1.7724, train_bce_dl_loss: 0.3497, step time: 0.4405\n",
      "batch: 15/17, train_dl_loss: 0.3649, train_bce_loss: 1.7760, train_bce_dl_loss: 0.3649, step time: 0.3830\n",
      "batch: 16/17, train_dl_loss: 0.3608, train_bce_loss: 1.7692, train_bce_dl_loss: 0.3608, step time: 0.4318\n",
      "batch: 17/17, train_dl_loss: 0.3032, train_bce_loss: 1.7631, train_bce_dl_loss: 0.3032, step time: 0.1132\n",
      "LOSS train DiceLoss: 0.3613, LOSS train BCE: 1.7676, LOSS train BCE-DiceLoss: 0.3613, LOSS val DiceLoss: 0.4773, LOSS val BCE: 1.7632, LOSS val BCE-DiceLoss: 0.4773, METRIC val: 0.5664\n",
      "time consuming of epoch 549 is: 449.2507\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3307, train_bce_loss: 1.7764, train_bce_dl_loss: 0.3307, step time: 0.4326\n",
      "batch: 1/17, train_dl_loss: 0.3859, train_bce_loss: 1.7771, train_bce_dl_loss: 0.3859, step time: 0.3880\n",
      "batch: 2/17, train_dl_loss: 0.3568, train_bce_loss: 1.7793, train_bce_dl_loss: 0.3568, step time: 0.4377\n",
      "batch: 3/17, train_dl_loss: 0.3906, train_bce_loss: 1.7695, train_bce_dl_loss: 0.3906, step time: 0.3811\n",
      "batch: 4/17, train_dl_loss: 0.3331, train_bce_loss: 1.7614, train_bce_dl_loss: 0.3331, step time: 0.4544\n",
      "batch: 5/17, train_dl_loss: 0.3376, train_bce_loss: 1.7826, train_bce_dl_loss: 0.3376, step time: 0.4405\n",
      "batch: 6/17, train_dl_loss: 0.3663, train_bce_loss: 1.7703, train_bce_dl_loss: 0.3663, step time: 0.4404\n",
      "batch: 7/17, train_dl_loss: 0.3292, train_bce_loss: 1.7478, train_bce_dl_loss: 0.3292, step time: 0.4495\n",
      "batch: 8/17, train_dl_loss: 0.3573, train_bce_loss: 1.7365, train_bce_dl_loss: 0.3573, step time: 0.4232\n",
      "batch: 9/17, train_dl_loss: 0.3525, train_bce_loss: 1.7877, train_bce_dl_loss: 0.3525, step time: 0.4377\n",
      "batch: 10/17, train_dl_loss: 0.3617, train_bce_loss: 1.7791, train_bce_dl_loss: 0.3617, step time: 0.4392\n",
      "batch: 11/17, train_dl_loss: 0.3292, train_bce_loss: 1.7817, train_bce_dl_loss: 0.3292, step time: 0.4423\n",
      "batch: 12/17, train_dl_loss: 0.3230, train_bce_loss: 1.7591, train_bce_dl_loss: 0.3230, step time: 0.4361\n",
      "batch: 13/17, train_dl_loss: 0.4039, train_bce_loss: 1.7778, train_bce_dl_loss: 0.4039, step time: 0.4410\n",
      "batch: 14/17, train_dl_loss: 0.3544, train_bce_loss: 1.7777, train_bce_dl_loss: 0.3544, step time: 0.4362\n",
      "batch: 15/17, train_dl_loss: 0.3254, train_bce_loss: 1.7841, train_bce_dl_loss: 0.3254, step time: 0.3830\n",
      "batch: 16/17, train_dl_loss: 0.3482, train_bce_loss: 1.7703, train_bce_dl_loss: 0.3482, step time: 0.4249\n",
      "batch: 17/17, train_dl_loss: 0.3035, train_bce_loss: 1.7827, train_bce_dl_loss: 0.3035, step time: 0.1126\n",
      "LOSS train DiceLoss: 0.3494, LOSS train BCE: 1.7723, LOSS train BCE-DiceLoss: 0.3494, LOSS val DiceLoss: 0.4775, LOSS val BCE: 1.7631, LOSS val BCE-DiceLoss: 0.4775, METRIC val: 0.5659\n",
      "time consuming of epoch 550 is: 455.3081\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3534, train_bce_loss: 1.7540, train_bce_dl_loss: 0.3534, step time: 0.4271\n",
      "batch: 1/17, train_dl_loss: 0.3298, train_bce_loss: 1.7765, train_bce_dl_loss: 0.3298, step time: 0.3843\n",
      "batch: 2/17, train_dl_loss: 0.3253, train_bce_loss: 1.7634, train_bce_dl_loss: 0.3253, step time: 0.4267\n",
      "batch: 3/17, train_dl_loss: 0.4114, train_bce_loss: 1.7783, train_bce_dl_loss: 0.4114, step time: 0.3849\n",
      "batch: 4/17, train_dl_loss: 0.3239, train_bce_loss: 1.7717, train_bce_dl_loss: 0.3239, step time: 0.4191\n",
      "batch: 5/17, train_dl_loss: 0.3361, train_bce_loss: 1.7929, train_bce_dl_loss: 0.3361, step time: 0.4348\n",
      "batch: 6/17, train_dl_loss: 0.3945, train_bce_loss: 1.7917, train_bce_dl_loss: 0.3945, step time: 0.4449\n",
      "batch: 7/17, train_dl_loss: 0.3434, train_bce_loss: 1.7730, train_bce_dl_loss: 0.3434, step time: 0.3848\n",
      "batch: 8/17, train_dl_loss: 0.3506, train_bce_loss: 1.7493, train_bce_dl_loss: 0.3506, step time: 0.4328\n",
      "batch: 9/17, train_dl_loss: 0.4055, train_bce_loss: 1.7939, train_bce_dl_loss: 0.4055, step time: 0.4414\n",
      "batch: 10/17, train_dl_loss: 0.3964, train_bce_loss: 1.7534, train_bce_dl_loss: 0.3964, step time: 0.4344\n",
      "batch: 11/17, train_dl_loss: 0.3428, train_bce_loss: 1.7598, train_bce_dl_loss: 0.3428, step time: 0.4466\n",
      "batch: 12/17, train_dl_loss: 0.3287, train_bce_loss: 1.7578, train_bce_dl_loss: 0.3287, step time: 0.4436\n",
      "batch: 13/17, train_dl_loss: 0.4253, train_bce_loss: 1.7891, train_bce_dl_loss: 0.4253, step time: 0.3868\n",
      "batch: 14/17, train_dl_loss: 0.3535, train_bce_loss: 1.7728, train_bce_dl_loss: 0.3535, step time: 0.4318\n",
      "batch: 15/17, train_dl_loss: 0.3540, train_bce_loss: 1.7758, train_bce_dl_loss: 0.3540, step time: 0.3775\n",
      "batch: 16/17, train_dl_loss: 0.3302, train_bce_loss: 1.7855, train_bce_dl_loss: 0.3302, step time: 0.4251\n",
      "batch: 17/17, train_dl_loss: 0.2970, train_bce_loss: 1.7759, train_bce_dl_loss: 0.2970, step time: 0.1128\n",
      "LOSS train DiceLoss: 0.3557, LOSS train BCE: 1.7730, LOSS train BCE-DiceLoss: 0.3557, LOSS val DiceLoss: 0.4779, LOSS val BCE: 1.7632, LOSS val BCE-DiceLoss: 0.4779, METRIC val: 0.5654\n",
      "time consuming of epoch 551 is: 459.1831\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3300, train_bce_loss: 1.7777, train_bce_dl_loss: 0.3300, step time: 0.4312\n",
      "batch: 1/17, train_dl_loss: 0.3550, train_bce_loss: 1.7684, train_bce_dl_loss: 0.3550, step time: 0.3859\n",
      "batch: 2/17, train_dl_loss: 0.3467, train_bce_loss: 1.7759, train_bce_dl_loss: 0.3467, step time: 0.4428\n",
      "batch: 3/17, train_dl_loss: 0.3799, train_bce_loss: 1.7675, train_bce_dl_loss: 0.3799, step time: 0.3809\n",
      "batch: 4/17, train_dl_loss: 0.3323, train_bce_loss: 1.7719, train_bce_dl_loss: 0.3323, step time: 0.4292\n",
      "batch: 5/17, train_dl_loss: 0.3476, train_bce_loss: 1.7519, train_bce_dl_loss: 0.3476, step time: 0.3823\n",
      "batch: 6/17, train_dl_loss: 0.3805, train_bce_loss: 1.7795, train_bce_dl_loss: 0.3805, step time: 0.4356\n",
      "batch: 7/17, train_dl_loss: 0.3487, train_bce_loss: 1.7674, train_bce_dl_loss: 0.3487, step time: 0.3862\n",
      "batch: 8/17, train_dl_loss: 0.3271, train_bce_loss: 1.7486, train_bce_dl_loss: 0.3271, step time: 0.4249\n",
      "batch: 9/17, train_dl_loss: 0.3344, train_bce_loss: 1.7797, train_bce_dl_loss: 0.3344, step time: 0.3764\n",
      "batch: 10/17, train_dl_loss: 0.3596, train_bce_loss: 1.7636, train_bce_dl_loss: 0.3596, step time: 0.4350\n",
      "batch: 11/17, train_dl_loss: 0.3891, train_bce_loss: 1.7687, train_bce_dl_loss: 0.3891, step time: 0.4412\n",
      "batch: 12/17, train_dl_loss: 0.3931, train_bce_loss: 1.7711, train_bce_dl_loss: 0.3931, step time: 0.4236\n",
      "batch: 13/17, train_dl_loss: 0.4175, train_bce_loss: 1.7662, train_bce_dl_loss: 0.4175, step time: 0.3833\n",
      "batch: 14/17, train_dl_loss: 0.3602, train_bce_loss: 1.7832, train_bce_dl_loss: 0.3602, step time: 0.4292\n",
      "batch: 15/17, train_dl_loss: 0.3452, train_bce_loss: 1.7714, train_bce_dl_loss: 0.3452, step time: 0.3801\n",
      "batch: 16/17, train_dl_loss: 0.3495, train_bce_loss: 1.7770, train_bce_dl_loss: 0.3495, step time: 0.4367\n",
      "batch: 17/17, train_dl_loss: 0.3527, train_bce_loss: 1.7630, train_bce_dl_loss: 0.3527, step time: 0.1129\n",
      "LOSS train DiceLoss: 0.3583, LOSS train BCE: 1.7696, LOSS train BCE-DiceLoss: 0.3583, LOSS val DiceLoss: 0.4777, LOSS val BCE: 1.7635, LOSS val BCE-DiceLoss: 0.4777, METRIC val: 0.5657\n",
      "time consuming of epoch 552 is: 445.0563\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3659, train_bce_loss: 1.7562, train_bce_dl_loss: 0.3659, step time: 0.4438\n",
      "batch: 1/17, train_dl_loss: 0.3326, train_bce_loss: 1.7754, train_bce_dl_loss: 0.3326, step time: 0.3847\n",
      "batch: 2/17, train_dl_loss: 0.3657, train_bce_loss: 1.7591, train_bce_dl_loss: 0.3657, step time: 0.4548\n",
      "batch: 3/17, train_dl_loss: 0.3740, train_bce_loss: 1.7740, train_bce_dl_loss: 0.3740, step time: 0.4100\n",
      "batch: 4/17, train_dl_loss: 0.3148, train_bce_loss: 1.7775, train_bce_dl_loss: 0.3148, step time: 0.4337\n",
      "batch: 5/17, train_dl_loss: 0.3435, train_bce_loss: 1.7697, train_bce_dl_loss: 0.3435, step time: 0.3856\n",
      "batch: 6/17, train_dl_loss: 0.3756, train_bce_loss: 1.7743, train_bce_dl_loss: 0.3756, step time: 0.4258\n",
      "batch: 7/17, train_dl_loss: 0.3239, train_bce_loss: 1.7505, train_bce_dl_loss: 0.3239, step time: 0.3845\n",
      "batch: 8/17, train_dl_loss: 0.3312, train_bce_loss: 1.7772, train_bce_dl_loss: 0.3312, step time: 0.4418\n",
      "batch: 9/17, train_dl_loss: 0.3521, train_bce_loss: 1.7928, train_bce_dl_loss: 0.3521, step time: 0.4288\n",
      "batch: 10/17, train_dl_loss: 0.3806, train_bce_loss: 1.7757, train_bce_dl_loss: 0.3806, step time: 0.4331\n",
      "batch: 11/17, train_dl_loss: 0.3317, train_bce_loss: 1.7768, train_bce_dl_loss: 0.3317, step time: 0.4252\n",
      "batch: 12/17, train_dl_loss: 0.3466, train_bce_loss: 1.7517, train_bce_dl_loss: 0.3466, step time: 0.4378\n",
      "batch: 13/17, train_dl_loss: 0.3819, train_bce_loss: 1.7732, train_bce_dl_loss: 0.3819, step time: 0.4414\n",
      "batch: 14/17, train_dl_loss: 0.3634, train_bce_loss: 1.7751, train_bce_dl_loss: 0.3634, step time: 0.4193\n",
      "batch: 15/17, train_dl_loss: 0.3347, train_bce_loss: 1.7731, train_bce_dl_loss: 0.3347, step time: 0.4269\n",
      "batch: 16/17, train_dl_loss: 0.3366, train_bce_loss: 1.7814, train_bce_dl_loss: 0.3366, step time: 0.4230\n",
      "batch: 17/17, train_dl_loss: 0.3525, train_bce_loss: 1.7630, train_bce_dl_loss: 0.3525, step time: 0.1123\n",
      "LOSS train DiceLoss: 0.3504, LOSS train BCE: 1.7709, LOSS train BCE-DiceLoss: 0.3504, LOSS val DiceLoss: 0.4774, LOSS val BCE: 1.7635, LOSS val BCE-DiceLoss: 0.4774, METRIC val: 0.5661\n",
      "time consuming of epoch 553 is: 423.2944\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3925, train_bce_loss: 1.7612, train_bce_dl_loss: 0.3925, step time: 0.4280\n",
      "batch: 1/17, train_dl_loss: 0.3378, train_bce_loss: 1.7702, train_bce_dl_loss: 0.3378, step time: 0.3753\n",
      "batch: 2/17, train_dl_loss: 0.3749, train_bce_loss: 1.7689, train_bce_dl_loss: 0.3749, step time: 0.4259\n",
      "batch: 3/17, train_dl_loss: 0.3867, train_bce_loss: 1.7815, train_bce_dl_loss: 0.3867, step time: 0.3914\n",
      "batch: 4/17, train_dl_loss: 0.3359, train_bce_loss: 1.7790, train_bce_dl_loss: 0.3359, step time: 0.4299\n",
      "batch: 5/17, train_dl_loss: 0.3428, train_bce_loss: 1.7919, train_bce_dl_loss: 0.3428, step time: 0.4294\n",
      "batch: 6/17, train_dl_loss: 0.4117, train_bce_loss: 1.7836, train_bce_dl_loss: 0.4117, step time: 0.4508\n",
      "batch: 7/17, train_dl_loss: 0.3397, train_bce_loss: 1.7443, train_bce_dl_loss: 0.3397, step time: 0.4409\n",
      "batch: 8/17, train_dl_loss: 0.3269, train_bce_loss: 1.7764, train_bce_dl_loss: 0.3269, step time: 0.4181\n",
      "batch: 9/17, train_dl_loss: 0.3550, train_bce_loss: 1.7859, train_bce_dl_loss: 0.3550, step time: 0.3900\n",
      "batch: 10/17, train_dl_loss: 0.4204, train_bce_loss: 1.7749, train_bce_dl_loss: 0.4204, step time: 0.4466\n",
      "batch: 11/17, train_dl_loss: 0.3481, train_bce_loss: 1.7375, train_bce_dl_loss: 0.3481, step time: 0.4204\n",
      "batch: 12/17, train_dl_loss: 0.3252, train_bce_loss: 1.7572, train_bce_dl_loss: 0.3252, step time: 0.4405\n",
      "batch: 13/17, train_dl_loss: 0.4113, train_bce_loss: 1.7711, train_bce_dl_loss: 0.4113, step time: 0.3962\n",
      "batch: 14/17, train_dl_loss: 0.3810, train_bce_loss: 1.7994, train_bce_dl_loss: 0.3810, step time: 0.4394\n",
      "batch: 15/17, train_dl_loss: 0.3369, train_bce_loss: 1.7825, train_bce_dl_loss: 0.3369, step time: 0.3788\n",
      "batch: 16/17, train_dl_loss: 0.3605, train_bce_loss: 1.7896, train_bce_dl_loss: 0.3605, step time: 0.4187\n",
      "batch: 17/17, train_dl_loss: 0.3525, train_bce_loss: 1.7629, train_bce_dl_loss: 0.3525, step time: 0.1139\n",
      "LOSS train DiceLoss: 0.3633, LOSS train BCE: 1.7732, LOSS train BCE-DiceLoss: 0.3633, LOSS val DiceLoss: 0.4773, LOSS val BCE: 1.7636, LOSS val BCE-DiceLoss: 0.4773, METRIC val: 0.5661\n",
      "time consuming of epoch 554 is: 440.4915\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3359, train_bce_loss: 1.7556, train_bce_dl_loss: 0.3359, step time: 0.4261\n",
      "batch: 1/17, train_dl_loss: 0.3348, train_bce_loss: 1.7815, train_bce_dl_loss: 0.3348, step time: 0.3881\n",
      "batch: 2/17, train_dl_loss: 0.3681, train_bce_loss: 1.7724, train_bce_dl_loss: 0.3681, step time: 0.4233\n",
      "batch: 3/17, train_dl_loss: 0.4199, train_bce_loss: 1.7579, train_bce_dl_loss: 0.4199, step time: 0.4118\n",
      "batch: 4/17, train_dl_loss: 0.3579, train_bce_loss: 1.7539, train_bce_dl_loss: 0.3579, step time: 0.4373\n",
      "batch: 5/17, train_dl_loss: 0.3364, train_bce_loss: 1.7632, train_bce_dl_loss: 0.3364, step time: 0.3903\n",
      "batch: 6/17, train_dl_loss: 0.3706, train_bce_loss: 1.7699, train_bce_dl_loss: 0.3706, step time: 0.4267\n",
      "batch: 7/17, train_dl_loss: 0.3280, train_bce_loss: 1.7515, train_bce_dl_loss: 0.3280, step time: 0.4298\n",
      "batch: 8/17, train_dl_loss: 0.3415, train_bce_loss: 1.7680, train_bce_dl_loss: 0.3415, step time: 0.4343\n",
      "batch: 9/17, train_dl_loss: 0.3583, train_bce_loss: 1.7800, train_bce_dl_loss: 0.3583, step time: 0.4537\n",
      "batch: 10/17, train_dl_loss: 0.3981, train_bce_loss: 1.7646, train_bce_dl_loss: 0.3981, step time: 0.4229\n",
      "batch: 11/17, train_dl_loss: 0.3289, train_bce_loss: 1.7505, train_bce_dl_loss: 0.3289, step time: 0.4448\n",
      "batch: 12/17, train_dl_loss: 0.3655, train_bce_loss: 1.7571, train_bce_dl_loss: 0.3655, step time: 0.4294\n",
      "batch: 13/17, train_dl_loss: 0.4103, train_bce_loss: 1.7834, train_bce_dl_loss: 0.4103, step time: 0.3766\n",
      "batch: 14/17, train_dl_loss: 0.3505, train_bce_loss: 1.7805, train_bce_dl_loss: 0.3505, step time: 0.4387\n",
      "batch: 15/17, train_dl_loss: 0.3247, train_bce_loss: 1.7734, train_bce_dl_loss: 0.3247, step time: 0.3893\n",
      "batch: 16/17, train_dl_loss: 0.3396, train_bce_loss: 1.7878, train_bce_dl_loss: 0.3396, step time: 0.4204\n",
      "batch: 17/17, train_dl_loss: 0.4864, train_bce_loss: 1.7958, train_bce_dl_loss: 0.4864, step time: 0.1130\n",
      "LOSS train DiceLoss: 0.3642, LOSS train BCE: 1.7693, LOSS train BCE-DiceLoss: 0.3642, LOSS val DiceLoss: 0.4775, LOSS val BCE: 1.7636, LOSS val BCE-DiceLoss: 0.4775, METRIC val: 0.5659\n",
      "time consuming of epoch 555 is: 415.2832\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3355, train_bce_loss: 1.7510, train_bce_dl_loss: 0.3355, step time: 0.4427\n",
      "batch: 1/17, train_dl_loss: 0.3607, train_bce_loss: 1.7819, train_bce_dl_loss: 0.3607, step time: 0.4007\n",
      "batch: 2/17, train_dl_loss: 0.3602, train_bce_loss: 1.7777, train_bce_dl_loss: 0.3602, step time: 0.4384\n",
      "batch: 3/17, train_dl_loss: 0.4457, train_bce_loss: 1.7586, train_bce_dl_loss: 0.4457, step time: 0.3764\n",
      "batch: 4/17, train_dl_loss: 0.3435, train_bce_loss: 1.7757, train_bce_dl_loss: 0.3435, step time: 0.4332\n",
      "batch: 5/17, train_dl_loss: 0.3514, train_bce_loss: 1.7778, train_bce_dl_loss: 0.3514, step time: 0.3809\n",
      "batch: 6/17, train_dl_loss: 0.4009, train_bce_loss: 1.7721, train_bce_dl_loss: 0.4009, step time: 0.4390\n",
      "batch: 7/17, train_dl_loss: 0.3207, train_bce_loss: 1.7514, train_bce_dl_loss: 0.3207, step time: 0.3898\n",
      "batch: 8/17, train_dl_loss: 0.3411, train_bce_loss: 1.7518, train_bce_dl_loss: 0.3411, step time: 0.4273\n",
      "batch: 9/17, train_dl_loss: 0.3542, train_bce_loss: 1.7723, train_bce_dl_loss: 0.3542, step time: 0.4293\n",
      "batch: 10/17, train_dl_loss: 0.3870, train_bce_loss: 1.7670, train_bce_dl_loss: 0.3870, step time: 0.4309\n",
      "batch: 11/17, train_dl_loss: 0.3494, train_bce_loss: 1.7771, train_bce_dl_loss: 0.3494, step time: 0.4392\n",
      "batch: 12/17, train_dl_loss: 0.3678, train_bce_loss: 1.7582, train_bce_dl_loss: 0.3678, step time: 0.4495\n",
      "batch: 13/17, train_dl_loss: 0.4332, train_bce_loss: 1.7810, train_bce_dl_loss: 0.4332, step time: 0.4431\n",
      "batch: 14/17, train_dl_loss: 0.3315, train_bce_loss: 1.7806, train_bce_dl_loss: 0.3315, step time: 0.4378\n",
      "batch: 15/17, train_dl_loss: 0.3514, train_bce_loss: 1.7792, train_bce_dl_loss: 0.3514, step time: 0.3775\n",
      "batch: 16/17, train_dl_loss: 0.3304, train_bce_loss: 1.7659, train_bce_dl_loss: 0.3304, step time: 0.4336\n",
      "batch: 17/17, train_dl_loss: 0.3016, train_bce_loss: 1.7821, train_bce_dl_loss: 0.3016, step time: 0.1139\n",
      "LOSS train DiceLoss: 0.3592, LOSS train BCE: 1.7701, LOSS train BCE-DiceLoss: 0.3592, LOSS val DiceLoss: 0.4776, LOSS val BCE: 1.7637, LOSS val BCE-DiceLoss: 0.4776, METRIC val: 0.5659\n",
      "time consuming of epoch 556 is: 407.1506\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3664, train_bce_loss: 1.7524, train_bce_dl_loss: 0.3664, step time: 0.4331\n",
      "batch: 1/17, train_dl_loss: 0.3588, train_bce_loss: 1.7942, train_bce_dl_loss: 0.3588, step time: 0.3859\n",
      "batch: 2/17, train_dl_loss: 0.3775, train_bce_loss: 1.7615, train_bce_dl_loss: 0.3775, step time: 0.4273\n",
      "batch: 3/17, train_dl_loss: 0.3914, train_bce_loss: 1.7688, train_bce_dl_loss: 0.3914, step time: 0.3872\n",
      "batch: 4/17, train_dl_loss: 0.3239, train_bce_loss: 1.7593, train_bce_dl_loss: 0.3239, step time: 0.4389\n",
      "batch: 5/17, train_dl_loss: 0.3680, train_bce_loss: 1.7583, train_bce_dl_loss: 0.3680, step time: 0.4348\n",
      "batch: 6/17, train_dl_loss: 0.3815, train_bce_loss: 1.7798, train_bce_dl_loss: 0.3815, step time: 0.4355\n",
      "batch: 7/17, train_dl_loss: 0.3140, train_bce_loss: 1.7582, train_bce_dl_loss: 0.3140, step time: 0.4430\n",
      "batch: 8/17, train_dl_loss: 0.3533, train_bce_loss: 1.7368, train_bce_dl_loss: 0.3533, step time: 0.4313\n",
      "batch: 9/17, train_dl_loss: 0.4161, train_bce_loss: 1.7795, train_bce_dl_loss: 0.4161, step time: 0.4411\n",
      "batch: 10/17, train_dl_loss: 0.3645, train_bce_loss: 1.7754, train_bce_dl_loss: 0.3645, step time: 0.4330\n",
      "batch: 11/17, train_dl_loss: 0.3322, train_bce_loss: 1.7632, train_bce_dl_loss: 0.3322, step time: 0.4405\n",
      "batch: 12/17, train_dl_loss: 0.3337, train_bce_loss: 1.7639, train_bce_dl_loss: 0.3337, step time: 0.4402\n",
      "batch: 13/17, train_dl_loss: 0.4137, train_bce_loss: 1.7772, train_bce_dl_loss: 0.4137, step time: 0.4289\n",
      "batch: 14/17, train_dl_loss: 0.3538, train_bce_loss: 1.7809, train_bce_dl_loss: 0.3538, step time: 0.4401\n",
      "batch: 15/17, train_dl_loss: 0.3509, train_bce_loss: 1.7736, train_bce_dl_loss: 0.3509, step time: 0.3867\n",
      "batch: 16/17, train_dl_loss: 0.3310, train_bce_loss: 1.7724, train_bce_dl_loss: 0.3310, step time: 0.4146\n",
      "batch: 17/17, train_dl_loss: 0.3504, train_bce_loss: 1.7625, train_bce_dl_loss: 0.3504, step time: 0.1130\n",
      "LOSS train DiceLoss: 0.3601, LOSS train BCE: 1.7677, LOSS train BCE-DiceLoss: 0.3601, LOSS val DiceLoss: 0.4776, LOSS val BCE: 1.7636, LOSS val BCE-DiceLoss: 0.4776, METRIC val: 0.5658\n",
      "time consuming of epoch 557 is: 434.8087\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3359, train_bce_loss: 1.7553, train_bce_dl_loss: 0.3359, step time: 0.4405\n",
      "batch: 1/17, train_dl_loss: 0.3395, train_bce_loss: 1.7716, train_bce_dl_loss: 0.3395, step time: 0.3860\n",
      "batch: 2/17, train_dl_loss: 0.3856, train_bce_loss: 1.7809, train_bce_dl_loss: 0.3856, step time: 0.4272\n",
      "batch: 3/17, train_dl_loss: 0.3796, train_bce_loss: 1.7782, train_bce_dl_loss: 0.3796, step time: 0.3754\n",
      "batch: 4/17, train_dl_loss: 0.3243, train_bce_loss: 1.7835, train_bce_dl_loss: 0.3243, step time: 0.4239\n",
      "batch: 5/17, train_dl_loss: 0.3424, train_bce_loss: 1.7638, train_bce_dl_loss: 0.3424, step time: 0.3898\n",
      "batch: 6/17, train_dl_loss: 0.3893, train_bce_loss: 1.7826, train_bce_dl_loss: 0.3893, step time: 0.4404\n",
      "batch: 7/17, train_dl_loss: 0.3587, train_bce_loss: 1.7478, train_bce_dl_loss: 0.3587, step time: 0.3793\n",
      "batch: 8/17, train_dl_loss: 0.3676, train_bce_loss: 1.7713, train_bce_dl_loss: 0.3676, step time: 0.4337\n",
      "batch: 9/17, train_dl_loss: 0.3647, train_bce_loss: 1.7710, train_bce_dl_loss: 0.3647, step time: 0.3910\n",
      "batch: 10/17, train_dl_loss: 0.4330, train_bce_loss: 1.7761, train_bce_dl_loss: 0.4330, step time: 0.4261\n",
      "batch: 11/17, train_dl_loss: 0.3126, train_bce_loss: 1.7618, train_bce_dl_loss: 0.3126, step time: 0.3869\n",
      "batch: 12/17, train_dl_loss: 0.3333, train_bce_loss: 1.7611, train_bce_dl_loss: 0.3333, step time: 0.4359\n",
      "batch: 13/17, train_dl_loss: 0.3912, train_bce_loss: 1.7852, train_bce_dl_loss: 0.3912, step time: 0.3817\n",
      "batch: 14/17, train_dl_loss: 0.3367, train_bce_loss: 1.7729, train_bce_dl_loss: 0.3367, step time: 0.4300\n",
      "batch: 15/17, train_dl_loss: 0.3285, train_bce_loss: 1.7781, train_bce_dl_loss: 0.3285, step time: 0.3806\n",
      "batch: 16/17, train_dl_loss: 0.3342, train_bce_loss: 1.7762, train_bce_dl_loss: 0.3342, step time: 0.4224\n",
      "batch: 17/17, train_dl_loss: 0.3286, train_bce_loss: 1.7882, train_bce_dl_loss: 0.3286, step time: 0.1129\n",
      "LOSS train DiceLoss: 0.3548, LOSS train BCE: 1.7725, LOSS train BCE-DiceLoss: 0.3548, LOSS val DiceLoss: 0.4776, LOSS val BCE: 1.7636, LOSS val BCE-DiceLoss: 0.4776, METRIC val: 0.5657\n",
      "time consuming of epoch 558 is: 428.3932\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3517, train_bce_loss: 1.7613, train_bce_dl_loss: 0.3517, step time: 0.4367\n",
      "batch: 1/17, train_dl_loss: 0.3591, train_bce_loss: 1.7822, train_bce_dl_loss: 0.3591, step time: 0.3781\n",
      "batch: 2/17, train_dl_loss: 0.3394, train_bce_loss: 1.7763, train_bce_dl_loss: 0.3394, step time: 0.4202\n",
      "batch: 3/17, train_dl_loss: 0.4374, train_bce_loss: 1.7669, train_bce_dl_loss: 0.4374, step time: 0.3761\n",
      "batch: 4/17, train_dl_loss: 0.3201, train_bce_loss: 1.7684, train_bce_dl_loss: 0.3201, step time: 0.4220\n",
      "batch: 5/17, train_dl_loss: 0.3235, train_bce_loss: 1.7579, train_bce_dl_loss: 0.3235, step time: 0.3739\n",
      "batch: 6/17, train_dl_loss: 0.3910, train_bce_loss: 1.7681, train_bce_dl_loss: 0.3910, step time: 0.4386\n",
      "batch: 7/17, train_dl_loss: 0.3379, train_bce_loss: 1.7766, train_bce_dl_loss: 0.3379, step time: 0.3772\n",
      "batch: 8/17, train_dl_loss: 0.3416, train_bce_loss: 1.7707, train_bce_dl_loss: 0.3416, step time: 0.4411\n",
      "batch: 9/17, train_dl_loss: 0.3631, train_bce_loss: 1.7873, train_bce_dl_loss: 0.3631, step time: 0.3834\n",
      "batch: 10/17, train_dl_loss: 0.3611, train_bce_loss: 1.7633, train_bce_dl_loss: 0.3611, step time: 0.4389\n",
      "batch: 11/17, train_dl_loss: 0.3861, train_bce_loss: 1.7916, train_bce_dl_loss: 0.3861, step time: 0.4356\n",
      "batch: 12/17, train_dl_loss: 0.3503, train_bce_loss: 1.7678, train_bce_dl_loss: 0.3503, step time: 0.4227\n",
      "batch: 13/17, train_dl_loss: 0.4037, train_bce_loss: 1.7761, train_bce_dl_loss: 0.4037, step time: 0.3834\n",
      "batch: 14/17, train_dl_loss: 0.3549, train_bce_loss: 1.7941, train_bce_dl_loss: 0.3549, step time: 0.4336\n",
      "batch: 15/17, train_dl_loss: 0.3459, train_bce_loss: 1.7673, train_bce_dl_loss: 0.3459, step time: 0.3804\n",
      "batch: 16/17, train_dl_loss: 0.3194, train_bce_loss: 1.7848, train_bce_dl_loss: 0.3194, step time: 0.4354\n",
      "batch: 17/17, train_dl_loss: 0.3524, train_bce_loss: 1.7631, train_bce_dl_loss: 0.3524, step time: 0.1129\n",
      "LOSS train DiceLoss: 0.3577, LOSS train BCE: 1.7735, LOSS train BCE-DiceLoss: 0.3577, LOSS val DiceLoss: 0.4777, LOSS val BCE: 1.7636, LOSS val BCE-DiceLoss: 0.4777, METRIC val: 0.5657\n",
      "time consuming of epoch 559 is: 401.5367\n",
      "----------\n",
      "EPOCH 481/560\n",
      "batch: 0/17, train_dl_loss: 0.3426, train_bce_loss: 1.7644, train_bce_dl_loss: 0.3426, step time: 0.4242\n",
      "batch: 1/17, train_dl_loss: 0.3502, train_bce_loss: 1.7936, train_bce_dl_loss: 0.3502, step time: 0.3774\n",
      "batch: 2/17, train_dl_loss: 0.3403, train_bce_loss: 1.7662, train_bce_dl_loss: 0.3403, step time: 0.4873\n",
      "batch: 3/17, train_dl_loss: 0.4135, train_bce_loss: 1.7759, train_bce_dl_loss: 0.4135, step time: 0.3720\n",
      "batch: 4/17, train_dl_loss: 0.3298, train_bce_loss: 1.7619, train_bce_dl_loss: 0.3298, step time: 0.4252\n",
      "batch: 5/17, train_dl_loss: 0.3171, train_bce_loss: 1.7849, train_bce_dl_loss: 0.3171, step time: 0.3823\n",
      "batch: 6/17, train_dl_loss: 0.4141, train_bce_loss: 1.7861, train_bce_dl_loss: 0.4141, step time: 0.4221\n",
      "batch: 7/17, train_dl_loss: 0.3405, train_bce_loss: 1.7546, train_bce_dl_loss: 0.3405, step time: 0.3884\n",
      "batch: 8/17, train_dl_loss: 0.3372, train_bce_loss: 1.7573, train_bce_dl_loss: 0.3372, step time: 0.4313\n",
      "batch: 9/17, train_dl_loss: 0.3271, train_bce_loss: 1.7704, train_bce_dl_loss: 0.3271, step time: 0.4813\n",
      "batch: 10/17, train_dl_loss: 0.3750, train_bce_loss: 1.7694, train_bce_dl_loss: 0.3750, step time: 0.4445\n",
      "batch: 11/17, train_dl_loss: 0.3412, train_bce_loss: 1.7580, train_bce_dl_loss: 0.3412, step time: 0.4476\n",
      "batch: 12/17, train_dl_loss: 0.3987, train_bce_loss: 1.7933, train_bce_dl_loss: 0.3987, step time: 0.4256\n",
      "batch: 13/17, train_dl_loss: 0.3885, train_bce_loss: 1.7850, train_bce_dl_loss: 0.3885, step time: 0.3795\n",
      "batch: 14/17, train_dl_loss: 0.3460, train_bce_loss: 1.7766, train_bce_dl_loss: 0.3460, step time: 0.4433\n",
      "batch: 15/17, train_dl_loss: 0.3460, train_bce_loss: 1.7704, train_bce_dl_loss: 0.3460, step time: 0.3878\n",
      "batch: 16/17, train_dl_loss: 0.3513, train_bce_loss: 1.7780, train_bce_dl_loss: 0.3513, step time: 0.4173\n",
      "batch: 17/17, train_dl_loss: 0.3525, train_bce_loss: 1.7629, train_bce_dl_loss: 0.3525, step time: 0.1144\n",
      "LOSS train DiceLoss: 0.3562, LOSS train BCE: 1.7727, LOSS train BCE-DiceLoss: 0.3562, LOSS val DiceLoss: 0.4777, LOSS val BCE: 1.7636, LOSS val BCE-DiceLoss: 0.4777, METRIC val: 0.5657\n",
      "time consuming of epoch 560 is: 436.2595\n"
     ]
    }
   ],
   "source": [
    "# Resuming training loop\n",
    "resuming_train_loop(model=unet_model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    diceloss_function=dice_loss_fn,\n",
    "                    bce_function=bce_loss_fn,\n",
    "                    bce_diceloss_function=criterion,\n",
    "                    metric=dice_metric,\n",
    "                    optimizer=optimizer,\n",
    "                    lr_scheduler=lr_scheduler,\n",
    "                    start_epoch=start_epoch,\n",
    "                    config=config,\n",
    "                    output_dir=OUTPUT_DIR,\n",
    "                    output_file=OUTPUT_FILE,\n",
    "                    device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958698b",
   "metadata": {
    "papermill": {
     "duration": 0.123092,
     "end_time": "2024-09-05T15:36:29.379684",
     "exception": false,
     "start_time": "2024-09-05T15:36:29.256592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Plot Training/Validation Losses & Validation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39cc7c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T15:36:29.627142Z",
     "iopub.status.busy": "2024-09-05T15:36:29.626784Z",
     "iopub.status.idle": "2024-09-05T15:36:30.568233Z",
     "shell.execute_reply": "2024-09-05T15:36:30.567294Z"
    },
    "papermill": {
     "duration": 1.068781,
     "end_time": "2024-09-05T15:36:30.570261",
     "exception": false,
     "start_time": "2024-09-05T15:36:29.501480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAHHCAYAAACcHAM1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABu/UlEQVR4nO3dd3hUVf4G8HfSGykQSMGQUEMRCIZiYgE0KyAKqKuAEQJSVgQFWV1ABRRXUXEFcVnAEoI/RBCluLCCGAGRICJIR5AWQFMoKSSkwMz5/XG8U5KZZCaZybT38zzzzGTmzp1zEyAv39NUQggBIiIiIgfmYe8GEBEREdWGgYWIiIgcHgMLEREROTwGFiIiInJ4DCxERETk8BhYiIiIyOExsBAREZHDY2AhIiIih8fAQkRERA6PgYXIDZw7dw4qlQoZGRna51555RWoVCqz3q9SqfDKK69YtU19+vRBnz59rHpOZ7Z9+3aoVCps377d3k0hckgMLEQOZtCgQQgICMC1a9dMHpOamgofHx9cuXKlAVtmuWPHjuGVV17BuXPn7N0ULSUYqFQqrFixwugxd9xxB1QqFW699dY6fcbKlSuxYMGCerSSiKpiYCFyMKmpqSgrK8O6deuMvn79+nVs2LAB/fv3R5MmTer8OS+//DLKysrq/H5zHDt2DK+++qrRwPLNN9/gm2++senn18TPzw8rV66s9vy5c+eQlZUFPz+/Op+7LoHl7rvvRllZGe6+++46fy6RK2NgIXIwgwYNQqNGjYz+MgWADRs2oLS0FKmpqfX6HC8vr3r9Uq4vHx8f+Pj42O3z77//fmzduhWXL182eH7lypWIiIhA9+7dG6Qd5eXl0Gg08PDwgJ+fHzw8+M8ykTH8m0HkYPz9/fHwww8jMzMT+fn51V5fuXIlGjVqhEGDBuHq1at4/vnn0blzZwQFBSE4OBgDBgzAwYMHa/0cY2NYKioq8Nxzz6Fp06baz7h48WK192ZnZ+Ppp59GfHw8/P390aRJEzz66KMGlZSMjAw8+uijAIC+fftqu2GUMRrGxrDk5+djzJgxiIiIgJ+fH7p27Yrly5cbHKOMx3nnnXfwwQcfoHXr1vD19UWPHj2wd+/eWq9bMXjwYPj6+mLNmjUGz69cuRKPPfYYPD09jb5vxYoVSExMhL+/Pxo3boxhw4bhwoUL2tf79OmDTZs2ITs7W3vNcXFxAHTdUatWrcLLL7+M5s2bIyAgAMXFxSbHsOzZswf3338/wsLCEBgYiC5duuC9994z+zqJXIWXvRtARNWlpqZi+fLl+PzzzzFp0iTt81evXsWWLVswfPhw+Pv74+jRo1i/fj0effRRtGzZEnl5eVi6dCl69+6NY8eOITo62qLPHTt2LFasWIHHH38cycnJ+O677zBw4MBqx+3duxdZWVkYNmwYbrnlFpw7dw6LFy9Gnz59cOzYMQQEBODuu+/Gs88+i4ULF+LFF19Ehw4dAEB7X1VZWRn69OmDU6dOYdKkSWjZsiXWrFmDUaNGobCwEJMnTzY4fuXKlbh27Rr+9re/QaVS4e2338bDDz+MM2fOwNvbu9ZrDQgIwODBg/HZZ59hwoQJAICDBw/i6NGj+Oijj3Do0KFq73n99dcxc+ZMPPbYYxg7diwuXbqE999/H3fffTd++eUXhIaG4qWXXkJRUREuXryI+fPnAwCCgoIMzvPaa6/Bx8cHzz//PCoqKkxWmrZu3YoHHngAUVFRmDx5MiIjI3H8+HFs3Lix2veDyOUJInI4N2/eFFFRUSIpKcng+SVLlggAYsuWLUIIIcrLy4VarTY45uzZs8LX11fMmTPH4DkAYtmyZdrnZs+eLfT/CThw4IAAIJ5++mmD8z3++OMCgJg9e7b2uevXr1dr8+7duwUA8cknn2ifW7NmjQAgtm3bVu343r17i969e2u/XrBggQAgVqxYoX2usrJSJCUliaCgIFFcXGxwLU2aNBFXr17VHrthwwYBQPz3v/+t9ln6tm3bJgCINWvWiI0bNwqVSiXOnz8vhBDihRdeEK1atdK2r1OnTtr3nTt3Tnh6eorXX3/d4HyHDx8WXl5eBs8PHDhQxMbGmvzsVq1aVfseKq8p36ubN2+Kli1bitjYWFFQUGBwrEajqfEaiVwRu4SIHJCnpyeGDRuG3bt3G3SzKOMr7r33XgCAr6+vdsyDWq3GlStXEBQUhPj4eOzfv9+iz/zf//4HAHj22WcNnp8yZUq1Y/39/bWPb9y4gStXrqBNmzYIDQ21+HP1Pz8yMhLDhw/XPuft7Y1nn30WJSUl2LFjh8HxQ4cORVhYmPbru+66CwBw5swZsz/zvvvuQ+PGjbFq1SoIIbBq1SqDz9e3du1aaDQaPPbYY7h8+bL2FhkZibZt22Lbtm1mf25aWprB99CYX375BWfPnsWUKVMQGhpq8Jq509GJXAkDC5GDUgbVKoNvL168iJ07d2LYsGHa8RUajQbz589H27Zt4evri/DwcDRt2hSHDh1CUVGRRZ+XnZ0NDw8PtG7d2uD5+Pj4aseWlZVh1qxZiImJMfjcwsJCiz9X//Pbtm1bbdCp0oWUnZ1t8HyLFi0MvlbCS0FBgdmf6e3tjUcffRQrV67E999/jwsXLuDxxx83euxvv/0GIQTatm2Lpk2bGtyOHz9udLyRKS1btqz1mNOnTwNAnadWE7kajmEhclCJiYlo3749PvvsM7z44ov47LPPIIQwmB30xhtvYObMmXjyySfx2muvoXHjxvDw8MCUKVOg0Whs1rZnnnkGy5Ytw5QpU5CUlISQkBCoVCoMGzbMpp+rz9SgWCGERed5/PHHsWTJErzyyivo2rUrOnbsaPQ4jUYDlUqFr7/+2uhnVx2nUpPaqitEVB0DC5EDS01NxcyZM3Ho0CGsXLkSbdu2RY8ePbSvf/HFF+jbty8+/vhjg/cVFhYiPDzcos+KjY2FRqPB6dOnDaoqJ06cqHbsF198gbS0NPzrX//SPldeXo7CwkKD4yzpuoiNjcWhQ4e0U3wVv/76q/Z1W7jzzjvRokULbN++HW+99ZbJ41q3bg0hBFq2bIl27drVeE5rdNkola4jR44gJSWl3ucjcnbsEiJyYEo1ZdasWThw4EC1tVc8PT2rVRTWrFmD33//3eLPGjBgAABg4cKFBs8bWwDN2Oe+//77UKvVBs8FBgYCQLUgY8z999+P3NxcrF69WvvczZs38f777yMoKAi9e/c25zIsplKpsHDhQsyePRsjRowwedzDDz8MT09PvPrqq9WuXQhhsOpwYGBgnbvGFLfddhtatmyJBQsWVPv+WVpFInIFrLAQObCWLVsiOTkZGzZsAIBqgeWBBx7AnDlzMHr0aCQnJ+Pw4cP49NNP0apVK4s/KyEhAcOHD8d//vMfFBUVITk5GZmZmTh16lS1Yx944AH83//9H0JCQtCxY0fs3r0b3377bbWVdxMSEuDp6Ym33noLRUVF8PX1xT333INmzZpVO+f48eOxdOlSjBo1Cvv27UNcXBy++OIL7Nq1CwsWLECjRo0sviZzDR48GIMHD67xmNatW+Of//wnZsyYgXPnzmHIkCFo1KgRzp49i3Xr1mH8+PF4/vnnAcjuvNWrV2Pq1Kno0aMHgoKC8OCDD1rUJg8PDyxevBgPPvggEhISMHr0aERFReHXX3/F0aNHsWXLljpfL5EzYmAhcnCpqanIyspCz5490aZNG4PXXnzxRZSWlmLlypVYvXo1brvtNmzatAnTp0+v02elp6ejadOm+PTTT7F+/Xrcc8892LRpE2JiYgyOe++99+Dp6YlPP/0U5eXluOOOO/Dtt9+iX79+BsdFRkZiyZIlmDt3LsaMGQO1Wo1t27YZDSz+/v7Yvn07pk+fjuXLl6O4uBjx8fFYtmwZRo0aVafrsbbp06ejXbt2mD9/Pl599VUAQExMDO677z4MGjRIe9zTTz+NAwcOYNmyZZg/fz5iY2MtDiwA0K9fP2zbtg2vvvoq/vWvf0Gj0aB169YYN26c1a6JyFmoBGuLRERE5OA4hoWIiIgcHgMLEREROTwGFiIiInJ4dQosixYtQlxcHPz8/NCrVy/89NNPJo/NyMjQ7liq3KpuaV/1deU2b968ujSPiIiIXIzFgUWZqjd79mzs378fXbt2Rb9+/Wpcljo4OBg5OTnaW9UltvVfy8nJQXp6OlQqFR555BHLr4iIiIhcjsWzhHr16oUePXrg3//+NwC5XHVMTAyeeeYZo1MpMzIyMGXKFLMWjlIMGTIE165dQ2ZmpiVNIyIiIhdl0ToslZWV2LdvH2bMmKF9zsPDAykpKdi9e7fJ95WUlGiX/b7tttvwxhtvoFOnTkaPzcvLw6ZNm7B8+XKT56uoqEBFRYX2a41Gg6tXr6JJkybcxZSIiMhJCCFw7do1REdHV9v4tCqLAsvly5ehVqsRERFh8HxERIR2v4+q4uPjkZ6eji5duqCoqAjvvPMOkpOTcfToUdxyyy3Vjl++fDkaNWqEhx9+2GQ75s6dq120iYiIiJzbhQsXjGYCfTZf6TYpKQlJSUnar5OTk9GhQwcsXboUr732WrXj09PTkZqaWm1grr4ZM2Zg6tSp2q+LiorQokULXLhwAcHBwda9ACIiIrKJ4uJixMTEmLX1hkWBJTw8HJ6ensjLyzN4Pi8vD5GRkWadw9vbG926dTO6P8nOnTtx4sQJg83PjPH19YWvr2+154ODgxlYiIiInIw5wzksmiXk4+ODxMREg8GwGo0GmZmZBlWUmqjVahw+fBhRUVHVXvv444+RmJiIrl27WtIsIiIicnEWdwlNnToVaWlp6N69O3r27IkFCxagtLQUo0ePBgCMHDkSzZs3x9y5cwEAc+bMwe233442bdqgsLAQ8+bNQ3Z2NsaOHWtw3uLiYqxZswb/+te/rHBZRERE5EosDixDhw7FpUuXMGvWLOTm5iIhIQGbN2/WDsQ9f/68wUjfgoICjBs3Drm5uQgLC0NiYiKysrLQsWNHg/OuWrUKQggMHz68npdERERErsYldmsuLi5GSEgIioqKahzDolarcePGjQZsGZHj8fb2hqenp72bQURk9u9voAFmCTkCIQRyc3MtWryOyJWFhoYiMjKS6xYRkdNwi8CihJVmzZohICCA/0iT2xJC4Pr169qtNIwNficickQuH1jUarU2rDRp0sTezSGyO39/fwBAfn4+mjVrxu4hInIKddqt2ZkoY1YCAgLs3BIix6H8feCYLiJyFi4fWBTsBiLS4d8HInI2bhNYiIiIyHkxsLi4Pn36YMqUKdqv4+LisGDBghrfo1KpsH79+np/trXOYy1VvxfOztG+v0REtsTA4qAefPBB9O/f3+hrO3fuhEqlwqFDhyw+7969ezF+/Pj6Ns/AK6+8goSEhGrP5+TkYMCAAVb9rKoyMjKgUqmgUqng6emJsLAw9OrVC3PmzEFRUZHBsWvXrjW64WZdjRo1CiqVCk899VS11yZOnAiVSoVRo0aZfb7t27dDpVKZPf2+Ib6/RESOgoHFQY0ZMwZbt27FxYsXq722bNkydO/eHV26dLH4vE2bNm2wAciRkZFGN6m0tuDgYOTk5ODixYvIysrC+PHj8cknnyAhIQF//PGH9rjGjRubtSOoJWJiYrBq1SqUlZVpnysvL8fKlSvRokULq36WorKyEkDDfX+JzFJZCVRU2LsV5MIYWBzUAw88gKZNmyIjI8Pg+ZKSEqxZswZjxozBlStXMHz4cDRv3hwBAQHo3LkzPvvssxrPW7VL6LfffsPdd98NPz8/dOzYEVu3bq32nmnTpqFdu3YICAhAq1atMHPmTO3skoyMDLz66qs4ePCgttKhtLlql8Xhw4dxzz33wN/fH02aNMH48eNRUlKifX3UqFEYMmQI3nnnHURFRaFJkyaYOHFirTNZVCoVIiMjERUVhQ4dOmDMmDHIyspCSUkJ/vGPf2iPq9olVFFRgWnTpiEmJga+vr5o06YNPv74Y+3rR44cwYABAxAUFISIiAiMGDECly9fNvjs2267DTExMVi7dq32ubVr16JFixbo1q2bwbEajQZz585Fy5Yt4e/vj65du+KLL74AAJw7dw59+/YFAISFhRlUZ/r06YNJkyZhypQpCA8PR79+/Yx+fy9evIjhw4ejcePGCAwMRPfu3bFnz54av3dEVpGXB3TpArRqBVy9au/WkIty+XVYjBICuH7dPp8dEACYMUPDy8sLI0eOREZGBl566SXtrI41a9ZArVZj+PDhKCkpQWJiIqZNm4bg4GBs2rQJI0aMQOvWrdGzZ89aP0Oj0eDhhx9GREQE9uzZg6KiIqNjPBo1aoSMjAxER0fj8OHDGDduHBo1aoR//OMfGDp0KI4cOYLNmzfj22+/BQCEhIRUO0dpaSn69euHpKQk7N27F/n5+Rg7diwmTZpkEMq2bduGqKgobNu2DadOncLQoUORkJCAcePG1Xo9+po1a4bU1FSkp6dDrVYbXWtk5MiR2L17NxYuXIiuXbvi7Nmz2kBSWFiIe+65B2PHjsX8+fNRVlaGadOm4bHHHsN3331ncJ4nn3wSy5YtQ2pqKgAgPT0do0ePxvbt2w2Omzt3LlasWIElS5agbdu2+P777/HEE0+gadOmuPPOO/Hll1/ikUcewYkTJxAcHKxdLwUAli9fjgkTJmDXrl1Gr7ekpAS9e/dG8+bN8dVXXyEyMhL79++HRqOx6PtGZLGSEmDgQODECfn14sXASy/Zt03kmoQLKCoqEgBEUVFRtdfKysrEsWPHRFlZme7JkhIhZGxp+FtJidnXdfz4cQFAbNu2TfvcXXfdJZ544gmT7xk4cKD4+9//rv26d+/eYvLkydqvY2Njxfz584UQQmzZskV4eXmJ33//Xfv6119/LQCIdevWmfyMefPmicTERO3Xs2fPFl27dq12nP55PvjgAxEWFiZK9K5/06ZNwsPDQ+Tm5gohhEhLSxOxsbHi5s2b2mMeffRRMXToUJNtWbZsmQgJCTH62uLFiwUAkZeXJ4Qw/F6cOHFCABBbt241+t7XXntN3HfffQbPXbhwQQAQJ06c0LZ38ODBIj8/X/j6+opz586Jc+fOCT8/P3Hp0iUxePBgkZaWJoQQory8XAQEBIisrCyDc44ZM0YMHz5cCCHEtm3bBABRUFBgcEzv3r1Ft27dqrVR//u7dOlS0ahRI3HlyhWj11OV0b8XRJa6cUOIAQPkv23e3vK+WTMhrl+3d8vISdT0+7sq96ywOIn27dsjOTkZ6enp6NOnD06dOoWdO3dizpw5AOQqvm+88QY+//xz/P7776isrERFRYXZY1SOHz+OmJgYREdHa59LSkqqdtzq1auxcOFCnD59GiUlJbh582atm1QZ+6yuXbsiMDBQ+9wdd9wBjUaDEydOaHf77tSpk0E1JCoqCocPH7bosxTiz309ja05cuDAAXh6eqJ3795G33vw4EFs27YNQUFB1V47ffo02rVrp/26adOmGDhwIDIyMiCEwMCBAxEeHm7wnlOnTuH69ev4y1/+YvB8ZWVlta4jYxITE2t8/cCBA+jWrRsaN25c67mIrEIIYMIE4OuvAX9/4JtvgNRU4Px54JNPgL/9zd4tJBfjnoElIECWMe312RYYM2YMnnnmGSxatAjLli1D69attb9k582bh/feew8LFixA586dERgYiClTpmgHZVrD7t27kZqaildffRX9+vVDSEgIVq1ahX/9619W+wx93t7eBl+rVKo6d2scP34cwcHBRrdk0O9uMaakpAQPPvgg3nrrrWqvGdt/58knn8SkSZMAAIsWLTJ6PgDYtGkTmjdvbvCaOQNn9YOeMbVdD5HVvfYa8NFHgIcHsGoVcOedwN//DkyeDLzzDjB2LMBtH8iK3DOwqFRALb8AHMVjjz2GyZMnY+XKlfjkk08wYcIEbcVg165dGDx4MJ544gkAckzKyZMn0bFjR7PO3aFDB1y4cAE5OTnaX8I//vijwTFZWVmIjY3FS3p90tnZ2QbH+Pj4QK1W1/pZGRkZKC0t1f7y3bVrFzw8PBAfH29Wey2Rn5+PlStXYsiQIfDwqD62vHPnztBoNNixYwdSUlKqvX7bbbfhyy+/RFxcHLy8av9r0r9/f1RWVkKlUmkHxerr2LEjfH19cf78eZNVHR8fHwCo9XtpTJcuXfDRRx/h6tWrrLKQ7S1bBsyeLR8vWgQMGiQfjxkDvPoqcOoUsG4d8Ne/2q+N5HI4S8jBBQUFYejQoZgxYwZycnIM1vVo27Yttm7diqysLBw/fhx/+9vfkJeXZ/a5U1JS0K5dO6SlpeHgwYPYuXOnQTBRPuP8+fNYtWoVTp8+jYULF2LdunUGx8TFxeHs2bM4cOAALl++jAojUxtTU1Ph5+eHtLQ0HDlyBNu2bcMzzzyDESNGaLuD6koIgdzcXOTk5OD48eNIT09HcnIyQkJC8Oabbxp9T1xcHNLS0vDkk09i/fr1OHv2LLZv347PP/8cgFxH5erVqxg+fDj27t2L06dPY8uWLRg9erTRQOHp6Ynjx4/j2LFjRgf4NmrUCM8//zyee+45LF++HKdPn8b+/fvx/vvvY/ny5QCA2NhYqFQqbNy4EZcuXTKYQVWb4cOHIzIyEkOGDMGuXbtw5swZfPnll9i9e7fZ5yAyy7ffAsog+BkzAP11iAIDgYkT5eO335bdRo5i+XLgjTfqN4vp99+B/fut1yayCAOLExgzZgwKCgrQr18/g/EmL7/8Mm677Tb069cPffr00f7CMpeHhwfWrVuHsrIy9OzZE2PHjsXrr79ucMygQYPw3HPPYdKkSUhISEBWVhZmzpxpcMwjjzyC/v37o2/fvmjatKnRqdUBAQHYsmULrl69ih49euCvf/0r7r33Xvz73/+27JthRHFxMaKiotC8eXMkJSVh6dKlSEtLwy+//GK0+0axePFi/PWvf8XTTz+N9u3bY9y4cSgtLQUAREdHY9euXVCr1bjvvvvQuXNnTJkyBaGhoUYrNoBcD6amsT2vvfYaZs6ciblz56JDhw7o378/Nm3ahJYtWwIAmjdvjldffRXTp09HRESEtovJHD4+Pvjmm2/QrFkz3H///ejcuTPefPNN7sRM1vfCC4BaDTzxBFDl3wsAwDPPAH5+wN69QJWZcnZz6hQwapScvdS6NfDmm5bNFBVCVpXi44HEROCBB4Bff7VZc8k4lRCOFIHrpri4GCEhISgqKqr2C6O8vBxnz55Fy5Yt4efnZ6cWEjkW/r1wEbm5cuBrYqL8ZVzXTS337ZPj6zp0qPm4X34BbrsN8PEBcnIAU92PEycC//kP0L+/HJRbVWEhsHOnXLslNrZubbbErFlyzI2npwxbABAVJZ8fMwaoMnauWlufegpYvdrweS8v+b2fPRswMk7OYleuyAHLlZWGtxs3gBYt5M/GBReKrOn3dzW2nbDUMCye1kzk5vj3wgUUFgrRtatuyYRZs+p2nj17hPDwECIkRIirV2s+dtIk+VmPPVbzcadPy3MCQhw8qHteoxFi9WohIiJ07e7RQ4i33xbizBnj57p5U4g//hCiosKiy9JSq4WIjZWftWKFEP/3f0LExek+v3VrId56S34fbtwwfO8PP+je6+kpxOuvC3HsmBAPPqh7f1iYEAsWCFFZWbf2/fSTEKmpumnhpm6enkJ06iTEsGFCvPGGEOvXC7F7txAnTwpx5Yq8TidkybRmVliI3BD/Xji58nKgXz/g+++BRo2Aa9fk8wsXyi4Zc924AfToARw8KL9+801g2jTTnxkdDRQUAJs3y8+vybBhsiqRmgqsWCGrBxMnAhs3ytebNQMuXTIc55KYCCQlyZVzL16Utz/+kFWRhATZzWTGIHgD27cDffsCwcGyIuXvLysXH3wgqy75+bpjGzUC7rgD6NNHfk/nzgU0GrmC78qVQK9eumO//RaYOhVQll2Ijwfee6/27wsgP//LL+XPS3+iQ2SkbJ+Pj+7m4SG7tAoKaj6nhwcQFgaEhMhzBATIe+UWGSm7w5Rbq1aAkWUbGhorLHr4P0mi6vj3wonduCHEkCHyf93BwULs3y/EnDm6/4l/+qn553r7bfkepRoSHW26krF6tTzmlltk1aM2+/bpKgOzZwsRGKhbYO6VV4QoLxciJ0eI//xHiHvu0bWhptvHH5t/bYq0NPnesWOrv3btmhALFwoxaJAQoaHGP/OJJ4Qw9b//mzeFWLpUiKZNdccPHmy6WnT0qBAvvSREVJTueG9vIUaMEGLvXtPXoNEIcfGiEP/7nxBvvikrMomJslLUqFHdFzINDxeiRQsh2rQRokMHWbHr3l2I228XIjlZiDvuEOLOO4W46y4h7r5biL/8xcJvfu1YYdHD/0kSVce/F05KCDlD5+OP5XiGLVuA3r3l85MnA++/LysQX30F1LaT99mzQKdOQFkZsGSJnI6ckyNn04wcWf34/v3l5738sqxMmOMvf5GVCMWdd8rKhrGxMvn5wPr1wOnTspJzyy1ATIy8//RT4B//kF//9pv5YzlKSmRlobQU+OEHWT0xRa2W1ZLt24EdO+SMoGeflYOLa1NYKL9/778vz+PrK9s7fTpw+bJcp2blSl0lC5DtmjABGD9ePq6Pyko5++nKFaC4WP5Mr1+X92Vl8vovXpTf2zNn5H1dZkv5+spKmxVZUmFxm8ASFxfHxbWI/lRWVoZz584xsDibGTNkt42Hh+xS0J8VqNEAI0bIX4z+/jIoJCcbP48QMtBs2SK7P777Tp73xRflQNgDBwwH8F64IAfHCiG7J1q3Nq+927cD994rux7efluGLROz7GpUVga0aSO7h957TwYJc3zyCZCWJt978mTdByWb6+hR2TZlv7HQUBlmFN7e8vv++OPAQw/JLh97KSyU3XQVFbrBvcr9zZvyZ63RGNZkPDyARx6xajPYJaTn5s2b4tixY+Ly5ct2aBmRY7p8+bI4duyYwb5N5OAWLND96vjoI+PHVFTo9vYJDRVixw7jx61cKY/x8RHi11/lc1eu6Lptqu6x9c9/yud797a83UePCmGNf3+XLNHtVXTtmnnvuece+Z45c+r/+ebSaIRYs0Z2tQBCqFTy+/bBB/J7TAbYJVRFTk4OCgsL0axZMwQEBBjdW4bIHQghcP36deTn5yM0NLTGdWrIgRQUyGm4FRVyIOj06aaPLS2VXTHKooHDhwNvvSW7UwDZFdChg+yCmTMH0F9X6dlnZbdGv35yYC0g/5fdtq3sSjDVXdQQbtwA2reX7Xj9dVkNqkl2NtCypYx4Z88CcXEN0kyt69dlN1SHDrrvPVXDLqEqxJ8roRbql+aI3FhoaCgiIyMZ3p3FokXApElA585yHERtP7eiIrnA20cfyV/Y/v5y9s8LL8ixLh99JH+RHjhg2C1x5owMJxqNHM9x661yPEefPnIGTU6Ofbc1+fRTOaYkNFS2NSzM9LH//KcMY336ANu2NVQLyUIMLCao1WrcuHGjAVtG5Hi8vb25Aq6zSUyUS8IvWCADh7l++UUev3On/Do6Wo4DAeRzd95Z/T2PPgp88QUwejSQni5XiF2+XG5m+OGH9b2S+lGrga5d5ViRF180vtIuIENafLwcoLtsmbwGckgMLEREruLAAaBbNzlg848/gPBwy94vBLBmjayunD8vnxs/Hli61PjxP/4o10Lx9gaOHJGfff06kJUln7e39evlgNWAAFllMbYXWVaWnBEUGCjXXnGA9UbIOEt+f3MvISIiR5aeLu+HDLE8rACy++ixx+TeN2+8IZeif/tt08fffrucXXTjBvDggzKstG8vn3cEgwfLxe6uX5fXY8yfG4rikUcYVlwIAwsRkaMqL5erxALAk0/W71z+/nJa9EcfydVQa/L88/L+5EndZzvKeCeVShdUliwBjh0zXC23rEy3709aWsO3j2zGwjWOiYjqKCdHdjc0bizL+M2ayUGTjvKL0BF99ZWcIXTLLXLmT0MZNEiutXL6tNwwcMSIhvtsc9x7r1xuf9s2ufhdWJgcRNyhg/zzVFQkNwzs08feLSUrYmAhoobx4INyV2B9Xl4yuAwfDrzzjn3a5cg+/ljejxolg0ND8fSUK7X+7W+yK6q+K7Fam0olZ06lpspZUwUFctxKVpbumBEj6rZIHTksDrolIts7c0b+j93DQ97n58v/Beu7cEFWEkg6f16uHSKErHS0atWwny+EXKn2tttq70Kyp/Jy2XV17Bhw/Li8VVTIGU3Nmtm7dVQLS35/s8JCRLb33//K+7vv1q2JUVEhg8ujjwJ79gDr1lm207Cry8iQoaFv34YPK4CsYvTt2/Cfayk/P7mdQJcu9m4J2RjrZURke0pgefBB3XO+vnIF0KFD5ddfftnw7XJUGo1cPwSo/2BbIhfBwEJEtlVUJFdLBQwDi+Lhh+X9zp2y4kKyK+bcOSA4WPf9IXJzDCxEZFtbtsjdX+Pj5bLvVcXGAt27y6rC+vUN3jyHpAy2ffxxuUAaETGwEJGNGesOqkqpIqxda/v2OLqCAl33GLuDiLQYWIjIdm7eBP73P/l40CDTxz3yiLzPzJS/sN3ZZ5/JAcm33iorT0QEgIGFiOqjvFxuSGfK7t3A1atysbia9qFp107+gr55U1eRcUeVlbr1aMaO5aJ6RHoYWIhI59o1WeUoKan5uMuX5S7AjRrJjehMLef01Vfy/v775SJxNWG3kFw75OxZuVDb2LH2bg2RQ2FgISKd6dOBlBSgeXNgyhTdXjKK8nJg3jygTRtg4UJdReTzz42fz5zxKwqlW2jLltoDkysqLQVee00+njlT7jRMRFoMLESkoyxtXlwMvPeenNnTr5+slKxcKXft/cc/5FTlrl2BJ56Qxz//vPyFq++334ATJ2RlpV+/2j+7c2cZhMrLdeNe3Ml77wF5eXKROFZXiKphYCEiSaORAQOQu+A+8IAcQ/HNN8DgwXLfluxsWX3JyJD7An3wgZyWfPEi8OabhudTqiu9e5u3tLtK5b7dQlevAm+/LR/PmQP4+Ni3PUQOiIGFiKSLF4GyMsDbGxgzRgaOU6eAF16Qu+EGBQH//KfsJkpLkxvk+fsD774r3z9vntwzSKEElppmB1WldAtt2iQrLe7irbdk1apLF7kRJBFVw8BCZK7KSrkJnatSqiutW+sGyLZqJf/nf+mS7CZ66aXqC5k99BBw771yKu7UqfK5ggK5ci1g3vgVRY8ecgPEkhJZ2XEHv/8uxwMBwBtvcIdhIhP4N4PIXOPHyzEWu3fbuyW2oQSW+Pjqr3l6mp5iq1LJX7iensCGDXLQ7ObNcrpzp05Ay5bmt8Edu4XmzJHVpDvukLOpiMgoBhYicx08KO+VgamupqbAUpuOHXU7LU+erFup1ZLqikLpFvrqK+DGDcvf70x++023DP+bb3LdFaIaMLAQmevSJXl/6pR922Erv/4q79u3r9v7X3kFaNZMBp/6BJY77pDnKSgAtm2rW1vsZedOYNIkOX7HnLA1c6asRN1/P3DnnbZvH5ETY2AhMocQcrE0QP6v2BXVp8ICyJlAc+fqvg4PB3r1svw8np7AkCHy8Rdf1K0tajWwf7+sXugPBLa1qVOBRYvkQGNlLZtfftEtrHfjhuxSfOstGVJWr5bPv/FGw7WRyEmphDC1RKXzKC4uRkhICIqKihAcHGzv5pArunYNUP5stWghp/c6mq+/BjZuBP71L8DPz7L3lpbKWUCADGZNmtStDRoNcPvtwN69ciZRRkbdzrNtG3DPPfJ7npNT+47FarXsstu+Xd6+/17OugHktOrt2+vWDkuo1fJ7WF4utyK4elX32q23ytVrs7KA69cN3zdpEvD++7ZvH5EDsuT3dy1rZRMRAF13EABcuCB/KVkaCmztxReBAwfkSrUPPWTZe5WqUZMmdQ8rgJzh8tlnMjRNm1b38/TuDcTFAefOAevWyTVgTLl5U85S+v57w+cDAmQ4+PlnGaRsPfvmzBn558LfX4asrVuB5cvlQOQjR+QNkGHm7rvlNd59N9Ctm23bReQiGFiIzKF0BwGyvH/mjBxo6kjOn5f3dZl6Xd/uIH2tWwP/+U/9zuHhAYwaJcfFLFtWc2BZt06GFV9fGdb69JG3W2+VFZrSUvm9iYurX5tqowSSjh3lwm8DB8pbQYFsY0UFcNdd8nVOXSayGP/WEJlDv8ICON7A27IyXRdEXcZsKIGlrgNubWHUKDlrJjNTVlpMURaumzZNdok9/zzQvbusgCnXo4QJW1I+49ZbDZ8PCwOefBKYMEG+xrBCVCd1+puzaNEixMXFwc/PD7169cJPP/1k8tiMjAyoVCqDm5+RUvrx48cxaNAghISEIDAwED169MB55X+MRPamX2EBHG/g7R9/6B7XJbAoM4SsUWGxlthYOY4FkF0rxuzeDfz4o6xoPP109deV8NAQgeXoUcPPJCKrsjiwrF69GlOnTsXs2bOxf/9+dO3aFf369UN+fr7J9wQHByMnJ0d7y64yYPH06dO488470b59e2zfvh2HDh3CzJkzjQYbIrtw9ArLxYu6x/buErKmJ5+U98uWyXEoVc2fL+9TU4GIiOqvN2RgUT6jUyfbfxaRG7J4DMu7776LcePGYfTo0QCAJUuWYNOmTUhPT8f06dONvkelUiEyMtLkOV966SXcf//9eFvZ/AtA69atLW0ake0ogSUsTI5JcLQKy++/6x6fOydnrHh6mvdeIeT+QIDjBZaHHpLTpbOz5UwfpeICyOtU1nt57jnj71fCg1L9sJXKSl3oY4WFyCYsqrBUVlZi3759SElJ0Z3AwwMpKSnYXcNy5SUlJYiNjUVMTAwGDx6Mo3r/eGg0GmzatAnt2rVDv3790KxZM/Tq1Qvr1683eb6KigoUFxcb3IhsSukSSkqS944WWPQrLDdvGn5dmz/+kHv3eHrKvYMcib8/MGyYfLxsmeFrCxfKqktKCtC5s/H3K+Hh+HH5fbGVkyfl+YOD5V5IRGR1FgWWy5cvQ61WI6JK6TUiIgK5ublG3xMfH4/09HRs2LABK1asgEajQXJyMi7++Q9qfn4+SkpK8Oabb6J///745ptv8NBDD+Hhhx/Gjh07jJ5z7ty5CAkJ0d5iYmIsuQwiyykVluRkea9MbbbUiRNycTH9NTqsQb/CAlg2jkWpDLRqJceCOBqlW+iLL3RrqxQXAx99JB8rGy4a07KlDD0VFbbduFJ//AqX1yeyCZsPV09KSsLIkSORkJCA3r17Y+3atWjatCmWLl0KQFZYAGDw4MF47rnnkJCQgOnTp+OBBx7AkiVLjJ5zxowZKCoq0t4uXLhg68sgd6cElg4d5P+ilanNlpo1S467ePVV67avPoGlvkvy21qPHnIqcHm5bmXYjz+Wi/l16AD062f6vR4euunntuwW4vgVIpuzKLCEh4fD09MTeXl5Bs/n5eXVOEZFn7e3N7p164ZTfw5aDA8Ph5eXFzpWWdOiQ4cOJmcJ+fr6Ijg42OBGZFNKl1DTpnLHZqBuA29//lner14tx5lYi9IFFBUl7+tSYXG08SsKlUpXZUlPl10vCxfKr6dMqX2acEMMvDU1pZmIrMaiwOLj44PExERkZmZqn9NoNMjMzESS0rdfC7VajcOHDyPqz39YfXx80KNHD5xQ/tH808mTJxEbG2tJ84hsR6mwNG0KtG0rH1s6jqWoSBck8vKsu7GfUmG5+255b0n3h6MHFgB44gk5xmbPHrnvzrlzckXeESNqfy8DC5FLsLhLaOrUqfjwww+xfPlyHD9+HBMmTEBpaal21tDIkSMxY8YM7fFz5szBN998gzNnzmD//v144oknkJ2djbFjx2qPeeGFF7B69Wp8+OGHOHXqFP7973/jv//9L542tq4CUUO7cUM3diI8XFdhsTSwHDxo+PXKlfVvGyArNTk58rESWFypwgLIKcsPPCAfz54t759+Wo5PqY2tZwqVlekCIgMLkc1YPK156NChuHTpEmbNmoXc3FwkJCRg8+bN2oG458+fh4deibagoADjxo1Dbm4uwsLCkJiYiKysLIMuoIceeghLlizB3Llz8eyzzyI+Ph5ffvkl7uR26+QIlO4gDw+5D4xSYbG0S+iXX+R9dLScmbN2rVzCvr7rDeXl6aYxK5VOcwNLWZluI0dHDiwAMHq03JcHML1QnDFKiDh5Ug6+9fW1bruOH5djmsLDgWbNrHtuItLibs1EtTl0COjaVXYH5ecDu3YBd95p+a7No0bJFVtnzpRTdC9elKHF0o0Kq9q7F+jZE2jeXP7yVP4OFBbKNUxqolxbWBhw5Ypjz3C5cUNOGc7Pl9/LqtOcTRECCA2VM4sOHTI9BbquPvlE7kzdp491u/mI3IAlv7+5qQVRbZQKS3i4vFcqLJZObVYqLImJurVFrNEtpAy4veUWoFEjGawA4OzZ2t+r3x3kyGEFALy9gbfeAnr1kqHPXCqVbbuFOH6FqEEwsBDVRn/ArXJv6dTmigrg2DH5uFs34PHH5eONG+X//I3RaOT03R9/rPncyoDb5s3lvbL4mzltc4bxK/pGjZLfD0sXuLPlwFvuIUTUIBhYiGpTNbCoVJZPbT5yRE7HbdwYiIkBEhLkuifl5YCpVZ0XLADGjpX75NRECSzKCqvKthbmzBRytsBSV7YMLFyDhahBMLAQ1aZqlxBg+dRmpTuoWzcZeFQqYPhw+ZyxbqHDhwFltt2ZM3LpfFOULiF3qLDUla26hIqLAWW9KAYWIptiYCGqTdUKC2D51Gb9wKJQAsu338qBpIqKCrnuSGWl7jllc0Jj6tolJIRulVtXDyxKheX0aeD6deudVwlAzZvLgctEZDMMLES1MRZYLJ3abCywtG0rl51Xq4E1a3TPz5wpZ7OEh+v+115lYUUD+oNuAfMDS26uXN7ew0MXwFxVs2by+ymEnEllLRy/QtRgGFiIamOsS8iSCotarVs0Tj+wANW7hXbsAN55Rz7+6CPg9tvlY1OBRQjTFZZz52pe/l85Z8uW1l+bxNHYaqYQx68QNRgGFqLa1FRhMWdq82+/yW6IgACgXTvD14YOlb9Ms7JkVWXkSBlCxowBBg/WddUoXTdVFRXpujiUwBIdLRdWu3lTV30xxl3GryhsMfCWU5qJGgwDC1FtjFVYLJnarHQHdekiV6PVFx0N9O0rH6ekyAGcrVrJHZ0B3Q7KpiosSiBp3Fi3TL2np6yaADXPFGJgqT8GFqIGw8BCVBMhDHdqVuhPba6tW8jY+BV9SrfQpUtyPMn//Z9cAA7QhYmTJ+W6LFVV7Q5SmDOOxV0G3Cqs3SV0+bLcFgEAquw2T0TWx8BCVJPCQtm1AhhWWADzB97WFlgeeUSu4grIqczJybrXWrYEvLxkt48STvRVndKsMCewKBUWpYrj6pTAcv686cX6LKEEn5YtgcDA+p+PiGrEwEJUE6W6EhRUfZNCcyosQtQeWMLCgA8+AJ5/Hpg1y/A1b2/dQnDGxrFUXTROUVtgqaiQg3IB96mwNG4su+AA61RZ2B1E1KAYWMgy588Dkyebt0+NKzA24FZhToXl4kW5qaCnZ82/2EaNAubNk4Nlq6ppHEtdu4ROnZJdTMHBwJ87rbsFa45jYWAhalAMLGSZN98EFi4EFi+2d0saRk2BxZwKi1Jd6dixeoXGXEoFxFhgqboGi6K25fmVfY2cYdNDa7LmOBYGFqIGxcBCltmxQ94XFNi3HQ3F2AwhhTlTm2vrDjJHTYHFVIVFmSV09aoch1PVl1/K+6SkurfLGVmrwiKELvRwDRaiBsHAQua7dEn3P/Nr1+zbloZSU4XFnKnNtg4spgbdBgXJ1V2B6t13BQW6DRfT0ureLmdkrcCSkyO/j56e7jMGiMjOGFjIfDt36h7XtBmfK6mpwmLO1OYDB+R9fQKLMobl/HmgtFT3fHm5HB8DVO8SAkyPY1m1Sg667dy5fu1yRsr047w83c+2LpTA07Zt3bv6iMgiDCxkvu+/1z1mhUWqaeDt1atAdrZ8nJBQ9zY0aSJvgGEw+uMPee/nZ3zjPVOBZdkyeT96tHuNXwFk5SkuTj6uzzgWZasFdgcRNRgGFjKfMn4FcJ8KS22BpaYKi1JdadUKCAmpXzuMdQvpdwcZCx7KwFv9wHL0KLB3r1zbJTW1fm1yVtboFvruO3nvbmOAiOyIgYXMU1io+18l4D6BpaYuIaDmCos1xq8ojAUWU2uwKJQKi/5MoYwMeT9woG6Mi7tRAsvhw3V7f3m5Lrz362edNhFRrRhYyDw//CAHl3p5ya/ZJSTVVGGxZmBRxrHoLx5naoaQomqX0M2bctl/QHYHuasuXeT9oUN1e/8PPwBlZXIROnYJETUYBhZ3smAB4Osr/8G1lDJ+5c475b27VFjMHcNy/jzQpw+wbh2gVsvnbF1hMTVDSKEEluxsGVY2b5aDTZs2Be6/v/5tclZKYDl82Pj+TLX55ht5f9997jcGiMiOGFjcyccfA5WV8t5SSmAZOFDel5TIiouzKy83/UurrEw3K8dUl1CzZsCkSXJ6644dwMMPy6rL22/rqiH1GXCr0N8EUfm+19YlFB0tV869eVOGG6U76IkndHsXuaN27eT3paREtz2BJfQDCxE1GAYWd3H5sm6Q4ebNloWNkhLg55/lYyWwCCE35HNmRUVAixamf/Eo41e8vGoeNPv++/IX34wZcr+ac+eAadNkEGrWDIiKqn9bW7WSoaikRDc7qLYKi4eHbgG5n34CvvpKPh41qv7tcWbe3rrpzZZ2C+XmyrFcKhWQkmL9thGRSQws7kJ/SnJurmX/UO/eLbs5YmPlWAqlDO7s41gOHJBdPpmZMrxUpT/gtrbS/y23AG+8IVe9/eAD3diG+++3TreBj49u1o/SLVRbhQXQvef114EbN4DbbtN1ibizuo5j2bpV3t92m+luQiKyCQYWd7F9u+HXmzeb/15lRsTdd8tfvkFB8mtnH8ei3x2wf3/112sbv2JMQAAwbpwcH3HyJPCf/9SriQaUbqFff5UBMidHfm2qwgLoxrEov5jdebCtvroGFnYHEdkNA4u7UEJH797y3pLAolRnlPcqgcXZKyz6S9bv21f99boEFoVKJQfk+vvXrW3G6A+8zc+XY1M8PIDISNPvUQILIKs0w4dbrz3OrC6BRaNhYCGyIwYWd3D1qm7Niblz5f0PP5gXOMrLgT175OO775b3jRrJe1eqsBgLLLWtwdLQ9AOL0h0UGambam6MfmAZNEi3Yq67UwLLqVOG2x3U5NAhGRQDA4HkZNu1jYiMYmBxB99/LwfJduggV+Zs00b+71xZrbMme/bImUWRkbo1R1ylS0i/wmKtLiFb0g8stQ24VegHFncfbKsvIkIOiNbfdbk2SnWlb19ZrSKiBsXA4g6qdgf17y/vzekW0u8OUgaPKhUWZ+8S0q+wnDwJFBcbvu5ogUVZPC47W7eybm2BpV07GVoSErgqa1WWdgtt2SLv2R1EZBcMLO5AGXDbp4+81w8stU1vVgKL0h0EuEaF5cYNXZVCCWDKQm8KR+sSCg+XmxwKofuZ1jRDCJALBZ48KStlNXUduSNLAktpqW7BRQY/IrtgYHF1BQW6PYCUCkufPrKkfe6c/GVmyo0bQFaWfKwfWFyhwnLhghxE6ecH3HOPfK7qOBZHq7CoVLpuIaVqVluFBZDrt7ALozpLAsv338uu0dhY3erGRNSgGFhcnbIHULt2utkkgYG6AFJTt9C+fXJxuCZNdAttAa5RYVHGr8TFAd27y8emAoujVFgAXWBRvvfmBBYyTn+J/toqjVyOn8juGFhcXdXuIIU541iU/8XfdZecPqtwhWnNyviVli2BxET5uGpgUbqEHKXCAujGsShq6xIi0zp0kNWnq1d1qwebooxfYXcQkd0wsLi6qgNuFUpg2b5d7pljTNX1VxSuMK1Zv8KiBJaTJ3UhTK0GrlyRjx0psCgVFgUrLHXn56f7ftbULXThAnD8uAztSvchETU4BhZXVlSkG0haNXR07Cj/d15ebrhsv0Kt1g0y1B+/ArhehaVZM/m9EEIu1w/IsT9KN4EjrV3CwGJd5oxjUZbj79lTDnomIrtgYHFlP/wgB5a2aVP9F5tKVXO30Icfymm+wcFA166Gr1lSYSkrk2u+OBr9CgtQvVtIGb8SGupYOxu3bq3rngsNleORqO7MCSyczkzkEBhYXJkyfqVqdUVhKrAsWABMmCAfT5ok+/n1mTvotqxMhqU77zS3xQ1Hv8ICyM3sgOqBxZG6gwA5TVlZDI7VlfqrLbCo1cC338rHHL9CZFcMLK5MGb9SdcCt4t57ZRj59Vf5C1wIuavvc8/J1194AfjnP6u/z9xpzdnZcjDjnj1yirSjKC/XDbI0VWFxtDVY9CndQhxwW39KYPn1V6CiovrrmZlyUG5IiOwSIiK7YWBxVcXFuuXmTVVYQkPlUv2ArLLMmAG8/LL8es4c4K23jE/hNLfCUlioe1xQYG7Lbe/8eXkfFKQbn6IEll9/lYuEOWqFBdDNFGJgqb9bbpF/D27elD/7qt5+W96npXHhPSI7Y2BxVbt2yXJ2q1ZATIzp45RuoWnTZEABgH/9C5g50/R6E+ZWWIqKdI+VGTeOQH/8inKNkZFAdLRu4K0jrsGiGD8eePhh4Omn7d0S56dSme4W+vlnWWHx9ASmTm34thGRAQYWV2VqOnNVSmApLpb/eC9dWvs/znWpsFy9WvOxgBwgrOxCbEtVx68o9LuFHHENFkW7dsCXX+rG3VD9mAosSoB//HG5wi0R2RUDi6uqbcCtols3+Yvb0xP45BP5v/famDutWT+wmFNh+fvfZYl+27baj62PqjOEFPqBxZG7hMi6jAWW336ToRAA/vGPhm8TEVXDTllXVFIiy9lA7YHFw0PuF1RSImf0mEPpEqqokINpTU37tTSwKANeMzOBvn3Na0tdmFNhUWbgOGKXEFmXscDyzjuye3DgQODWW+3TLiIywMDiirKy5PiV2NjqVQRjlD2GzKVUWAAZdEwtpqU/hsWcLiGlG+bwYcvaYylTFRali+X4cd3aMaywuL5OnWR3aG4ukJ8vuyaXL5evTZtm37YRkRa7hFyRUl1JTrbN+X18dLv/1jSOxdIKixJYjhypc9PMYqrCEh0tw5tGA5w4IZ9jYHF9QUFyQT5AhuX33pPVw6Qkx1xDiMhNMbC4IqW0XXWFWmsyZxyLJYFFo9Edc+aM7fYpKi2V/4sGjFeflG4hBbuE3IPSLbRzJ/Cf/8jH06dzZ2YiB8LA4ooOHpT3tgws5izPb8ksocJCGVoUR4/WtWU1U6oroaHyVlXVwMIKi3tQAsu8eXLGXMeOwAMP2LdNRGSAgcXVlJXJXYcB3T/CtmDO1GZL1mFRuoMUthrHogQWU2N79AOLnx/36nEXyt+V69fl/Qsv6PZsIiKHwL+RruboUVmpCA8HoqJs9znW7hKyVmD5/Xe5Sq/+Z+tTBtxWHb+i0A8s4eHsEnAX+uH+llvk2itE5FAYWFyNMn6lSxfb/rK1dpdQ1cBS14G38+cDs2fLlXqNqa3CEh0NNGsmH7M7yH20bKkL4VOn6gaVE5HDYGBxNQ0xfgWwXYVFCQt1rbDk5cn7DRvkOhpV1VZhUal0VRYOuHUfHh7Aa68Bw4aZt3giETU4BhZXo19hsaXaKiw3bujGAwBybE1ZmenzKYHlrrtkaLh0SRc+LFFcLO8vXAB++aX667VVWACge3d5b+n6NOTcpkwBPvuM45aIHBQDiysRwnEqLPoDbj095X1N3UJKBaZFC8M1MSylBBZAVlmqqq3CAgBPPQWMHg0895zln09ERDZRp8CyaNEixMXFwc/PD7169cJPP/1k8tiMjAyoVCqDm5+fn8Exo0aNqnZMf2VTPjLfxYtAQYEMCB072vazaquwKN1BQUFAkybycU3dQkqFJTwc6NxZPq5vYFm/3vC1oiL5/QFqrrBERwPp6XKfJSIicggWB5bVq1dj6tSpmD17Nvbv34+uXbuiX79+yFcW4zIiODgYOTk52lt2dna1Y/r3729wzGeffWZp00jpDmrfHvD1te1nmVthCQ21X2A5dEhXUQF03UHh4YbbCxARkcOzOLC8++67GDduHEaPHo2OHTtiyZIlCAgIQHp6usn3qFQqREZGam8RERHVjvH19TU4JszU/jRkWkN1BwHmV1hCQoDGjeXjmrqEjAWWuswUUgKUMsPnq690r5kzfoWIiBySRYGlsrIS+/btQ0pKiu4EHh5ISUnB7t27Tb6vpKQEsbGxiImJweDBg3HUyCqm27dvR7NmzRAfH48JEybgSg3/G6+oqEBxcbHBjdBwA26B2heOUwJLfSosypoyllD+LAwfLu/1u4XMGb9CREQOyaLAcvnyZajV6moVkoiICOTm5hp9T3x8PNLT07FhwwasWLECGo0GycnJuHjxovaY/v3745NPPkFmZibeeust7NixAwMGDIBarTZ6zrlz5yIkJER7i4mJseQyXFdDVlhq6xKqT2Bp00Z2aV2/LvcVMteNG7qZSCNGyPudO3WVHVZYiIicls1nCSUlJWHkyJFISEhA7969sXbtWjRt2hRLly7VHjNs2DAMGjQInTt3xpAhQ7Bx40bs3bsX27dvN3rOGTNmoKioSHu7cOGCrS/D8TXUkvyK2rqE9Mew1NYldPOmbjBseLjhoGFLxrHoh6euXWWlRq0GNm2Sz7HCQkTktCwKLOHh4fD09ERelfUx8vLyEGnmmhXe3t7o1q0bTp06ZfKYVq1aITw83OQxvr6+CA4ONri5vYZakl9hboUlJKT2CktBgW6RNyXc1GXgrdId5O8PeHsDgwfLr5XpzaywEBE5LYsCi4+PDxITE5GZmal9TqPRIDMzE0lJSWadQ61W4/Dhw4iq4ZfqxYsXceXKlRqPoSoaakl+hbmDbs3pElK6g8LCAC8v+bguA2+VwKIEWCWwbN4MlJezwkJE5MQs7hKaOnUqPvzwQyxfvhzHjx/HhAkTUFpaitGjRwMARo4ciRkzZmiPnzNnDr755hucOXMG+/fvxxNPPIHs7GyMHTsWgByQ+8ILL+DHH3/EuXPnkJmZicGDB6NNmzbo16+flS7TDTTk+BXAsjEstXUJ6Y9fUdSnwqKEqcREoHlzoLQUWLNG19bYWPPPSUREDsHL0jcMHToUly5dwqxZs5Cbm4uEhARs3rxZOxD3/Pnz8NDblr2goADjxo1Dbm4uwsLCkJiYiKysLHT8c4yCp6cnDh06hOXLl6OwsBDR0dG477778Nprr8HX1muJuJKGnCEEGFZYhKhe1bFkHZaaAstvv8nqSJXFBo1SAolSYVGpZJXlP/8BFiyQz0VGyi4jIiJyKhYHFgCYNGkSJk2aZPS1qgNl58+fj/nz55s8l7+/P7Zs2VKXZpCiIZfkVygVFo1GDvgNCDB83ZIxLEpgUY4D5DicsDA5vuX4cfNWna3aJQToAsv+/fJrjl8hInJK3EvIFTTkkvwK/Q3ijI1jMdUlZGwHZSXI6FdYVCrLu4WMBZY+fQy/5vgVIiKnxMDiChpySX6Fh4cutNQWWJTKyc2bxse8GOsSAqwTWHx8gAEDdF+zwkJE5JQYWFxBQ3cHKWoaeKs/hsXfXzduxFi3UG2BxdyZQsYCCwAMGaJ7zAoLEZFTYmBxBQ094FZhamqzRqMLDyEh8l7pFqpLYKlPhQWQFRZvb/mYFRYiIqfEwOIKHK3CUlysG6uiBBalW8jY1GZTgeXWW+X977/rVsKtSdVpzYqQEGDWLKB/f+Cuu2o/DxERORwGFmfX0Evy6zNVYVG6g/z8dNOR61JhCQ4GWrSQj82pspiqsADAyy8DX39t3vRoIiJyOAwszq6hl+TXZ6rCoj/gVlHT1GZTgQWwrFuo6josRETkMhhYnF1DL8mvz1SFRX8NFoWpLqEbN3QVmZoCizkDb2uqsBARkVNjYHF29hq/AlhWYTHVJaR87eFheLzCkgoLAwsRkctiYHF29pohBOgCi6kxLOZ0CSndQY0by4XvqtKvsBhbdE4fAwsRkctiYHFm9liSX5/SJWSqwmJOl1BN41cAID5eVl+KioC8vJrbw8BCROSyGFic2ZUruum+HTo0/OebqrBY0iVUW2Dx8ZF7CgGmd3sGZHhjYCEiclkMLM5MCQaNGtlnum5tg27N6RJSvtbf+LCqmqZEK0pLdV1GVddhISIip8fA4syMBYOGZGrQbU1jWCztEgIMN080RWmDh0f1naOJiMjpMbA4M3sHFkumNSuho7AQUKt1z1srsOh3BzX09G4iIrI5BhZnZiwYNKS6TGsWwnCZfVsEFiIicjkMLM7M0Sss+u3y9taFCf3gwcBCRERmYGBxZsbGijQkS8awAMYHz5oTWGraOFHBwEJE5NIYWJyZvSsstU1rrtpVZWymkCUVlppmCTGwEBG5NAYWZ2bvwKJ0CZWVATdvysdCmG6XsUqJtbuEOKWZiMglMbA4M3sHFqXCAsh1UADg+nXdLKDauoTKy3XVGWtNa2aFhYjIJTGwODN7zxLy9QW8vORjJXgobfLyqr4eStUuIeXe07Pma+CgWyIit8fA4szsXWFRqaoPvNUPUVXXQ6naJaTfHVTT2ikMLEREbo+BxZnZe5YQUH1qc00hqmqXkHJfU3cQoAs6164BN24YP4aBhYjIpTGwODN7V1iA6hWWmkJU1S4hpcJS0z5CgGG1xlSVhYGFiMilMbA4M0cILKYqLMbGpNTUJVQTT0/dNTKwEBG5JQYWZ3Xzpq6q4UgVFku6hMwNLPrvrS2wcFozEZFLYmBxVsovaMB+s4SA6ovH1RRYTHUJWTOwsMJCROSSGFiclRIMAgLkPj32olQ0LBnDUloKVFRYFlhqW56f67AQEbk0BhZn5QgzhADTFRZjVZ+QEMDjzz9yV6/WrcJianl+VliIiFwaA4uzcoQBt4Bl05o9PICwMPn4yhXrdQlVVspVcwEGFiIiF8XA4qwcJbBYMq0ZMOzasVZg0d8tmoNuiYhcEgOLs3KUwGJJhQUw7NqxVmBRuoMCAnRbBRARkUthYHFW9t5HSFHT0vzGKBWWCxfkLs+A9QILu4OIiFwWA4uzctYKixJYTp6U9z4+hrs+m1LTLCGuwUJE5PIYWJyVowQWS8ewKJUSJbDUtvFh1fcZmyXEKc1ERC6PgcVZOeK05vJy3Wyd2iosJ07Ie3O6gwB2CRERuTkGFmflKBUW/YXjlBClUpnunlECy/nzhl/XRgksxcXVd2xmYCEicnkMLM7KUQKLfoVFaVNwsG6BuKqU4KEwt8Kif53K5ygYWIiIXB4Di7NylFlC+hUWc0JU1YqKuYHFy8v0js0MLERELo+BxVk5WoVFrQby8uRjWwQWwPTAWwYWIiKXx8DirBwtsADAxYvyvqaqT127hPTfa6rCwmnNREQui4HFGWk0ul/S9g4snp6Av798rAQWW1dY2CVEROR2GFic0bVrgBDysb3HsAC6Kos5gSUwUC4Wp7BGYOE6LERELo+BxRkp3UF+fvJmb0pXzIUL8r6mEKVSGXYLscJCRERmYGBxRo4yQ0hhSYUFMOwWsiSwmFqen4GFiMjlMbA4I0cZcKtQKiy2DiycJURE5LYYWJyRowUWpcJS27L8CiV4+PsDAQHmfw67hIiI3BYDizNylH2EFFWnE9fWVaVUWCyprgDGA4tGoxt0y2nNREQui4HFGTlqhUVhbpeQNQJLaaluxhQrLERELouBxRk5WmCpWtkwt0vI3I0PFcYG3SrVFf31YIiIyOUwsDgjR50lpKgtsHTrJu9vu82yz1GCTmEhcPOmfKw/fkWlsux8RETkNLzs3QCqA0ersFQNLLUFqfvuA86fB5o3t+xzwsJ0jwsLZZcSB9wSEbkFVlickaMFFksH3QJATAzgYeEfPy8vXTBRuoUYWIiI3EKdAsuiRYsQFxcHPz8/9OrVCz/99JPJYzMyMqBSqQxufjWszvrUU09BpVJhwYIFdWmae3C0WUL6FZbAQMDb23afVXXgLQMLEZFbsDiwrF69GlOnTsXs2bOxf/9+dO3aFf369UN+fr7J9wQHByMnJ0d7y87ONnrcunXr8OOPPyI6OtrSZrkXR66w2LpNDCxERG7J4sDy7rvvYty4cRg9ejQ6duyIJUuWICAgAOnp6Sbfo1KpEBkZqb1FRERUO+b333/HM888g08//RTetvwfuitwtMCiX2Gx9UDgqjOFlMDCNViIiFyaRYGlsrIS+/btQ0pKiu4EHh5ISUnB7t27Tb6vpKQEsbGxiImJweDBg3H06FGD1zUaDUaMGIEXXngBnTp1qrUdFRUVKC4uNri5FUebJWSPCouyPD93aiYicgsWBZbLly9DrVZXq5BEREQgNzfX6Hvi4+ORnp6ODRs2YMWKFdBoNEhOTsZFZd8ZAG+99Ra8vLzw7LPPmtWOuXPnIiQkRHuLiYmx5DKcmxCOXWFhlxAREdmAzWcJJSUlYeTIkUhISEDv3r2xdu1aNG3aFEuXLgUA7Nu3D++99552cK45ZsyYgaKiIu3twoULtrwEx1JSIpejBxwnsOhXWGxd9WFgISJySxYFlvDwcHh6eiIvL8/g+by8PERGRpp1Dm9vb3Tr1g2nTp0CAOzcuRP5+flo0aIFvLy84OXlhezsbPz9739HXFyc0XP4+voiODjY4ObQ1q8HXntNt4R8fSjVFW9vx1nZlRUWIiKyMYsCi4+PDxITE5GZmal9TqPRIDMzE0lJSWadQ61W4/Dhw4iKigIAjBgxAocOHcKBAwe0t+joaLzwwgvYsmWLJc1zXBMnArNmAVXG7tSJ/pRmR1nZlYGFiIhszOKVbqdOnYq0tDR0794dPXv2xIIFC1BaWorRo0cDAEaOHInmzZtj7ty5AIA5c+bg9ttvR5s2bVBYWIh58+YhOzsbY8eOBQA0adIETarsKePt7Y3IyEjEx8fX9/rsTwhAmfJ96VL9z+do41cAWenx8JBdVbZul6lZQgwsREQuzeLAMnToUFy6dAmzZs1Cbm4uEhISsHnzZu1A3PPnz8NDbwXTgoICjBs3Drm5uQgLC0NiYiKysrLQsWNH612FIyst1e17o4SN+nC0GUKArPQEBcnw0FBjWJRZQpzWTETkFuq0l9CkSZMwadIko69t377d4Ov58+dj/vz5Fp3/3LlzdWmWYyoo0D22ZmBxpAoLIANDcTG7hIiIyCa4l5CtuUtgUQJDQwWWwkJAreY6LEREboKBxdb0Q4orB5a//x0YNAi46y7bfo6yY7MQcgAyKyxERG6BgcXWrF1hcbSNDxVjxgAbNgABAbb9HB8f3XiVnBygokI+ZmAhInJpDCy2ph9Y9B/XlaNWWBqS0i2kP9aJg26JiFwaA4ut2WoMiyPNEmpoVQNLYCDg6Wm35hARke0xsNiauwy6bUhVAwurK0RELo+BxdYYWKyvamDh+BUiIpfHwGJrDCzWpwSW7Gx5z8BCROTyGFhszV1mCTUkZXl+VliIiNwGA4ut6YeUa9d0y/TXhRAcdAvoKizK3kwMLERELo+BxdaqTmVWKiR1UVYG3LghH7tzhUUJLAoGFiIil8fAYmtVA0t9uoWU93p4yM0G3RUDCxGR22FgsTUlsCg7WFsjsISGyh2S3RUDCxGR22FgsaWyMt3S8bfcIu+tFVjcmTLoVsF1WIiIXB4Diy0p1RVPTyAmRj6uT2DhDCGJFRYiIrfDwGJLSmAJDdXtMmyNCos7zxACdN9LBQMLEZHLY2CxJf3AolRF2CVUf76+cv8gBQMLEZHLY2CxJSVghIXpQkZ9dmxmYNHR7xZiYCEicnkMLLakhBP9wMIKi3UwsBARuRUGFltiYLEd/ZlCDCxERC6PgcWWrB1YOEtIR7/CwmnNREQuj4HFlvQDC2cJWRe7hIiI3AoDiy2xS8h2lMDi5QX4+dm3LUREZHMMLLbEwGI7SmAJDnbvbQqIiNwEA4stGVuHhdOarUM/sBARkctjYLElY+uwXL8OVFbW73wMLLpZQgwsRERugYHFlvS7hPR/sSqzfSxRXq7bSJGBBejTB0hOBsaNs3dLiIioAXjZuwEuTT+weHnJ6bfXrslKSdOmlp1LCTkqFafxAjK07dpl71YQEVEDYYXFViorZfcPoJvSXJ+pzcp7goMBD/7YiIjIvfA3n60o1RWVSrduSn1mCnH8ChERuTEGFltRAktIiK4iUp+ZQgwsRETkxhhYbEV//IqCFRYiIqI6YWCxFf01WBQMLERERHXCwGIr+muwKOoTWJRZQtxHiIiI3BADi60Y6xKyxiwhVliIiMgNMbDYCsewEBERWQ0Di60wsBAREVkNA4ut1BRYOK2ZiIjIIgwstmKrCgsH3RIRkRtiYLEVaweWa9fkPXcnJiIiN8TAYivWXodFCSzc+JCIiNwQA4utGFuHRXlcXi5vlmBgISIiN8bAYivGuoQaNZKbIQK6heDMxcBCRERujIHFFm7e1AUM/cDi4aEbNGtJt1BFBXDjhnzMwEJERG6IgcUW9MNI1WnIdZnarIQfAAgKqmOjiIiInBcDiy0oYaRRI8DLy/C1ugy8VQKLv3/18xEREbkBBhZbMDZ+RVGfwMLuICIiclMMLLbAwEJERGRVDCy2YGwNFkVddmxmYCEiIjfHwGILxtZgUbDCQkREZDEGFluwdpdQcbG8Z2AhIiI3xcBiC+YElrpMa2ZgISIiN8XAYgscdEtERGRVDCy2wMBCRERkVXUKLIsWLUJcXBz8/PzQq1cv/PTTTyaPzcjIgEqlMrj5+fkZHPPKK6+gffv2CAwMRFhYGFJSUrBnz566NM0xMLAQERFZlcWBZfXq1Zg6dSpmz56N/fv3o2vXrujXrx/y8/NNvic4OBg5OTnaW3Z2tsHr7dq1w7///W8cPnwYP/zwA+Li4nDffffh0qVLll+RI6gpsHBaMxERkcUsDizvvvsuxo0bh9GjR6Njx45YsmQJAgICkJ6ebvI9KpUKkZGR2ltERITB648//jhSUlLQqlUrdOrUCe+++y6Ki4tx6NAhy6/IEdS0Dot+hUUI887HwEJERG7OosBSWVmJffv2ISUlRXcCDw+kpKRg9+7dJt9XUlKC2NhYxMTEYPDgwTh69GiNn/HBBx8gJCQEXbt2NXpMRUUFiouLDW4OxZx1WCorgbIy887HwEJERG7OosBy+fJlqNXqahWSiIgI5ObmGn1PfHw80tPTsWHDBqxYsQIajQbJycm4ePGiwXEbN25EUFAQ/Pz8MH/+fGzduhXh4eFGzzl37lyEhIRobzExMZZchm1pNEBRkXxsLLAEBQEef37bze0WYmAhIiI3Z/NZQklJSRg5ciQSEhLQu3dvrF27Fk2bNsXSpUsNjuvbty8OHDiArKws9O/fH4899pjJcTEzZsxAUVGR9nbhwgVbX4b5iop0XT3GAotKZfnAWwYWIiJycxYFlvDwcHh6eiIvL8/g+by8PERGRpp1Dm9vb3Tr1g2nTp0yeD4wMBBt2rTB7bffjo8//hheXl74+OOPjZ7D19cXwcHBBjeHoYxfCQgAfHyMH8PAQkREZBGLAouPjw8SExORmZmpfU6j0SAzMxNJSUlmnUOtVuPw4cOIioqq8TiNRoOKigpLmucYapohpGBgISIisoiXpW+YOnUq0tLS0L17d/Ts2RMLFixAaWkpRo8eDQAYOXIkmjdvjrlz5wIA5syZg9tvvx1t2rRBYWEh5s2bh+zsbIwdOxYAUFpaitdffx2DBg1CVFQULl++jEWLFuH333/Ho48+asVLbSDmBBZLpjYLAZSUyMcMLERE5KYsDixDhw7FpUuXMGvWLOTm5iIhIQGbN2/WDsQ9f/48PDx0hZuCggKMGzcOubm5CAsLQ2JiIrKystCxY0cAgKenJ3799VcsX74cly9fRpMmTdCjRw/s3LkTnTp1stJlNiBrV1hKS3VjYhhYiIjITVkcWABg0qRJmDRpktHXtm/fbvD1/PnzMX/+fJPn8vPzw9q1a+vSDMdU0xosCks2QFS6g1QqIDCwPi0jIiJyWtxLyNpqWoNFYUmFRQksQUEytBAREbkhBhZrs3aXEAfcEhERMbBYHQMLERGR1TGwWBsDCxERkdUxsFibtac1M7AQERExsFgdKyxERERWx8BibZYEFkumNTOwEBGRG2NgsTZLKyzKonCmMLAQERExsFiVELpuHnMWjlOr5Uq2NWFgISIiYmCxqmvXAI1GPq6pwhIQAHj9uchwbeNYGFiIiIgYWKxK6Q7y9QX8/U0fp1KZP1OIgYWIiIiBxarMGb+iMHemkBJYgoPr2ioiIiKnx8BiTXUJLLXNFGKFhYiIiIHFqmxZYWFgISIiN8bAYk0MLERERDbBwGIOJTTUhoGFiIjIJhhYapKbCzz8MPCXv8g1U2pjzhosCgYWIiIiszGw1EStBr79FtizB1i8uPbjz5+X9+ZUWMyZ1nzzJlBWJh8zsBARkRtjYKlJ8+bAm2/KxzNmABcumD42Kwv49FP5+K67aj+3ORWWkhLdYwYWIiJyYwwstXnqKSApSYaHiRON7/1TUgKMHClXuR05UnYh1cacac1Kd5C3t1yMjoiIyE0xsNTGwwP48EMZGv77X+DLL6sf8/zzwOnTQIsWwMKF5p3XnAoLx68QEREBYGAxT6dOwLRp8vEzzxiGjP/9D1i6VD7OyABCQsw7pyUVFgYWIiJycwws5nrpJSA+Xs4cmj5dPnflCjBmjHw8ZQrQt6/55wsPl/eXL5s+hoGFiIgIAAOL+fz8dJWUpUuBnTuBCRNkgOnQAXjjDcvO16yZvC8pAa5fN34MAwsREREABhbL9O4NjB0rHw8ZAqxZA3h5Af/3fzXvzmxMcLBuIG1envFjGFiIiIgAMLBY7u23gYgI4OpV+fWsWUBiouXnUankeQAGFiIiolowsFgqLAxYtEg+TkqS67PUlRJY8vONv87AQkREBADwsncDnNIjjwDHjwMxMbJLqK5YYSEiIjILA0tdtW9f/3MoA28ZWIiIiGrELiF7YoWFiIjILAws9sTAQkREZBYGFnvioFsiIiKzMLDYEyssREREZmFgsScGFiIiIrMwsNiTMkuooACorKz+OgMLERERAAYW+2rcGPD0lI+NjWNhYCEiIgLAwGJfHh41r8XCwEJERASAgcX+TM0UqqgAbtyQjxlYiIjIzTGw2JupgbdKdQUAgoIarj1EREQOiIHF3moLLP7+9duviIiIyAUwsNibqTEsHL9CRESkxcBib7VVWBhYiIiIGFjsztSgWwYWIiIiLQYWe2OFhYiIqFYMLPbGwEJERFQrBhZ7UwbdXr4MqNW65xlYiIiItBhY7K1pU0ClAjQaGVoUDCxERERaDCz25uUFNGkiH+t3CzGwEBERaTGwOAJjM4UYWIiIiLQYWByBsYG3DCxERERaDCyOgIGFiIioRgwsjsDY8vwMLERERFoMLI6AFRYiIqIaMbA4AgYWIiKiGtUpsCxatAhxcXHw8/NDr1698NNPP5k8NiMjAyqVyuDm5+enff3GjRuYNm0aOnfujMDAQERHR2PkyJH4448/6tI058RZQkRERDWyOLCsXr0aU6dOxezZs7F//3507doV/fr1Q37Vzfv0BAcHIycnR3vLzs7Wvnb9+nXs378fM2fOxP79+7F27VqcOHECgwYNqtsVOSNWWIiIiGqkEkIIS97Qq1cv9OjRA//+978BABqNBjExMXjmmWcwffr0asdnZGRgypQpKCwsNPsz9u7di549eyI7OxstWrSo9fji4mKEhISgqKgIwcHBZn+Owzh/HoiNBby9gYoK+ZyXl1z99o8/gKgo+7aPiIjIBiz5/W1RhaWyshL79u1DSkqK7gQeHkhJScHu3btNvq+kpASxsbGIiYnB4MGDcfTo0Ro/p6ioCCqVCqGhoUZfr6ioQHFxscHNqSmzhG7cAAoKgOvXZVgBWGEhIiKChYHl8uXLUKvViFC6MP4UERGB3Nxco++Jj49Heno6NmzYgBUrVkCj0SA5ORkXL140enx5eTmmTZuG4cOHm0xbc+fORUhIiPYWExNjyWU4Hj8/ICREPs7L03UHqVRAYKD92kVEROQgbD5LKCkpCSNHjkRCQgJ69+6NtWvXomnTpli6dGm1Y2/cuIHHHnsMQggsXrzY5DlnzJiBoqIi7e3ChQu2vISGoT/wVgksQUEytBAREbk5L0sODg8Ph6enJ/L0B4cCyMvLQ2RkpFnn8Pb2Rrdu3XDq1CmD55Wwkp2dje+++67GvixfX1/4+vpa0nTHFxEBnDwpKyxKNxC7g4iIiABYWGHx8fFBYmIiMjMztc9pNBpkZmYiKSnJrHOo1WocPnwYUXoDSZWw8ttvv+Hbb79FE2X3YneiP1OIM4SIiIgMWFRhAYCpU6ciLS0N3bt3R8+ePbFgwQKUlpZi9OjRAICRI0eiefPmmDt3LgBgzpw5uP3229GmTRsUFhZi3rx5yM7OxtixYwHIsPLXv/4V+/fvx8aNG6FWq7XjYRo3bgwfHx9rXatj01+en4GFiIjIgMWBZejQobh06RJmzZqF3NxcJCQkYPPmzdqBuOfPn4eHh65wU1BQgHHjxiE3NxdhYWFITExEVlYWOnbsCAD4/fff8dVXXwEAEhISDD5r27Zt6NOnTx0vzcmwwkJERGSSxeuwOCKnX4cFAJYuBZ56CnjwQeCBB4C//Q0YNAjYsMHeLSMiIrIJm63DQjZkbJYQKyxEREQAGFgcB7uEiIiITGJgcRQMLERERCYxsDgKZZZQWRmQkyMfO+t4HCIiIitjYHEUQUFAQIB8rCyqxwoLERERAAYWx6J0CzGwEBERGWBgcSRKYCkokPcMLERERAAYWBxLlV2wGViIiIgkBhZHogy8VTCwEBERAWBgcSyssBARERnFwOJIGFiIiIiMYmBxJAwsRERERjGwOBIGFiIiIqMYWByJfmDx9gZ8fe3XFiIiIgfCwOJI9GcJsbpCRESkxcDiSEJDAR8f+ZiBhYiISIuBxZGoVLoqCwMLERGRFgOLo1HGsTCwEBERaTGwOBoGFiIiomoYWBwNAwsREVE1DCyOhmNYiIiIqmFgcTSDBgEtWwJDhti7JURERA7Dy94NoCqSk4EzZ+zdCiIiIofCCgsRERE5PAYWIiIicngMLEREROTwGFiIiIjI4TGwEBERkcNjYCEiIiKHx8BCREREDo+BhYiIiBweAwsRERE5PAYWIiIicngMLEREROTwGFiIiIjI4TGwEBERkcNjYCEiIiKH52XvBliDEAIAUFxcbOeWEBERkbmU39vK7/GauERguXbtGgAgJibGzi0hIiIiS127dg0hISE1HqMS5sQaB6fRaPDHH3+gUaNGUKlUdTpHcXExYmJicOHCBQQHB1u5hY6B1+g63OE6eY2ugdfoGmx1jUIIXLt2DdHR0fDwqHmUiktUWDw8PHDLLbdY5VzBwcEu+wdOwWt0He5wnbxG18BrdA22uMbaKisKDrolIiIih8fAQkRERA6PgeVPvr6+mD17Nnx9fe3dFJvhNboOd7hOXqNr4DW6Bke4RpcYdEtERESujRUWIiIicngMLEREROTwGFiIiIjI4TGwEBERkcNjYPnTokWLEBcXBz8/P/Tq1Qs//fSTvZtUZ99//z0efPBBREdHQ6VSYf369QavCyEwa9YsREVFwd/fHykpKfjtt9/s09g6mjt3Lnr06IFGjRqhWbNmGDJkCE6cOGFwTHl5OSZOnIgmTZogKCgIjzzyCPLy8uzUYsstXrwYXbp00S7UlJSUhK+//lr7urNfnzFvvvkmVCoVpkyZon3O2a/zlVdegUqlMri1b99e+7qzX5/i999/xxNPPIEmTZrA398fnTt3xs8//6x93dn/3YmLi6v2c1SpVJg4cSIA1/g5qtVqzJw5Ey1btoS/vz9at26N1157zWCfH7v+HAWJVatWCR8fH5Geni6OHj0qxo0bJ0JDQ0VeXp69m1Yn//vf/8RLL70k1q5dKwCIdevWGbz+5ptvipCQELF+/Xpx8OBBMWjQINGyZUtRVlZmnwbXQb9+/cSyZcvEkSNHxIEDB8T9998vWrRoIUpKSrTHPPXUUyImJkZkZmaKn3/+Wdx+++0iOTnZjq22zFdffSU2bdokTp48KU6cOCFefPFF4e3tLY4cOSKEcP7rq+qnn34ScXFxokuXLmLy5Mna5539OmfPni06deokcnJytLdLly5pX3f26xNCiKtXr4rY2FgxatQosWfPHnHmzBmxZcsWcerUKe0xzv7vTn5+vsHPcOvWrQKA2LZtmxDCNX6Or7/+umjSpInYuHGjOHv2rFizZo0ICgoS7733nvYYe/4cGViEED179hQTJ07Ufq1Wq0V0dLSYO3euHVtlHVUDi0ajEZGRkWLevHna5woLC4Wvr6/47LPP7NBC68jPzxcAxI4dO4QQ8pq8vb3FmjVrtMccP35cABC7d++2VzPrLSwsTHz00Ucud33Xrl0Tbdu2FVu3bhW9e/fWBhZXuM7Zs2eLrl27Gn3NFa5PCCGmTZsm7rzzTpOvu+K/O5MnTxatW7cWGo3GZX6OAwcOFE8++aTBcw8//LBITU0VQtj/5+j2XUKVlZXYt28fUlJStM95eHggJSUFu3fvtmPLbOPs2bPIzc01uN6QkBD06tXLqa+3qKgIANC4cWMAwL59+3Djxg2D62zfvj1atGjhlNepVquxatUqlJaWIikpyeWub+LEiRg4cKDB9QCu83P87bffEB0djVatWiE1NRXnz58H4DrX99VXX6F79+549NFH0axZM3Tr1g0ffvih9nVX+3ensrISK1aswJNPPgmVSuUyP8fk5GRkZmbi5MmTAICDBw/ihx9+wIABAwDY/+foEpsf1sfly5ehVqsRERFh8HxERAR+/fVXO7XKdnJzcwHA6PUqrzkbjUaDKVOm4I477sCtt94KQF6nj48PQkNDDY51tus8fPgwkpKSUF5ejqCgIKxbtw4dO3bEgQMHXOL6AGDVqlXYv38/9u7dW+01V/g59urVCxkZGYiPj0dOTg5effVV3HXXXThy5IhLXB8AnDlzBosXL8bUqVPx4osvYu/evXj22Wfh4+ODtLQ0l/t3Z/369SgsLMSoUaMAuMafUwCYPn06iouL0b59e3h6ekKtVuP1119HamoqAPv//nD7wELOb+LEiThy5Ah++OEHezfF6uLj43HgwAEUFRXhiy++QFpaGnbs2GHvZlnNhQsXMHnyZGzduhV+fn72bo5NKP87BYAuXbqgV69eiI2Nxeeffw5/f387tsx6NBoNunfvjjfeeAMA0K1bNxw5cgRLlixBWlqanVtnfR9//DEGDBiA6OhoezfFqj7//HN8+umnWLlyJTp16oQDBw5gypQpiI6Odoifo9t3CYWHh8PT07PaaO68vDxERkbaqVW2o1yTq1zvpEmTsHHjRmzbtg233HKL9vnIyEhUVlaisLDQ4Hhnu04fHx+0adMGiYmJmDt3Lrp27Yr33nvPZa5v3759yM/Px2233QYvLy94eXlhx44dWLhwIby8vBAREeES16kvNDQU7dq1w6lTp1zm5xgVFYWOHTsaPNehQwdt15cr/buTnZ2Nb7/9FmPHjtU+5yo/xxdeeAHTp0/HsGHD0LlzZ4wYMQLPPfcc5s6dC8D+P0e3Dyw+Pj5ITExEZmam9jmNRoPMzEwkJSXZsWW20bJlS0RGRhpcb3FxMfbs2eNU1yuEwKRJk7Bu3Tp89913aNmypcHriYmJ8Pb2NrjOEydO4Pz58051nVVpNBpUVFS4zPXde++9OHz4MA4cOKC9de/eHampqdrHrnCd+kpKSnD69GlERUW5zM/xjjvuqLaswMmTJxEbGwvAdf7dAYBly5ahWbNmGDhwoPY5V/k5Xr9+HR4ehrHA09MTGo0GgAP8HG0+rNcJrFq1Svj6+oqMjAxx7NgxMX78eBEaGipyc3Pt3bQ6uXbtmvjll1/EL7/8IgCId999V/zyyy8iOztbCCGnpYWGhooNGzaIQ4cOicGDBzvV9EIhhJgwYYIICQkR27dvN5hqeP36de0xTz31lGjRooX47rvvxM8//yySkpJEUlKSHVttmenTp4sdO3aIs2fPikOHDonp06cLlUolvvnmGyGE81+fKfqzhIRw/uv8+9//LrZv3y7Onj0rdu3aJVJSUkR4eLjIz88XQjj/9Qkhp6R7eXmJ119/Xfz222/i008/FQEBAWLFihXaY1zh3x21Wi1atGghpk2bVu01V/g5pqWliebNm2unNa9du1aEh4eLf/zjH9pj7PlzZGD50/vvvy9atGghfHx8RM+ePcWPP/5o7ybV2bZt2wSAare0tDQhhJyaNnPmTBERESF8fX3FvffeK06cOGHfRlvI2PUBEMuWLdMeU1ZWJp5++mkRFhYmAgICxEMPPSRycnLs12gLPfnkkyI2Nlb4+PiIpk2binvvvVcbVoRw/uszpWpgcfbrHDp0qIiKihI+Pj6iefPmYujQoQbrkzj79Sn++9//iltvvVX4+vqK9u3biw8++MDgdVf4d2fLli0CgNF2u8LPsbi4WEyePFm0aNFC+Pn5iVatWomXXnpJVFRUaI+x589RJYTeEnZEREREDsjtx7AQERGR42NgISIiIofHwEJEREQOj4GFiIiIHB4DCxERETk8BhYiIiJyeAwsRERE5PAYWIjIZahUKqxfv97ezSAiG2BgISKrGDVqFFQqVbVb//797d00InIBXvZuABG5jv79+2PZsmUGz/n6+tqpNUTkSlhhISKr8fX1RWRkpMEtLCwMgOyuWbx4MQYMGAB/f3+0atUKX3zxhcH7Dx8+jHvuuQf+/v5o0qQJxo8fj5KSEoNj0tPT0alTJ/j6+iIqKgqTJk0yeP3y5ct46KGHEBAQgLZt2+Krr77SvlZQUIDU1FQ0bdoU/v7+aNu2bbWARUSOiYGFiBrMzJkz8cgjj+DgwYNITU3FsGHDcPz4cQBAaWkp+vXrh7CwMOzduxdr1qzBt99+axBIFi9ejIkTJ2L8+PE4fPgwvvrqK7Rp08bgM1599VU89thjOHToEO6//36kpqbi6tWr2s8/duwYvv76axw/fhyLFy9GeHh4w30DiKjuGmSLRSJyeWlpacLT01MEBgYa3F5//XUhhNxh+6mnnjJ4T69evcSECROEEEJ88MEHIiwsTJSUlGhf37Rpk/Dw8BC5ublCCCGio6PFSy+9ZLINAMTLL7+s/bqkpEQAEF9//bUQQogHH3xQjB492joXTEQNimNYiMhq+vbti8WLFxs817hxY+3jpKQkg9eSkpJw4MABAMDx48fRtWtXBAYGal+/4447oNFocOLECahUKvzxxx+49957a2xDly5dtI8DAwMRHByM/Px8AMCECRPwyCOPYP/+/bjvvvswZMgQJCcn1+laiahhMbAQkdUEBgZW66KxFn9/f7OO8/b2NvhapVJBo9EAAAYMGIDs7Gz873//w9atW3Hvvfdi4sSJeOedd6zeXiKyLo5hIaIG8+OPP1b7ukOHDgCADh064ODBgygtLdW+vmvXLnh4eCA+Ph6NGjVCXFwcMjMz69WGpk2bIi0tDStWrMCCBQvwwQcf1Ot8RNQwWGEhIqupqKhAbm6uwXNeXl7aga1r1qxB9+7dceedd+LTTz/FTz/9hI8//hgAkJqaitmzZyMtLQ2vvPIKLl26hGeeeQYjRoxAREQEAOCVV17BU089hWbNmmHAgAG4du0adu3ahWeeecas9s2aNQuJiYno1KkTKioqsHHjRm1gIiLHxsBCRFazefNmREVFGTwXHx+PX3/9FYCcwbNq1So8/fTTiIqKwmeffYaOHTsCAAICArBlyxZMnjwZPXr0QEBAAB555BG8++672nOlpaWhvLwc8+fPx/PPP4/w8HD89a9/Nbt9Pj4+mDFjBs6dOwd/f3/cddddWLVqlRWunIhsTSWEEPZuBBG5PpVKhXXr1mHIkCH2bgoROSGOYSEiIiKHx8BCREREDo9jWIioQbD3mYjqgxUWIiIicngMLEREROTwGFiIiIjI4TGwEBERkcNjYCEiIiKHx8BCREREDo+BhYiIiBweAwsRERE5PAYWIiIicnj/D7ahe3aWtXcgAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load txt file\n",
    "path = f\"/kaggle/working/{config['ID']}_train_val_losses.csv\"\n",
    "train_val_losses = pd.read_csv(path)\n",
    "\n",
    "# Plot and Save Train & Val Losses as png in output dir\n",
    "training_plot(train_val_losses, OUTPUT_DIR, config)\n",
    "\n",
    "# Plot and Save Val Metric as png in output dir\n",
    "validation_metric_plot(train_val_losses, OUTPUT_DIR, config)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 4050810,
     "sourceId": 36363,
     "sourceType": "competition"
    },
    {
     "datasetId": 5564224,
     "sourceId": 9202981,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5569045,
     "sourceId": 9210129,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5574103,
     "sourceId": 9217666,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5620645,
     "sourceId": 9285436,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5635939,
     "sourceId": 9306874,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5639156,
     "sourceId": 9311278,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5641482,
     "sourceId": 9314624,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5645281,
     "sourceId": 9319733,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5647456,
     "sourceId": 9322751,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 194700142,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35123.077313,
   "end_time": "2024-09-05T15:36:33.745492",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-05T05:51:10.668179",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
