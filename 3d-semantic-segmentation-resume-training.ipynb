{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5812ebc4",
   "metadata": {
    "papermill": {
     "duration": 0.007611,
     "end_time": "2024-08-21T11:11:13.454968",
     "exception": false,
     "start_time": "2024-08-21T11:11:13.447357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3D Cervical Vertebrae Segmentation (RSNA 2022) with MONAI - Resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a6df9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:11:13.470051Z",
     "iopub.status.busy": "2024-08-21T11:11:13.469761Z",
     "iopub.status.idle": "2024-08-21T11:12:06.868277Z",
     "shell.execute_reply": "2024-08-21T11:12:06.867150Z"
    },
    "papermill": {
     "duration": 53.408734,
     "end_time": "2024-08-21T11:12:06.870814",
     "exception": false,
     "start_time": "2024-08-21T11:11:13.462080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-gdcm\r\n",
      "  Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\r\n",
      "Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: python-gdcm\r\n",
      "Successfully installed python-gdcm-3.0.24.1\r\n",
      "Collecting pylibjpeg\r\n",
      "  Downloading pylibjpeg-2.0.1-py3-none-any.whl.metadata (7.8 kB)\r\n",
      "Collecting pylibjpeg-libjpeg\r\n",
      "  Downloading pylibjpeg_libjpeg-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: pydicom in /opt/conda/lib/python3.10/site-packages (2.4.4)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pylibjpeg) (1.26.4)\r\n",
      "Downloading pylibjpeg-2.0.1-py3-none-any.whl (24 kB)\r\n",
      "Downloading pylibjpeg_libjpeg-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pylibjpeg-libjpeg, pylibjpeg\r\n",
      "Successfully installed pylibjpeg-2.0.1 pylibjpeg-libjpeg-2.2.0\r\n",
      "Collecting pyjpegls\r\n",
      "  Downloading pyjpegls-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\r\n",
      "Requirement already satisfied: numpy>=1.24 in /opt/conda/lib/python3.10/site-packages (from pyjpegls) (1.26.4)\r\n",
      "Downloading pyjpegls-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyjpegls\r\n",
      "Successfully installed pyjpegls-1.4.0\r\n",
      "Collecting monai\r\n",
      "  Downloading monai-1.3.2-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from monai) (2.1.2)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from monai) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (2024.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\r\n",
      "Downloading monai-1.3.2-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: monai\r\n",
      "Successfully installed monai-1.3.2\r\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "! pip install python-gdcm\n",
    "! pip install pylibjpeg pylibjpeg-libjpeg pydicom\n",
    "! pip install pyjpegls\n",
    "! pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b634c2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:06.893062Z",
     "iopub.status.busy": "2024-08-21T11:12:06.892730Z",
     "iopub.status.idle": "2024-08-21T11:12:50.814935Z",
     "shell.execute_reply": "2024-08-21T11:12:50.814149Z"
    },
    "papermill": {
     "duration": 43.936026,
     "end_time": "2024-08-21T11:12:50.817298",
     "exception": false,
     "start_time": "2024-08-21T11:12:06.881272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 11:12:41.043102: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 11:12:41.043210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 11:12:41.198840: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "from glob import glob\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "# DICOM image files (.dcm)\n",
    "import pydicom\n",
    "from pydicom import dcmread\n",
    "\n",
    "# NIfTI image files (.nii)\n",
    "import nibabel as nib\n",
    "\n",
    "# Required dependencies\n",
    "import gdcm\n",
    "import pylibjpeg\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "# Monai\n",
    "import monai\n",
    "from monai.data import ArrayDataset, DataLoader, decollate_batch\n",
    "from monai.networks.layers import Norm\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    "    RandFlip,\n",
    "    RandAffine,\n",
    "    RandGridDistortion\n",
    ")\n",
    "from monai.utils import set_determinism, first\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c21656",
   "metadata": {
    "papermill": {
     "duration": 0.010038,
     "end_time": "2024-08-21T11:12:50.837904",
     "exception": false,
     "start_time": "2024-08-21T11:12:50.827866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resuming Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00adab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:50.861289Z",
     "iopub.status.busy": "2024-08-21T11:12:50.860234Z",
     "iopub.status.idle": "2024-08-21T11:12:50.879920Z",
     "shell.execute_reply": "2024-08-21T11:12:50.878875Z"
    },
    "papermill": {
     "duration": 0.033847,
     "end_time": "2024-08-21T11:12:50.881865",
     "exception": false,
     "start_time": "2024-08-21T11:12:50.848018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: HM8659LQ\n",
      "spatial_size: (128, 128, 128)\n",
      "prob: 0.5\n",
      "k: 5\n",
      "batch_size: 4\n",
      "epochs: 240\n",
      "lr: 0.0001\n",
      "loss_weights: (0.0, 1.0)\n",
      "channels: (16, 32, 64, 128, 256)\n",
      "strides: (2, 2, 2, 2)\n",
      "kernel_size: 3\n",
      "up_kernel_size: 3\n",
      "num_res_units: 2\n",
      "act: PRELU\n",
      "dropout: 0.0\n",
      "bias: True\n",
      "val_fold_idx: 1\n"
     ]
    }
   ],
   "source": [
    "# Path \n",
    "base_path = \"../input/rsna-2022-cervical-spine-fracture-detection\"\n",
    "TRAIN_IMAGES_PATH = f'{base_path}/train_images'\n",
    "SEGMENTATIONS_PATH = f'{base_path}/segmentations'\n",
    "OUTPUT_DIR = '.'\n",
    "\n",
    "\n",
    "last_model_output = 'HM8659LQ_last_model' # INSERT MODEL ID\n",
    "last_config_pkl = 'HM8659LQ_config.pkl' # INSERT MODEL ID\n",
    "MODEL_PATH = f'/kaggle/input/hm8659lq-160-240-epochs/{last_model_output}' # INSERT DATASET\n",
    "CONFIG_PATH = f'/kaggle/input/hm8659lq-160-240-epochs/{last_config_pkl}' # INSERT DATASET\n",
    "\n",
    "# Load config from the pkl file\n",
    "with open(CONFIG_PATH, 'rb') as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "for k, v in config.items(): print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71fa1ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:50.903914Z",
     "iopub.status.busy": "2024-08-21T11:12:50.903468Z",
     "iopub.status.idle": "2024-08-21T11:12:50.907600Z",
     "shell.execute_reply": "2024-08-21T11:12:50.906813Z"
    },
    "papermill": {
     "duration": 0.017081,
     "end_time": "2024-08-21T11:12:50.909404",
     "exception": false,
     "start_time": "2024-08-21T11:12:50.892323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get output and config path file\n",
    "OUTPUT_FILE = OUTPUT_DIR + f'/{config[\"ID\"]}_train_val_losses.csv'                   \n",
    "CONFIG_FILE = OUTPUT_DIR + f'/{config[\"ID\"]}_config.pkl'\n",
    "\n",
    "# New max number of epochs\n",
    "config['epochs'] = 320 # INSERT MAX EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5786875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:50.931538Z",
     "iopub.status.busy": "2024-08-21T11:12:50.930889Z",
     "iopub.status.idle": "2024-08-21T11:12:50.935097Z",
     "shell.execute_reply": "2024-08-21T11:12:50.934304Z"
    },
    "papermill": {
     "duration": 0.017124,
     "end_time": "2024-08-21T11:12:50.936862",
     "exception": false,
     "start_time": "2024-08-21T11:12:50.919738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save config to pickle file\n",
    "with open(CONFIG_FILE, 'wb') as f:\n",
    "    pickle.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3114c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:50.958536Z",
     "iopub.status.busy": "2024-08-21T11:12:50.958269Z",
     "iopub.status.idle": "2024-08-21T11:12:50.964469Z",
     "shell.execute_reply": "2024-08-21T11:12:50.963810Z"
    },
    "papermill": {
     "duration": 0.019194,
     "end_time": "2024-08-21T11:12:50.966436",
     "exception": false,
     "start_time": "2024-08-21T11:12:50.947242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set deterministic training for reproducibility\n",
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acaf1cf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:50.988693Z",
     "iopub.status.busy": "2024-08-21T11:12:50.988432Z",
     "iopub.status.idle": "2024-08-21T11:12:51.092871Z",
     "shell.execute_reply": "2024-08-21T11:12:51.091794Z"
    },
    "papermill": {
     "duration": 0.1177,
     "end_time": "2024-08-21T11:12:51.094827",
     "exception": false,
     "start_time": "2024-08-21T11:12:50.977127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla T4 is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Enabling GPU\n",
    "# https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "    \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# Enable cuDNN benchmark. Set to True whenever the input model does not change over training, False if, eg, some layers are deactivated\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5675f1a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:51.117727Z",
     "iopub.status.busy": "2024-08-21T11:12:51.117426Z",
     "iopub.status.idle": "2024-08-21T11:12:51.121432Z",
     "shell.execute_reply": "2024-08-21T11:12:51.120685Z"
    },
    "papermill": {
     "duration": 0.01765,
     "end_time": "2024-08-21T11:12:51.123368",
     "exception": false,
     "start_time": "2024-08-21T11:12:51.105718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Masks to be reverted \n",
    "revert_mask = [\n",
    "    '1.2.826.0.1.3680043.1363',\n",
    "    '1.2.826.0.1.3680043.20120',\n",
    "    '1.2.826.0.1.3680043.2243',\n",
    "    '1.2.826.0.1.3680043.24606',\n",
    "    '1.2.826.0.1.3680043.32071'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484373f",
   "metadata": {
    "papermill": {
     "duration": 0.010485,
     "end_time": "2024-08-21T11:12:51.144783",
     "exception": false,
     "start_time": "2024-08-21T11:12:51.134298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b79fabdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:51.167413Z",
     "iopub.status.busy": "2024-08-21T11:12:51.167138Z",
     "iopub.status.idle": "2024-08-21T11:12:51.184461Z",
     "shell.execute_reply": "2024-08-21T11:12:51.183487Z"
    },
    "papermill": {
     "duration": 0.030793,
     "end_time": "2024-08-21T11:12:51.186312",
     "exception": false,
     "start_time": "2024-08-21T11:12:51.155519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_dicom_scan(folder_path):\n",
    "    \"\"\" Read CT scan (dicom files) and stack the slices\"\"\"\n",
    "    slices = []\n",
    "    for filename in sorted(os.listdir(folder_path), key=lambda x: int(x.split(\".\")[0])):\n",
    "        if filename.endswith('.dcm'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            ds = pydicom.dcmread(filepath)\n",
    "            slices.append(ds.pixel_array)\n",
    "    scan = np.stack(slices, -1).astype('float64')\n",
    "    return scan\n",
    "\n",
    "\n",
    "def read_nifti_file(file_path, revert_mask=revert_mask):\n",
    "    \"\"\" Read nifit file segmentation\"\"\"    \n",
    "    data = nib.load(file_path).get_fdata()\n",
    "    shape = data.shape\n",
    "    # Reorient because segmentations are done over the sagittal plane\n",
    "    data = data.transpose(1, 0, 2)[::-1, :, ::-1]\n",
    "    # Revert the files that have inverted sequence (from bottom to top)\n",
    "    if file_path in revert_mask:\n",
    "        data[:, :, ::-1]\n",
    "    return data\n",
    "\n",
    "\n",
    "def zoom_volume(vol, spatial_size):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    \"\"\" NON UTILIZZATA SOSTITUITA DA RESIZE\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_width, desired_height, desired_depth = spatial_size\n",
    "    # Get current depth\n",
    "    current_depth = vol.shape[-1]\n",
    "    current_width = vol.shape[0]\n",
    "    current_height = vol.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Resize across z-axis\n",
    "    #vol = ndi.zoom(vol, (width_factor, height_factor, depth_factor), order=0, mode='constant')\n",
    "    zoom_transform = Zoom(zoom=(width_factor, height_factor, depth_factor), keep_size=False)\n",
    "    zoom_vol = zoom_transform(vol)\n",
    "    return zoom_vol\n",
    "\n",
    "def one_hot_encoding_multiclass_mask(mask):\n",
    "    \"\"\" Binary OneHot Encoding of Multi-class masks\"\"\"\n",
    "    labels = list(range(8))\n",
    "    num_labels = len(labels)\n",
    "    c, h, w, d = mask.shape\n",
    "    enc_mask = np.zeros((num_labels, h, w, d))\n",
    "    for c in range(1, num_labels):  # this loop starts from label 1 to ignore background 0\n",
    "        enc_mask[c, :, :, :] = (mask == labels[c]) # 1 for the pixel belonging to that class, 0 for the rest of the pixel\n",
    "        \n",
    "    return enc_mask\n",
    "\n",
    "def expand_dims(arr):\n",
    "    return np.expand_dims(arr, axis=0)\n",
    "\n",
    "def training_plot(file, output_path, config):\n",
    "    train_bce_dl_loss = file['Train_bce_dl_loss']\n",
    "    val_bce_dl_loss = file['Val_bce_dl_loss']\n",
    "    epochs = range(1, len(train_bce_dl_loss) + 1)\n",
    "    plt.plot(epochs, train_bce_dl_loss, label='Training BCE-DiceLoss', color='darkblue')\n",
    "    plt.plot(epochs, val_bce_dl_loss, label='Val BCE-DiceLoss', color='darkorange')\n",
    "    plt.title('Training & Val Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.savefig(os.path.join(output_path, f'{config[\"ID\"]}_Training_Losses_plot.png'))\n",
    "    plt.show()\n",
    "    \n",
    "def validation_metric_plot(file, output_path, config):\n",
    "    val_metric = file['Val_metric']\n",
    "    epochs = range(1, len(val_metric)+1)\n",
    "    plt.plot(epochs, val_metric, label='Validation DiceMetric', color='red')\n",
    "    plt.title('Validation Metric')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.savefig(os.path.join(output_path, f'{config[\"ID\"]}_Validation_Metric_plot.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1af3f3",
   "metadata": {
    "papermill": {
     "duration": 0.011171,
     "end_time": "2024-08-21T11:12:51.208747",
     "exception": false,
     "start_time": "2024-08-21T11:12:51.197576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f36408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:51.231437Z",
     "iopub.status.busy": "2024-08-21T11:12:51.231169Z",
     "iopub.status.idle": "2024-08-21T11:12:51.296537Z",
     "shell.execute_reply": "2024-08-21T11:12:51.295686Z"
    },
    "papermill": {
     "duration": 0.078854,
     "end_time": "2024-08-21T11:12:51.298353",
     "exception": false,
     "start_time": "2024-08-21T11:12:51.219499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label_path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.780</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.21321</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.6125</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.826.0.1.3680043.30067</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.826.0.1.3680043.12833</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>../input/rsna-2022-cervical-spine-fracture-det...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ID  \\\n",
       "0    1.2.826.0.1.3680043.780   \n",
       "1  1.2.826.0.1.3680043.21321   \n",
       "2   1.2.826.0.1.3680043.6125   \n",
       "3  1.2.826.0.1.3680043.30067   \n",
       "4  1.2.826.0.1.3680043.12833   \n",
       "\n",
       "                                          label_path  \\\n",
       "0  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
       "1  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
       "2  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
       "3  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
       "4  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
       "\n",
       "                                          image_path  fold  \n",
       "0  ../input/rsna-2022-cervical-spine-fracture-det...     0  \n",
       "1  ../input/rsna-2022-cervical-spine-fracture-det...     4  \n",
       "2  ../input/rsna-2022-cervical-spine-fracture-det...     4  \n",
       "3  ../input/rsna-2022-cervical-spine-fracture-det...     2  \n",
       "4  ../input/rsna-2022-cervical-spine-fracture-det...     0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataset\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Store all the nifti files in the segmentation folder\n",
    "df['ID'] = os.listdir(SEGMENTATIONS_PATH)\n",
    "\n",
    "# Remove the extension '.nii'\n",
    "df['ID'] = df['ID'].apply(lambda x: x[:-4])\n",
    "\n",
    "# Add complete path to reach segmentation file (nifti)\n",
    "df['label_path'] = df['ID'].apply(lambda x: os.path.join(SEGMENTATIONS_PATH, x + '.nii'))\n",
    "\n",
    "# Add complete path to reach CT scan folder in train_images\n",
    "df['image_path'] = df['ID'].apply(lambda x: os.path.join(TRAIN_IMAGES_PATH, x))\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=config['k'], shuffle=True, random_state=42)\n",
    "\n",
    "# Create a new column for fold indices\n",
    "df['fold'] = -1\n",
    "\n",
    "# Assign fold indices\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(df)):\n",
    "    df.loc[val_index, 'fold'] = fold\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(df.shape)\n",
    "\n",
    "# Show the head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bef26c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:51.322671Z",
     "iopub.status.busy": "2024-08-21T11:12:51.322009Z",
     "iopub.status.idle": "2024-08-21T11:12:51.336191Z",
     "shell.execute_reply": "2024-08-21T11:12:51.335223Z"
    },
    "papermill": {
     "duration": 0.027851,
     "end_time": "2024-08-21T11:12:51.338056",
     "exception": false,
     "start_time": "2024-08-21T11:12:51.310205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69, 5)\n",
      "(18, 5)\n"
     ]
    }
   ],
   "source": [
    "# Define training and validation dataset\n",
    "\n",
    "val_fold_idx = config['val_fold_idx']\n",
    "\n",
    "df_train = df[df.fold != val_fold_idx].reset_index()\n",
    "print(df_train.shape)\n",
    "\n",
    "df_val = df[df.fold == val_fold_idx].reset_index()\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0a9c2d",
   "metadata": {
    "papermill": {
     "duration": 0.011387,
     "end_time": "2024-08-21T11:12:51.360635",
     "exception": false,
     "start_time": "2024-08-21T11:12:51.349248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c35482f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:51.384576Z",
     "iopub.status.busy": "2024-08-21T11:12:51.383888Z",
     "iopub.status.idle": "2024-08-21T11:12:51.417970Z",
     "shell.execute_reply": "2024-08-21T11:12:51.416850Z"
    },
    "papermill": {
     "duration": 0.048941,
     "end_time": "2024-08-21T11:12:51.420700",
     "exception": false,
     "start_time": "2024-08-21T11:12:51.371759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define training transforms for image and label\n",
    "train_image_trans = Compose(\n",
    "    [   \n",
    "        # Load Data\n",
    "        read_dicom_scan,\n",
    "        # Data Preparation\n",
    "        expand_dims, \n",
    "        Resize(spatial_size=config['spatial_size'], mode=\"area\"), # Resize the volume to target spatial_size\n",
    "        ScaleIntensity(), # scale between (0,1)\n",
    "        # Data Augmentation \n",
    "        RandFlip(prob=config['prob'], spatial_axis=0), # width\n",
    "        RandFlip(prob=config['prob'], spatial_axis=1), # height\n",
    "        RandGridDistortion(num_cells=5, distort_limit=(-0.03, 0.03), prob=config['prob']),\n",
    "        RandAffine(prob=config['prob'], \n",
    "                   translate_range=[int(x*y) for x, y in zip(config['spatial_size'], [0.3, 0.3, 0.3])], padding_mode='zeros')\n",
    "    ]\n",
    ")\n",
    "train_label_trans = Compose(\n",
    "    [   \n",
    "        # Load data\n",
    "        read_nifti_file,\n",
    "        # Data Preparation\n",
    "        expand_dims,\n",
    "        Resize(spatial_size=config['spatial_size'], mode=\"area\"),\n",
    "        one_hot_encoding_multiclass_mask, \n",
    "        # Data Augmentation\n",
    "        RandFlip(prob=config['prob'], spatial_axis=0), # width\n",
    "        RandFlip(prob=config['prob'], spatial_axis=1), # height\n",
    "        RandGridDistortion(num_cells=5, distort_limit=(-0.03, 0.03), prob=config['prob']),\n",
    "        RandAffine(prob=config['prob'], \n",
    "                   translate_range=[int(x*y) for x, y in zip(config['spatial_size'], [0.3, 0.3, 0.3])], padding_mode='zeros')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define validation transforms for image and label (no augmentation)\n",
    "val_image_trans = Compose(\n",
    "    [\n",
    "        # Load data\n",
    "        read_dicom_scan,\n",
    "        # Data Preparation\n",
    "        expand_dims,\n",
    "        Resize(spatial_size=config['spatial_size'], mode=\"area\"), \n",
    "        ScaleIntensity()\n",
    "    ]\n",
    ")\n",
    "val_label_trans = Compose(\n",
    "    [\n",
    "        # Load data\n",
    "        read_nifti_file,\n",
    "        # Data preparation\n",
    "        expand_dims,\n",
    "        Resize(spatial_size=config['spatial_size'], mode=\"area\"),\n",
    "        one_hot_encoding_multiclass_mask\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fe87d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:51.457128Z",
     "iopub.status.busy": "2024-08-21T11:12:51.456831Z",
     "iopub.status.idle": "2024-08-21T11:12:51.463797Z",
     "shell.execute_reply": "2024-08-21T11:12:51.462967Z"
    },
    "papermill": {
     "duration": 0.030842,
     "end_time": "2024-08-21T11:12:51.466173",
     "exception": false,
     "start_time": "2024-08-21T11:12:51.435331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define train dataset and dataloader\n",
    "train_ds = ArrayDataset(df_train.image_path, train_image_trans, df_train.label_path, train_label_trans)\n",
    "train_loader = DataLoader(train_ds, batch_size=config['batch_size'], num_workers=2)\n",
    "\n",
    "# Define validation dataset and dataloader\n",
    "val_ds = ArrayDataset(df_val.image_path, val_image_trans, df_val.label_path, val_label_trans)\n",
    "val_loader = DataLoader(val_ds, batch_size=config['batch_size'], num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d59396e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:12:51.497602Z",
     "iopub.status.busy": "2024-08-21T11:12:51.497331Z",
     "iopub.status.idle": "2024-08-21T11:13:38.015174Z",
     "shell.execute_reply": "2024-08-21T11:13:38.013859Z"
    },
    "papermill": {
     "duration": 46.553427,
     "end_time": "2024-08-21T11:13:38.035399",
     "exception": false,
     "start_time": "2024-08-21T11:12:51.481972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 128, 128, 128]) torch.Size([4, 8, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Take the first processed train batch and print the shape\n",
    "first_train_image, first_train_label = first(train_loader)\n",
    "print(first_train_image.shape, first_train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c46cc4",
   "metadata": {
    "papermill": {
     "duration": 0.011317,
     "end_time": "2024-08-21T11:13:38.058430",
     "exception": false,
     "start_time": "2024-08-21T11:13:38.047113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resuming Model, Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "632c131f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:13:38.083729Z",
     "iopub.status.busy": "2024-08-21T11:13:38.082881Z",
     "iopub.status.idle": "2024-08-21T11:13:38.330815Z",
     "shell.execute_reply": "2024-08-21T11:13:38.329874Z"
    },
    "papermill": {
     "duration": 0.263225,
     "end_time": "2024-08-21T11:13:38.333288",
     "exception": false,
     "start_time": "2024-08-21T11:13:38.070063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the 3D Unet model\n",
    "unet_model = UNet(\n",
    "    spatial_dims = 3, # (Height, Width, Depth)\n",
    "    in_channels = 1,\n",
    "    out_channels = 8, # 8 Binary mask 7 as the vertebrae(C1->C7) + background\n",
    "    channels = config['channels'], # Channels per layer\n",
    "    strides = config['strides'], # Stride per layers\n",
    "    kernel_size = config['kernel_size'], # Size of the kernel for each layer\n",
    "    up_kernel_size = config['up_kernel_size'], # Upsampling convolution kernel size\n",
    "    num_res_units = config['num_res_units'], # Number of residual units\n",
    "    act = config['act'], # Activation function\n",
    "    dropout = config['dropout'], # Dropout rate\n",
    "    bias = config['bias'] # Presence of bias term in convolution blocks\n",
    ")\n",
    "unet_model = unet_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be70894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:13:38.359782Z",
     "iopub.status.busy": "2024-08-21T11:13:38.359501Z",
     "iopub.status.idle": "2024-08-21T11:13:38.367187Z",
     "shell.execute_reply": "2024-08-21T11:13:38.366348Z"
    },
    "papermill": {
     "duration": 0.022603,
     "end_time": "2024-08-21T11:13:38.369291",
     "exception": false,
     "start_time": "2024-08-21T11:13:38.346688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Loss function\n",
    "\n",
    "# DiceLoss function\n",
    "#loss_function = DiceLoss(\n",
    "#    include_background = True,  # If False, channel index 0 (background) is excluded from the calculation\n",
    "#    squared_pred = False, # Use squared versions of targets and predictions in the denominator or not\n",
    "#    reduction = 'mean' # Reduction to apply to the output\n",
    "#)\n",
    "\n",
    "# BCE-DiceLoss function\n",
    "# Define DiceLoss (MONAI)\n",
    "dice_loss_fn = DiceLoss(\n",
    "    include_background=False,  # Include background class in the Dice computation\n",
    "    to_onehot_y=False,  # Assuming the labels are not one-hot encoded\n",
    "    sigmoid=False,  # Apply sigmoid to the input tensor\n",
    "    softmax=True,  # Do not apply softmax to the input tensor\n",
    "    squared_pred=True,  # Do not use squared predictions\n",
    "    reduction='mean', # Reduction to apply to the output\n",
    "    smooth_nr=1.0, # constant added to the numerator to avoid zero\n",
    "    smooth_dr=1.0 # constant added to the denominator to avoid nan\n",
    ")\n",
    "# Define BCEWithLogitsLoss (PyTorch)\n",
    "bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "# Combine BCE and Dice losses\n",
    "def bce_diceloss(input, target, loss_weights=config['loss_weights']):    # Compute the BCE loss\n",
    "    bce_loss = loss_weights[0] * bce_loss_fn(input, target)\n",
    "    # Compute the Dice loss\n",
    "    dice_loss = loss_weights[1] * dice_loss_fn(input, target)\n",
    "    # Combine the losses\n",
    "    total_loss = (bce_loss + dice_loss) / sum(loss_weights)\n",
    "    return total_loss\n",
    "# Set the combined loss function\n",
    "criterion = bce_diceloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5cad2ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:13:38.398909Z",
     "iopub.status.busy": "2024-08-21T11:13:38.398656Z",
     "iopub.status.idle": "2024-08-21T11:13:38.402504Z",
     "shell.execute_reply": "2024-08-21T11:13:38.401704Z"
    },
    "papermill": {
     "duration": 0.020848,
     "end_time": "2024-08-21T11:13:38.404288",
     "exception": false,
     "start_time": "2024-08-21T11:13:38.383440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define metric\n",
    "\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a640e653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:13:38.428882Z",
     "iopub.status.busy": "2024-08-21T11:13:38.428640Z",
     "iopub.status.idle": "2024-08-21T11:13:38.434406Z",
     "shell.execute_reply": "2024-08-21T11:13:38.433533Z"
    },
    "papermill": {
     "duration": 0.020327,
     "end_time": "2024-08-21T11:13:38.436299",
     "exception": false,
     "start_time": "2024-08-21T11:13:38.415972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "\n",
    "# Adam optimzier\n",
    "#optimizer = torch.optim.Adam(\n",
    "#    unet_model.parameters(), # Model's params\n",
    "#    lr = 1e-3 # Learning rate\n",
    "#)\n",
    "\n",
    "# AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    unet_model.parameters(), \n",
    "    lr = config['lr'] # weight_decay = config['weight_decay'] \n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab3a8cc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:13:38.460764Z",
     "iopub.status.busy": "2024-08-21T11:13:38.460522Z",
     "iopub.status.idle": "2024-08-21T11:13:38.465587Z",
     "shell.execute_reply": "2024-08-21T11:13:38.464795Z"
    },
    "papermill": {
     "duration": 0.019259,
     "end_time": "2024-08-21T11:13:38.467355",
     "exception": false,
     "start_time": "2024-08-21T11:13:38.448096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Define the transforms to apply to model prediction \n",
    "post_trans = Compose([Activations(sigmoid=False, softmax=True, dim=0), AsDiscrete(threshold=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70622209",
   "metadata": {
    "papermill": {
     "duration": 0.011645,
     "end_time": "2024-08-21T11:13:38.490785",
     "exception": false,
     "start_time": "2024-08-21T11:13:38.479140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resuming Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eacc55c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:13:38.515324Z",
     "iopub.status.busy": "2024-08-21T11:13:38.515072Z",
     "iopub.status.idle": "2024-08-21T11:13:39.799273Z",
     "shell.execute_reply": "2024-08-21T11:13:39.798467Z"
    },
    "papermill": {
     "duration": 1.299074,
     "end_time": "2024-08-21T11:13:39.801589",
     "exception": false,
     "start_time": "2024-08-21T11:13:38.502515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resume State from checkpoint\n",
    "checkpoint = torch.load(MODEL_PATH)\n",
    "\n",
    "# Resume model, optimizer, learning rate scheduler, loss and last epoch \n",
    "unet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "#loss = checkpoint['avg_train_bce_dl_loss']\n",
    "start_epoch = checkpoint['epoch']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e0b2299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:13:39.827698Z",
     "iopub.status.busy": "2024-08-21T11:13:39.827395Z",
     "iopub.status.idle": "2024-08-21T11:13:39.852839Z",
     "shell.execute_reply": "2024-08-21T11:13:39.852018Z"
    },
    "papermill": {
     "duration": 0.040742,
     "end_time": "2024-08-21T11:13:39.854751",
     "exception": false,
     "start_time": "2024-08-21T11:13:39.814009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resuming_train_loop(model,\n",
    "                        train_loader,\n",
    "                        val_loader,\n",
    "                        diceloss_function,\n",
    "                        bce_function,\n",
    "                        bce_diceloss_function,\n",
    "                        metric,\n",
    "                        optimizer,\n",
    "                        lr_scheduler,\n",
    "                        start_epoch,\n",
    "                        config,\n",
    "                        output_dir,\n",
    "                        output_file,\n",
    "                        device):\n",
    "    \n",
    "    # Container to store train losses values per epoch\n",
    "    train_dl_values = []\n",
    "    train_bce_values = []\n",
    "    train_bce_dl_values = []\n",
    "    \n",
    "    # Container to store val losses values per epoch\n",
    "    val_dl_values = []\n",
    "    val_bce_values = []\n",
    "    val_bce_dl_values = []\n",
    "\n",
    "    # Container to store val metric values per epoch\n",
    "    val_metric_values = []\n",
    "\n",
    "    # Store best val metric\n",
    "    best_val_metric = -1\n",
    "\n",
    "    total_start = time.time()\n",
    "    # Iterate over the epochs\n",
    "    for epoch in range(start_epoch, config['epochs']+1):\n",
    "        epoch_start = time.time()\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"EPOCH {start_epoch}/{config['epochs']}\")\n",
    "        model.train() # Set the model in training mode\n",
    "        epoch_train_dl_loss, epoch_train_bce_loss, epoch_train_bce_dl_loss = 0, 0, 0\n",
    "        epoch_val_dl_loss, epoch_val_bce_loss, epoch_val_bce_dl_loss = 0, 0, 0\n",
    "        # Iterate over the batches\n",
    "        for step, batch_data in enumerate(train_loader):\n",
    "            step_start = time.time()\n",
    "            inputs, labels = batch_data\n",
    "            inputs, labels = (inputs.to(device), labels.to(device))\n",
    "            optimizer.zero_grad() # Clear the old gradients before computing new ones\n",
    "            # Enable automatic mixed precision (amp)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs) # Make predictions for current batch\n",
    "                train_dl_loss = diceloss_function(outputs, labels) # Compute DiceLoss (train)\n",
    "                train_bce_loss = bce_function(outputs, labels) # Compute BCE loss (train)\n",
    "                train_bce_dl_loss = bce_diceloss_function(outputs, labels) # Compute BCE-DiceLoss (train)\n",
    "            scaler.scale(train_bce_dl_loss).backward() # Compute the gradients\n",
    "            scaler.step(optimizer) # Update model weights\n",
    "            scaler.update()\n",
    "            epoch_train_dl_loss += train_dl_loss.item()\n",
    "            epoch_train_bce_loss += train_bce_loss.item()\n",
    "            epoch_train_bce_dl_loss += train_bce_dl_loss.item()\n",
    "            # REPORT PER BATCH \n",
    "            print(\n",
    "                f\"batch: {step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "                f\", train_dl_loss: {train_dl_loss.item():.4f}\"\n",
    "                f\", train_bce_loss: {train_bce_loss.item():.4f}\"\n",
    "                f\", train_bce_dl_loss: {train_bce_dl_loss.item():.4f}\"\n",
    "                f\", step time: {(time.time() - step_start):.4f}\"\n",
    "            )\n",
    "        lr_scheduler.step()\n",
    "        # Compute mean losses over the batches\n",
    "        avg_train_dl_loss = epoch_train_dl_loss / (step+1)\n",
    "        avg_train_bce_loss = epoch_train_bce_loss / (step+1)\n",
    "        avg_train_bce_dl_loss = epoch_train_bce_dl_loss / (step+1)\n",
    "\n",
    "        # EVALUATE MODEL ON VALIDATION SET\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        with torch.no_grad(): # Disable gradient computation and reduce memory consumption\n",
    "            for step, val_data in enumerate(val_loader):\n",
    "                val_inputs, val_labels = val_data\n",
    "                val_inputs, val_labels = (val_inputs.to(device), val_labels.to(device))\n",
    "                val_outputs = model(val_inputs)\n",
    "                # Compute losses\n",
    "                val_dl_loss = diceloss_function(val_outputs, val_labels) # Compute DiceLoss (val)\n",
    "                val_bce_loss = bce_function(val_outputs, val_labels) # Compute BCE (val)\n",
    "                val_bce_dl_loss = bce_diceloss_function(val_outputs, val_labels) # Compute BCE-DiceLoss (val)\n",
    "                # Apply post transforms\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                # Compute metric\n",
    "                metric(y_pred=val_outputs, y=val_labels)\n",
    "                \n",
    "                epoch_val_dl_loss += val_dl_loss.item()\n",
    "                epoch_val_bce_loss += val_bce_loss.item()\n",
    "                epoch_val_bce_dl_loss += val_bce_dl_loss.item()\n",
    "                \n",
    "            # Aggregate the final mean dice result\n",
    "            avg_val_metric = metric.aggregate().item()\n",
    "            \n",
    "        # Compute mean val losses over the batches\n",
    "        avg_val_dl_loss = epoch_val_dl_loss / (step + 1)\n",
    "        avg_val_bce_loss = epoch_val_bce_loss / (step + 1)\n",
    "        avg_val_bce_dl_loss = epoch_val_bce_dl_loss / (step + 1)\n",
    "\n",
    "        # REPORT PER EPOCH\n",
    "        print(f'LOSS train DiceLoss: {avg_train_dl_loss:.4f}, LOSS train BCE: {avg_train_bce_loss:.4f}, LOSS train BCE-DiceLoss: {avg_train_bce_dl_loss:.4f}, LOSS val DiceLoss: {avg_val_dl_loss:.4f}, LOSS val BCE: {avg_val_bce_loss:.4f}, LOSS val BCE-DiceLoss: {avg_val_bce_dl_loss:.4f}, METRIC val: {avg_val_metric:.4f}')\n",
    "\n",
    "        # Store train/val losses and val metric per epoch\n",
    "        train_dl_values.append(avg_train_dl_loss)\n",
    "        train_bce_values.append(avg_train_bce_loss)\n",
    "        train_bce_dl_values.append(avg_train_bce_dl_loss)\n",
    "        val_dl_values.append(avg_val_dl_loss)\n",
    "        val_bce_values.append(avg_val_bce_loss)\n",
    "        val_bce_dl_values.append(avg_val_bce_dl_loss)\n",
    "        val_metric_values.append(avg_val_metric)\n",
    "        \n",
    "        # Reset the metric status\n",
    "        metric.reset()\n",
    "\n",
    "        # Log the running loss averaged per batch for train and the running metric averaged per batch for val\n",
    "        with open(output_file, \"a\") as file:\n",
    "            file.write(f\"{epoch}, {avg_train_dl_loss}, {avg_train_bce_loss}, {avg_train_bce_dl_loss}, {avg_val_dl_loss}, {avg_val_bce_loss}, {avg_val_bce_dl_loss}, {avg_val_metric}\\n\")\n",
    "        \n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_val_metric > best_val_metric:\n",
    "            best_val_metric = avg_val_metric\n",
    "            best_model = {'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                          'train DiceLoss loss': avg_train_dl_loss,\n",
    "                          'train BCE loss': avg_train_bce_loss,\n",
    "                          'train BCE-DiceLoss loss': avg_train_bce_dl_loss,\n",
    "                          'val DiceLoss loss': avg_val_dl_loss,\n",
    "                          'val BCE loss': avg_val_bce_loss,\n",
    "                          'val BCE-DiceLoss loss': avg_val_bce_dl_loss\n",
    "                         }\n",
    "        print(f\"time consuming of epoch {epoch} is: {(time.time() - epoch_start):.4f}\")\n",
    "        \n",
    "        # Save last model's state\n",
    "        if epoch == config['epochs']:\n",
    "            last_model = {'epoch': epoch,\n",
    "                          'model_state_dict': model.state_dict(),\n",
    "                          'optimizer_state_dict': optimizer.state_dict(),\n",
    "                          'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                          'train DiceLoss loss': avg_train_dl_loss,\n",
    "                          'train BCE loss': avg_train_bce_loss,\n",
    "                          'train BCE-DiceLoss loss': avg_train_bce_dl_loss,\n",
    "                          'val DiceLoss loss': avg_val_dl_loss,\n",
    "                          'val BCE loss': avg_val_bce_loss,\n",
    "                          'val BCE-DiceLoss loss': avg_val_bce_dl_loss\n",
    "                          }\n",
    "            \n",
    "    total_time = time.time() - total_start\n",
    "\n",
    "    # Save Train Losses and Val Metric\n",
    "    with open(OUTPUT_FILE, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Train_dl_loss', 'Train_bce_loss', 'Train_bce_dl_loss', 'Val_dl_loss', 'Val_bce_loss', 'Val_bce_dl_loss', 'Val_metric'])\n",
    "        csvwriter.writerows(zip(train_dl_values, train_bce_values, train_bce_dl_values, val_dl_values, val_bce_values, val_bce_dl_values, val_metric_values))    \n",
    "        \n",
    "    # Save Best Model's State\n",
    "    best_model_path = os.path.join(OUTPUT_DIR, f\"{config['ID']}_best_model\")\n",
    "    torch.save(best_model, best_model_path)\n",
    "\n",
    "    # Save Last Model's State\n",
    "    last_model_path = os.path.join(OUTPUT_DIR, f\"{config['ID']}_last_model\")\n",
    "    torch.save(last_model, last_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7eb3348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T11:13:39.879618Z",
     "iopub.status.busy": "2024-08-21T11:13:39.879363Z",
     "iopub.status.idle": "2024-08-21T21:26:05.970698Z",
     "shell.execute_reply": "2024-08-21T21:26:05.969646Z"
    },
    "papermill": {
     "duration": 36746.162674,
     "end_time": "2024-08-21T21:26:06.029295",
     "exception": false,
     "start_time": "2024-08-21T11:13:39.866621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2699, train_bce_loss: 1.6537, train_bce_dl_loss: 0.2699, step time: 5.2209\n",
      "batch: 1/17, train_dl_loss: 0.2669, train_bce_loss: 1.6722, train_bce_dl_loss: 0.2669, step time: 0.3737\n",
      "batch: 2/17, train_dl_loss: 0.2979, train_bce_loss: 1.6599, train_bce_dl_loss: 0.2979, step time: 0.4153\n",
      "batch: 3/17, train_dl_loss: 0.4166, train_bce_loss: 1.6696, train_bce_dl_loss: 0.4166, step time: 0.3737\n",
      "batch: 4/17, train_dl_loss: 0.2693, train_bce_loss: 1.6595, train_bce_dl_loss: 0.2693, step time: 0.4180\n",
      "batch: 5/17, train_dl_loss: 0.2892, train_bce_loss: 1.6691, train_bce_dl_loss: 0.2892, step time: 0.3711\n",
      "batch: 6/17, train_dl_loss: 0.3432, train_bce_loss: 1.6653, train_bce_dl_loss: 0.3432, step time: 0.4410\n",
      "batch: 7/17, train_dl_loss: 0.2691, train_bce_loss: 1.6521, train_bce_dl_loss: 0.2691, step time: 0.3754\n",
      "batch: 8/17, train_dl_loss: 0.2911, train_bce_loss: 1.6663, train_bce_dl_loss: 0.2911, step time: 0.4169\n",
      "batch: 9/17, train_dl_loss: 0.3051, train_bce_loss: 1.6699, train_bce_dl_loss: 0.3051, step time: 0.3680\n",
      "batch: 10/17, train_dl_loss: 0.4519, train_bce_loss: 1.6624, train_bce_dl_loss: 0.4519, step time: 0.4162\n",
      "batch: 11/17, train_dl_loss: 0.2655, train_bce_loss: 1.6334, train_bce_dl_loss: 0.2655, step time: 0.4303\n",
      "batch: 12/17, train_dl_loss: 0.3034, train_bce_loss: 1.6577, train_bce_dl_loss: 0.3034, step time: 0.4236\n",
      "batch: 13/17, train_dl_loss: 0.3961, train_bce_loss: 1.6665, train_bce_dl_loss: 0.3961, step time: 0.3696\n",
      "batch: 14/17, train_dl_loss: 0.3113, train_bce_loss: 1.6750, train_bce_dl_loss: 0.3113, step time: 0.4099\n",
      "batch: 15/17, train_dl_loss: 0.2627, train_bce_loss: 1.6712, train_bce_dl_loss: 0.2627, step time: 0.3699\n",
      "batch: 16/17, train_dl_loss: 0.2568, train_bce_loss: 1.6607, train_bce_dl_loss: 0.2568, step time: 0.4219\n",
      "batch: 17/17, train_dl_loss: 0.2379, train_bce_loss: 1.6558, train_bce_dl_loss: 0.2379, step time: 1.1869\n",
      "LOSS train DiceLoss: 0.3058, LOSS train BCE: 1.6622, LOSS train BCE-DiceLoss: 0.3058, LOSS val DiceLoss: 0.4323, LOSS val BCE: 1.6565, LOSS val BCE-DiceLoss: 0.4323, METRIC val: 0.5339\n",
      "time consuming of epoch 241 is: 488.9892\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3015, train_bce_loss: 1.6582, train_bce_dl_loss: 0.3015, step time: 0.4452\n",
      "batch: 1/17, train_dl_loss: 0.2955, train_bce_loss: 1.6593, train_bce_dl_loss: 0.2955, step time: 0.3826\n",
      "batch: 2/17, train_dl_loss: 0.2796, train_bce_loss: 1.6591, train_bce_dl_loss: 0.2796, step time: 0.4289\n",
      "batch: 3/17, train_dl_loss: 0.3681, train_bce_loss: 1.6601, train_bce_dl_loss: 0.3681, step time: 0.3766\n",
      "batch: 4/17, train_dl_loss: 0.2694, train_bce_loss: 1.6472, train_bce_dl_loss: 0.2694, step time: 0.4146\n",
      "batch: 5/17, train_dl_loss: 0.2717, train_bce_loss: 1.6472, train_bce_dl_loss: 0.2717, step time: 0.3788\n",
      "batch: 6/17, train_dl_loss: 0.3244, train_bce_loss: 1.6672, train_bce_dl_loss: 0.3244, step time: 0.4147\n",
      "batch: 7/17, train_dl_loss: 0.2804, train_bce_loss: 1.6386, train_bce_dl_loss: 0.2804, step time: 0.3745\n",
      "batch: 8/17, train_dl_loss: 0.2980, train_bce_loss: 1.6435, train_bce_dl_loss: 0.2980, step time: 0.4182\n",
      "batch: 9/17, train_dl_loss: 0.2877, train_bce_loss: 1.6651, train_bce_dl_loss: 0.2877, step time: 0.3741\n",
      "batch: 10/17, train_dl_loss: 0.4159, train_bce_loss: 1.6601, train_bce_dl_loss: 0.4159, step time: 0.4345\n",
      "batch: 11/17, train_dl_loss: 0.2554, train_bce_loss: 1.6477, train_bce_dl_loss: 0.2554, step time: 0.4184\n",
      "batch: 12/17, train_dl_loss: 0.3103, train_bce_loss: 1.6480, train_bce_dl_loss: 0.3103, step time: 0.4310\n",
      "batch: 13/17, train_dl_loss: 0.3902, train_bce_loss: 1.6695, train_bce_dl_loss: 0.3902, step time: 0.4343\n",
      "batch: 14/17, train_dl_loss: 0.2706, train_bce_loss: 1.6785, train_bce_dl_loss: 0.2706, step time: 0.4207\n",
      "batch: 15/17, train_dl_loss: 0.2988, train_bce_loss: 1.6775, train_bce_dl_loss: 0.2988, step time: 0.3722\n",
      "batch: 16/17, train_dl_loss: 0.2819, train_bce_loss: 1.6656, train_bce_dl_loss: 0.2819, step time: 0.4131\n",
      "batch: 17/17, train_dl_loss: 0.2304, train_bce_loss: 1.6543, train_bce_dl_loss: 0.2304, step time: 0.1103\n",
      "LOSS train DiceLoss: 0.3017, LOSS train BCE: 1.6581, LOSS train BCE-DiceLoss: 0.3017, LOSS val DiceLoss: 0.4323, LOSS val BCE: 1.6565, LOSS val BCE-DiceLoss: 0.4323, METRIC val: 0.5338\n",
      "time consuming of epoch 242 is: 437.3453\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3224, train_bce_loss: 1.6610, train_bce_dl_loss: 0.3224, step time: 0.4265\n",
      "batch: 1/17, train_dl_loss: 0.2697, train_bce_loss: 1.6594, train_bce_dl_loss: 0.2697, step time: 0.3780\n",
      "batch: 2/17, train_dl_loss: 0.3065, train_bce_loss: 1.6434, train_bce_dl_loss: 0.3065, step time: 0.4248\n",
      "batch: 3/17, train_dl_loss: 0.3722, train_bce_loss: 1.6610, train_bce_dl_loss: 0.3722, step time: 0.3729\n",
      "batch: 4/17, train_dl_loss: 0.2604, train_bce_loss: 1.6555, train_bce_dl_loss: 0.2604, step time: 0.4351\n",
      "batch: 5/17, train_dl_loss: 0.2498, train_bce_loss: 1.6568, train_bce_dl_loss: 0.2498, step time: 0.3799\n",
      "batch: 6/17, train_dl_loss: 0.3449, train_bce_loss: 1.6696, train_bce_dl_loss: 0.3449, step time: 0.4195\n",
      "batch: 7/17, train_dl_loss: 0.3301, train_bce_loss: 1.6573, train_bce_dl_loss: 0.3301, step time: 0.3729\n",
      "batch: 8/17, train_dl_loss: 0.2878, train_bce_loss: 1.6475, train_bce_dl_loss: 0.2878, step time: 0.4367\n",
      "batch: 9/17, train_dl_loss: 0.2849, train_bce_loss: 1.6587, train_bce_dl_loss: 0.2849, step time: 0.3783\n",
      "batch: 10/17, train_dl_loss: 0.3611, train_bce_loss: 1.6597, train_bce_dl_loss: 0.3611, step time: 0.4407\n",
      "batch: 11/17, train_dl_loss: 0.2724, train_bce_loss: 1.6481, train_bce_dl_loss: 0.2724, step time: 0.3812\n",
      "batch: 12/17, train_dl_loss: 0.2876, train_bce_loss: 1.6561, train_bce_dl_loss: 0.2876, step time: 0.4216\n",
      "batch: 13/17, train_dl_loss: 0.3525, train_bce_loss: 1.6712, train_bce_dl_loss: 0.3525, step time: 0.3807\n",
      "batch: 14/17, train_dl_loss: 0.3189, train_bce_loss: 1.6724, train_bce_dl_loss: 0.3189, step time: 0.4307\n",
      "batch: 15/17, train_dl_loss: 0.3268, train_bce_loss: 1.6614, train_bce_dl_loss: 0.3268, step time: 0.4161\n",
      "batch: 16/17, train_dl_loss: 0.3022, train_bce_loss: 1.6614, train_bce_dl_loss: 0.3022, step time: 0.4096\n",
      "batch: 17/17, train_dl_loss: 0.4954, train_bce_loss: 1.6806, train_bce_dl_loss: 0.4954, step time: 0.1107\n",
      "LOSS train DiceLoss: 0.3192, LOSS train BCE: 1.6601, LOSS train BCE-DiceLoss: 0.3192, LOSS val DiceLoss: 0.4323, LOSS val BCE: 1.6565, LOSS val BCE-DiceLoss: 0.4323, METRIC val: 0.5339\n",
      "time consuming of epoch 243 is: 408.9480\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3166, train_bce_loss: 1.6551, train_bce_dl_loss: 0.3166, step time: 0.4336\n",
      "batch: 1/17, train_dl_loss: 0.3579, train_bce_loss: 1.6801, train_bce_dl_loss: 0.3579, step time: 0.3764\n",
      "batch: 2/17, train_dl_loss: 0.3378, train_bce_loss: 1.6735, train_bce_dl_loss: 0.3378, step time: 0.4715\n",
      "batch: 3/17, train_dl_loss: 0.3456, train_bce_loss: 1.6684, train_bce_dl_loss: 0.3456, step time: 0.3735\n",
      "batch: 4/17, train_dl_loss: 0.2801, train_bce_loss: 1.6633, train_bce_dl_loss: 0.2801, step time: 0.4242\n",
      "batch: 5/17, train_dl_loss: 0.2724, train_bce_loss: 1.6572, train_bce_dl_loss: 0.2724, step time: 0.3806\n",
      "batch: 6/17, train_dl_loss: 0.3697, train_bce_loss: 1.6629, train_bce_dl_loss: 0.3697, step time: 0.4298\n",
      "batch: 7/17, train_dl_loss: 0.2856, train_bce_loss: 1.6550, train_bce_dl_loss: 0.2856, step time: 0.4203\n",
      "batch: 8/17, train_dl_loss: 0.2514, train_bce_loss: 1.6578, train_bce_dl_loss: 0.2514, step time: 0.4236\n",
      "batch: 9/17, train_dl_loss: 0.2750, train_bce_loss: 1.6705, train_bce_dl_loss: 0.2750, step time: 0.4315\n",
      "batch: 10/17, train_dl_loss: 0.4116, train_bce_loss: 1.6596, train_bce_dl_loss: 0.4116, step time: 0.4351\n",
      "batch: 11/17, train_dl_loss: 0.2631, train_bce_loss: 1.6421, train_bce_dl_loss: 0.2631, step time: 0.4284\n",
      "batch: 12/17, train_dl_loss: 0.3634, train_bce_loss: 1.6565, train_bce_dl_loss: 0.3634, step time: 0.4229\n",
      "batch: 13/17, train_dl_loss: 0.3804, train_bce_loss: 1.6681, train_bce_dl_loss: 0.3804, step time: 0.3731\n",
      "batch: 14/17, train_dl_loss: 0.3034, train_bce_loss: 1.6793, train_bce_dl_loss: 0.3034, step time: 0.4144\n",
      "batch: 15/17, train_dl_loss: 0.3064, train_bce_loss: 1.6688, train_bce_dl_loss: 0.3064, step time: 0.3783\n",
      "batch: 16/17, train_dl_loss: 0.2680, train_bce_loss: 1.6636, train_bce_dl_loss: 0.2680, step time: 0.4261\n",
      "batch: 17/17, train_dl_loss: 0.2249, train_bce_loss: 1.6518, train_bce_dl_loss: 0.2249, step time: 0.1111\n",
      "LOSS train DiceLoss: 0.3118, LOSS train BCE: 1.6630, LOSS train BCE-DiceLoss: 0.3118, LOSS val DiceLoss: 0.4323, LOSS val BCE: 1.6565, LOSS val BCE-DiceLoss: 0.4323, METRIC val: 0.5338\n",
      "time consuming of epoch 244 is: 367.3021\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3216, train_bce_loss: 1.6598, train_bce_dl_loss: 0.3216, step time: 0.4348\n",
      "batch: 1/17, train_dl_loss: 0.3052, train_bce_loss: 1.6786, train_bce_dl_loss: 0.3052, step time: 0.3774\n",
      "batch: 2/17, train_dl_loss: 0.2978, train_bce_loss: 1.6547, train_bce_dl_loss: 0.2978, step time: 0.4245\n",
      "batch: 3/17, train_dl_loss: 0.3530, train_bce_loss: 1.6622, train_bce_dl_loss: 0.3530, step time: 0.3833\n",
      "batch: 4/17, train_dl_loss: 0.2598, train_bce_loss: 1.6413, train_bce_dl_loss: 0.2598, step time: 0.4179\n",
      "batch: 5/17, train_dl_loss: 0.2590, train_bce_loss: 1.6571, train_bce_dl_loss: 0.2590, step time: 0.4285\n",
      "batch: 6/17, train_dl_loss: 0.3533, train_bce_loss: 1.6717, train_bce_dl_loss: 0.3533, step time: 0.4341\n",
      "batch: 7/17, train_dl_loss: 0.2768, train_bce_loss: 1.6426, train_bce_dl_loss: 0.2768, step time: 0.4362\n",
      "batch: 8/17, train_dl_loss: 0.2620, train_bce_loss: 1.6475, train_bce_dl_loss: 0.2620, step time: 0.4378\n",
      "batch: 9/17, train_dl_loss: 0.2944, train_bce_loss: 1.6687, train_bce_dl_loss: 0.2944, step time: 0.4137\n",
      "batch: 10/17, train_dl_loss: 0.4469, train_bce_loss: 1.6623, train_bce_dl_loss: 0.4469, step time: 0.4287\n",
      "batch: 11/17, train_dl_loss: 0.2760, train_bce_loss: 1.6454, train_bce_dl_loss: 0.2760, step time: 0.4222\n",
      "batch: 12/17, train_dl_loss: 0.2840, train_bce_loss: 1.6559, train_bce_dl_loss: 0.2840, step time: 0.4114\n",
      "batch: 13/17, train_dl_loss: 0.4105, train_bce_loss: 1.6789, train_bce_dl_loss: 0.4105, step time: 0.4191\n",
      "batch: 14/17, train_dl_loss: 0.3797, train_bce_loss: 1.6706, train_bce_dl_loss: 0.3797, step time: 0.4147\n",
      "batch: 15/17, train_dl_loss: 0.3239, train_bce_loss: 1.6637, train_bce_dl_loss: 0.3239, step time: 0.3731\n",
      "batch: 16/17, train_dl_loss: 0.2708, train_bce_loss: 1.6729, train_bce_dl_loss: 0.2708, step time: 0.4252\n",
      "batch: 17/17, train_dl_loss: 0.3629, train_bce_loss: 1.6724, train_bce_dl_loss: 0.3629, step time: 0.1096\n",
      "LOSS train DiceLoss: 0.3188, LOSS train BCE: 1.6615, LOSS train BCE-DiceLoss: 0.3188, LOSS val DiceLoss: 0.4324, LOSS val BCE: 1.6565, LOSS val BCE-DiceLoss: 0.4324, METRIC val: 0.5338\n",
      "time consuming of epoch 245 is: 425.2288\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2927, train_bce_loss: 1.6694, train_bce_dl_loss: 0.2927, step time: 0.4259\n",
      "batch: 1/17, train_dl_loss: 0.2749, train_bce_loss: 1.6564, train_bce_dl_loss: 0.2749, step time: 0.3821\n",
      "batch: 2/17, train_dl_loss: 0.3118, train_bce_loss: 1.6627, train_bce_dl_loss: 0.3118, step time: 0.4509\n",
      "batch: 3/17, train_dl_loss: 0.3683, train_bce_loss: 1.6649, train_bce_dl_loss: 0.3683, step time: 0.3769\n",
      "batch: 4/17, train_dl_loss: 0.3183, train_bce_loss: 1.6550, train_bce_dl_loss: 0.3183, step time: 0.4158\n",
      "batch: 5/17, train_dl_loss: 0.2938, train_bce_loss: 1.6528, train_bce_dl_loss: 0.2938, step time: 0.3751\n",
      "batch: 6/17, train_dl_loss: 0.3479, train_bce_loss: 1.6647, train_bce_dl_loss: 0.3479, step time: 0.4206\n",
      "batch: 7/17, train_dl_loss: 0.2644, train_bce_loss: 1.6378, train_bce_dl_loss: 0.2644, step time: 0.3802\n",
      "batch: 8/17, train_dl_loss: 0.3346, train_bce_loss: 1.6669, train_bce_dl_loss: 0.3346, step time: 0.4291\n",
      "batch: 9/17, train_dl_loss: 0.3253, train_bce_loss: 1.6645, train_bce_dl_loss: 0.3253, step time: 0.4284\n",
      "batch: 10/17, train_dl_loss: 0.4651, train_bce_loss: 1.6705, train_bce_dl_loss: 0.4651, step time: 0.4170\n",
      "batch: 11/17, train_dl_loss: 0.2789, train_bce_loss: 1.6411, train_bce_dl_loss: 0.2789, step time: 0.4225\n",
      "batch: 12/17, train_dl_loss: 0.3838, train_bce_loss: 1.6633, train_bce_dl_loss: 0.3838, step time: 0.4360\n",
      "batch: 13/17, train_dl_loss: 0.3886, train_bce_loss: 1.6669, train_bce_dl_loss: 0.3886, step time: 0.3773\n",
      "batch: 14/17, train_dl_loss: 0.2933, train_bce_loss: 1.6641, train_bce_dl_loss: 0.2933, step time: 0.4310\n",
      "batch: 15/17, train_dl_loss: 0.2956, train_bce_loss: 1.6585, train_bce_dl_loss: 0.2956, step time: 0.3781\n",
      "batch: 16/17, train_dl_loss: 0.2829, train_bce_loss: 1.6715, train_bce_dl_loss: 0.2829, step time: 0.4152\n",
      "batch: 17/17, train_dl_loss: 0.2232, train_bce_loss: 1.6697, train_bce_dl_loss: 0.2232, step time: 0.1116\n",
      "LOSS train DiceLoss: 0.3191, LOSS train BCE: 1.6611, LOSS train BCE-DiceLoss: 0.3191, LOSS val DiceLoss: 0.4325, LOSS val BCE: 1.6565, LOSS val BCE-DiceLoss: 0.4325, METRIC val: 0.5337\n",
      "time consuming of epoch 246 is: 405.5242\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3475, train_bce_loss: 1.6776, train_bce_dl_loss: 0.3475, step time: 0.4202\n",
      "batch: 1/17, train_dl_loss: 0.2832, train_bce_loss: 1.6643, train_bce_dl_loss: 0.2832, step time: 0.3778\n",
      "batch: 2/17, train_dl_loss: 0.3027, train_bce_loss: 1.6703, train_bce_dl_loss: 0.3027, step time: 0.4165\n",
      "batch: 3/17, train_dl_loss: 0.4061, train_bce_loss: 1.6731, train_bce_dl_loss: 0.4061, step time: 0.3764\n",
      "batch: 4/17, train_dl_loss: 0.2501, train_bce_loss: 1.6407, train_bce_dl_loss: 0.2501, step time: 0.4312\n",
      "batch: 5/17, train_dl_loss: 0.2712, train_bce_loss: 1.6658, train_bce_dl_loss: 0.2712, step time: 0.3810\n",
      "batch: 6/17, train_dl_loss: 0.3350, train_bce_loss: 1.6653, train_bce_dl_loss: 0.3350, step time: 0.4368\n",
      "batch: 7/17, train_dl_loss: 0.2742, train_bce_loss: 1.6409, train_bce_dl_loss: 0.2742, step time: 0.3769\n",
      "batch: 8/17, train_dl_loss: 0.2930, train_bce_loss: 1.6473, train_bce_dl_loss: 0.2930, step time: 0.4232\n",
      "batch: 9/17, train_dl_loss: 0.3046, train_bce_loss: 1.6697, train_bce_dl_loss: 0.3046, step time: 0.3795\n",
      "batch: 10/17, train_dl_loss: 0.3717, train_bce_loss: 1.6651, train_bce_dl_loss: 0.3717, step time: 0.4246\n",
      "batch: 11/17, train_dl_loss: 0.2681, train_bce_loss: 1.6304, train_bce_dl_loss: 0.2681, step time: 0.4350\n",
      "batch: 12/17, train_dl_loss: 0.2831, train_bce_loss: 1.6495, train_bce_dl_loss: 0.2831, step time: 0.4214\n",
      "batch: 13/17, train_dl_loss: 0.3858, train_bce_loss: 1.6748, train_bce_dl_loss: 0.3858, step time: 0.3937\n",
      "batch: 14/17, train_dl_loss: 0.3032, train_bce_loss: 1.6749, train_bce_dl_loss: 0.3032, step time: 0.4212\n",
      "batch: 15/17, train_dl_loss: 0.2732, train_bce_loss: 1.6744, train_bce_dl_loss: 0.2732, step time: 0.3773\n",
      "batch: 16/17, train_dl_loss: 0.2676, train_bce_loss: 1.6632, train_bce_dl_loss: 0.2676, step time: 0.4138\n",
      "batch: 17/17, train_dl_loss: 0.2884, train_bce_loss: 1.6539, train_bce_dl_loss: 0.2884, step time: 0.1101\n",
      "LOSS train DiceLoss: 0.3060, LOSS train BCE: 1.6612, LOSS train BCE-DiceLoss: 0.3060, LOSS val DiceLoss: 0.4327, LOSS val BCE: 1.6565, LOSS val BCE-DiceLoss: 0.4327, METRIC val: 0.5336\n",
      "time consuming of epoch 247 is: 434.1700\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2784, train_bce_loss: 1.6655, train_bce_dl_loss: 0.2784, step time: 0.4441\n",
      "batch: 1/17, train_dl_loss: 0.2945, train_bce_loss: 1.6622, train_bce_dl_loss: 0.2945, step time: 0.3779\n",
      "batch: 2/17, train_dl_loss: 0.3218, train_bce_loss: 1.6668, train_bce_dl_loss: 0.3218, step time: 0.4281\n",
      "batch: 3/17, train_dl_loss: 0.3439, train_bce_loss: 1.6560, train_bce_dl_loss: 0.3439, step time: 0.3786\n",
      "batch: 4/17, train_dl_loss: 0.2731, train_bce_loss: 1.6507, train_bce_dl_loss: 0.2731, step time: 0.4176\n",
      "batch: 5/17, train_dl_loss: 0.3020, train_bce_loss: 1.6553, train_bce_dl_loss: 0.3020, step time: 0.3851\n",
      "batch: 6/17, train_dl_loss: 0.3234, train_bce_loss: 1.6623, train_bce_dl_loss: 0.3234, step time: 0.4158\n",
      "batch: 7/17, train_dl_loss: 0.2660, train_bce_loss: 1.6564, train_bce_dl_loss: 0.2660, step time: 0.3758\n",
      "batch: 8/17, train_dl_loss: 0.3024, train_bce_loss: 1.6419, train_bce_dl_loss: 0.3024, step time: 0.4245\n",
      "batch: 9/17, train_dl_loss: 0.2980, train_bce_loss: 1.6633, train_bce_dl_loss: 0.2980, step time: 0.3808\n",
      "batch: 10/17, train_dl_loss: 0.3954, train_bce_loss: 1.6615, train_bce_dl_loss: 0.3954, step time: 0.4226\n",
      "batch: 11/17, train_dl_loss: 0.2751, train_bce_loss: 1.6575, train_bce_dl_loss: 0.2751, step time: 0.4361\n",
      "batch: 12/17, train_dl_loss: 0.2981, train_bce_loss: 1.6498, train_bce_dl_loss: 0.2981, step time: 0.4200\n",
      "batch: 13/17, train_dl_loss: 0.4174, train_bce_loss: 1.6746, train_bce_dl_loss: 0.4174, step time: 0.4008\n",
      "batch: 14/17, train_dl_loss: 0.3157, train_bce_loss: 1.6669, train_bce_dl_loss: 0.3157, step time: 0.4328\n",
      "batch: 15/17, train_dl_loss: 0.3126, train_bce_loss: 1.6566, train_bce_dl_loss: 0.3126, step time: 0.3786\n",
      "batch: 16/17, train_dl_loss: 0.2977, train_bce_loss: 1.6679, train_bce_dl_loss: 0.2977, step time: 0.4255\n",
      "batch: 17/17, train_dl_loss: 0.2245, train_bce_loss: 1.6731, train_bce_dl_loss: 0.2245, step time: 0.1106\n",
      "LOSS train DiceLoss: 0.3078, LOSS train BCE: 1.6605, LOSS train BCE-DiceLoss: 0.3078, LOSS val DiceLoss: 0.4324, LOSS val BCE: 1.6563, LOSS val BCE-DiceLoss: 0.4324, METRIC val: 0.5337\n",
      "time consuming of epoch 248 is: 401.8794\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3105, train_bce_loss: 1.6611, train_bce_dl_loss: 0.3105, step time: 0.4214\n",
      "batch: 1/17, train_dl_loss: 0.2961, train_bce_loss: 1.6599, train_bce_dl_loss: 0.2961, step time: 0.3798\n",
      "batch: 2/17, train_dl_loss: 0.2976, train_bce_loss: 1.6620, train_bce_dl_loss: 0.2976, step time: 0.4287\n",
      "batch: 3/17, train_dl_loss: 0.3736, train_bce_loss: 1.6595, train_bce_dl_loss: 0.3736, step time: 0.3768\n",
      "batch: 4/17, train_dl_loss: 0.2858, train_bce_loss: 1.6590, train_bce_dl_loss: 0.2858, step time: 0.4202\n",
      "batch: 5/17, train_dl_loss: 0.2615, train_bce_loss: 1.6490, train_bce_dl_loss: 0.2615, step time: 0.3947\n",
      "batch: 6/17, train_dl_loss: 0.3459, train_bce_loss: 1.6681, train_bce_dl_loss: 0.3459, step time: 0.4331\n",
      "batch: 7/17, train_dl_loss: 0.2890, train_bce_loss: 1.6664, train_bce_dl_loss: 0.2890, step time: 0.3785\n",
      "batch: 8/17, train_dl_loss: 0.2945, train_bce_loss: 1.6633, train_bce_dl_loss: 0.2945, step time: 0.4386\n",
      "batch: 9/17, train_dl_loss: 0.3272, train_bce_loss: 1.6693, train_bce_dl_loss: 0.3272, step time: 0.3805\n",
      "batch: 10/17, train_dl_loss: 0.3893, train_bce_loss: 1.6532, train_bce_dl_loss: 0.3893, step time: 0.4318\n",
      "batch: 11/17, train_dl_loss: 0.2972, train_bce_loss: 1.6472, train_bce_dl_loss: 0.2972, step time: 0.3847\n",
      "batch: 12/17, train_dl_loss: 0.2896, train_bce_loss: 1.6460, train_bce_dl_loss: 0.2896, step time: 0.4253\n",
      "batch: 13/17, train_dl_loss: 0.3467, train_bce_loss: 1.6713, train_bce_dl_loss: 0.3467, step time: 0.3799\n",
      "batch: 14/17, train_dl_loss: 0.3160, train_bce_loss: 1.6736, train_bce_dl_loss: 0.3160, step time: 0.4181\n",
      "batch: 15/17, train_dl_loss: 0.2731, train_bce_loss: 1.6567, train_bce_dl_loss: 0.2731, step time: 0.3727\n",
      "batch: 16/17, train_dl_loss: 0.2597, train_bce_loss: 1.6643, train_bce_dl_loss: 0.2597, step time: 0.4114\n",
      "batch: 17/17, train_dl_loss: 0.2237, train_bce_loss: 1.6697, train_bce_dl_loss: 0.2237, step time: 0.1105\n",
      "LOSS train DiceLoss: 0.3043, LOSS train BCE: 1.6611, LOSS train BCE-DiceLoss: 0.3043, LOSS val DiceLoss: 0.4325, LOSS val BCE: 1.6565, LOSS val BCE-DiceLoss: 0.4325, METRIC val: 0.5336\n",
      "time consuming of epoch 249 is: 426.7975\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2793, train_bce_loss: 1.6491, train_bce_dl_loss: 0.2793, step time: 0.4199\n",
      "batch: 1/17, train_dl_loss: 0.2866, train_bce_loss: 1.6635, train_bce_dl_loss: 0.2866, step time: 0.3749\n",
      "batch: 2/17, train_dl_loss: 0.3031, train_bce_loss: 1.6535, train_bce_dl_loss: 0.3031, step time: 0.4537\n",
      "batch: 3/17, train_dl_loss: 0.3534, train_bce_loss: 1.6597, train_bce_dl_loss: 0.3534, step time: 0.4404\n",
      "batch: 4/17, train_dl_loss: 0.2663, train_bce_loss: 1.6550, train_bce_dl_loss: 0.2663, step time: 0.4150\n",
      "batch: 5/17, train_dl_loss: 0.2997, train_bce_loss: 1.6565, train_bce_dl_loss: 0.2997, step time: 0.3786\n",
      "batch: 6/17, train_dl_loss: 0.3410, train_bce_loss: 1.6624, train_bce_dl_loss: 0.3410, step time: 0.4271\n",
      "batch: 7/17, train_dl_loss: 0.2890, train_bce_loss: 1.6390, train_bce_dl_loss: 0.2890, step time: 0.3749\n",
      "batch: 8/17, train_dl_loss: 0.2689, train_bce_loss: 1.6681, train_bce_dl_loss: 0.2689, step time: 0.4290\n",
      "batch: 9/17, train_dl_loss: 0.3078, train_bce_loss: 1.6692, train_bce_dl_loss: 0.3078, step time: 0.3786\n",
      "batch: 10/17, train_dl_loss: 0.3792, train_bce_loss: 1.6525, train_bce_dl_loss: 0.3792, step time: 0.4140\n",
      "batch: 11/17, train_dl_loss: 0.2738, train_bce_loss: 1.6446, train_bce_dl_loss: 0.2738, step time: 0.4252\n",
      "batch: 12/17, train_dl_loss: 0.2785, train_bce_loss: 1.6547, train_bce_dl_loss: 0.2785, step time: 0.4472\n",
      "batch: 13/17, train_dl_loss: 0.3512, train_bce_loss: 1.6755, train_bce_dl_loss: 0.3512, step time: 0.3720\n",
      "batch: 14/17, train_dl_loss: 0.3115, train_bce_loss: 1.6689, train_bce_dl_loss: 0.3115, step time: 0.4137\n",
      "batch: 15/17, train_dl_loss: 0.2699, train_bce_loss: 1.6696, train_bce_dl_loss: 0.2699, step time: 0.3773\n",
      "batch: 16/17, train_dl_loss: 0.2919, train_bce_loss: 1.6718, train_bce_dl_loss: 0.2919, step time: 0.4347\n",
      "batch: 17/17, train_dl_loss: 0.2885, train_bce_loss: 1.6539, train_bce_dl_loss: 0.2885, step time: 0.1110\n",
      "LOSS train DiceLoss: 0.3022, LOSS train BCE: 1.6593, LOSS train BCE-DiceLoss: 0.3022, LOSS val DiceLoss: 0.4325, LOSS val BCE: 1.6565, LOSS val BCE-DiceLoss: 0.4325, METRIC val: 0.5340\n",
      "time consuming of epoch 250 is: 479.8208\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2936, train_bce_loss: 1.6681, train_bce_dl_loss: 0.2936, step time: 0.4240\n",
      "batch: 1/17, train_dl_loss: 0.2949, train_bce_loss: 1.6655, train_bce_dl_loss: 0.2949, step time: 0.3792\n",
      "batch: 2/17, train_dl_loss: 0.2983, train_bce_loss: 1.6652, train_bce_dl_loss: 0.2983, step time: 0.4274\n",
      "batch: 3/17, train_dl_loss: 0.3597, train_bce_loss: 1.6672, train_bce_dl_loss: 0.3597, step time: 0.3833\n",
      "batch: 4/17, train_dl_loss: 0.3255, train_bce_loss: 1.6588, train_bce_dl_loss: 0.3255, step time: 0.4373\n",
      "batch: 5/17, train_dl_loss: 0.2927, train_bce_loss: 1.6672, train_bce_dl_loss: 0.2927, step time: 0.3836\n",
      "batch: 6/17, train_dl_loss: 0.3809, train_bce_loss: 1.6667, train_bce_dl_loss: 0.3809, step time: 0.4365\n",
      "batch: 7/17, train_dl_loss: 0.2936, train_bce_loss: 1.6647, train_bce_dl_loss: 0.2936, step time: 0.4349\n",
      "batch: 8/17, train_dl_loss: 0.3195, train_bce_loss: 1.6570, train_bce_dl_loss: 0.3195, step time: 0.4238\n",
      "batch: 9/17, train_dl_loss: 0.3426, train_bce_loss: 1.6696, train_bce_dl_loss: 0.3426, step time: 0.4530\n",
      "batch: 10/17, train_dl_loss: 0.3917, train_bce_loss: 1.6538, train_bce_dl_loss: 0.3917, step time: 0.4245\n",
      "batch: 11/17, train_dl_loss: 0.2685, train_bce_loss: 1.6316, train_bce_dl_loss: 0.2685, step time: 0.4466\n",
      "batch: 12/17, train_dl_loss: 0.2925, train_bce_loss: 1.6598, train_bce_dl_loss: 0.2925, step time: 0.4280\n",
      "batch: 13/17, train_dl_loss: 0.3746, train_bce_loss: 1.6737, train_bce_dl_loss: 0.3746, step time: 0.4335\n",
      "batch: 14/17, train_dl_loss: 0.3041, train_bce_loss: 1.6707, train_bce_dl_loss: 0.3041, step time: 0.4397\n",
      "batch: 15/17, train_dl_loss: 0.2941, train_bce_loss: 1.6561, train_bce_dl_loss: 0.2941, step time: 0.3811\n",
      "batch: 16/17, train_dl_loss: 0.2725, train_bce_loss: 1.6587, train_bce_dl_loss: 0.2725, step time: 0.4366\n",
      "batch: 17/17, train_dl_loss: 0.2877, train_bce_loss: 1.6536, train_bce_dl_loss: 0.2877, step time: 0.1127\n",
      "LOSS train DiceLoss: 0.3159, LOSS train BCE: 1.6616, LOSS train BCE-DiceLoss: 0.3159, LOSS val DiceLoss: 0.4317, LOSS val BCE: 1.6564, LOSS val BCE-DiceLoss: 0.4317, METRIC val: 0.5346\n",
      "time consuming of epoch 251 is: 450.9062\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3031, train_bce_loss: 1.6579, train_bce_dl_loss: 0.3031, step time: 0.4342\n",
      "batch: 1/17, train_dl_loss: 0.2985, train_bce_loss: 1.6760, train_bce_dl_loss: 0.2985, step time: 0.3801\n",
      "batch: 2/17, train_dl_loss: 0.2631, train_bce_loss: 1.6572, train_bce_dl_loss: 0.2631, step time: 0.4285\n",
      "batch: 3/17, train_dl_loss: 0.3569, train_bce_loss: 1.6554, train_bce_dl_loss: 0.3569, step time: 0.3768\n",
      "batch: 4/17, train_dl_loss: 0.2640, train_bce_loss: 1.6476, train_bce_dl_loss: 0.2640, step time: 0.4257\n",
      "batch: 5/17, train_dl_loss: 0.2783, train_bce_loss: 1.6576, train_bce_dl_loss: 0.2783, step time: 0.3865\n",
      "batch: 6/17, train_dl_loss: 0.3543, train_bce_loss: 1.6714, train_bce_dl_loss: 0.3543, step time: 0.4287\n",
      "batch: 7/17, train_dl_loss: 0.2794, train_bce_loss: 1.6461, train_bce_dl_loss: 0.2794, step time: 0.3859\n",
      "batch: 8/17, train_dl_loss: 0.2687, train_bce_loss: 1.6562, train_bce_dl_loss: 0.2687, step time: 0.4447\n",
      "batch: 9/17, train_dl_loss: 0.2982, train_bce_loss: 1.6657, train_bce_dl_loss: 0.2982, step time: 0.3785\n",
      "batch: 10/17, train_dl_loss: 0.3966, train_bce_loss: 1.6620, train_bce_dl_loss: 0.3966, step time: 0.4294\n",
      "batch: 11/17, train_dl_loss: 0.2988, train_bce_loss: 1.6419, train_bce_dl_loss: 0.2988, step time: 0.3883\n",
      "batch: 12/17, train_dl_loss: 0.2797, train_bce_loss: 1.6465, train_bce_dl_loss: 0.2797, step time: 0.4203\n",
      "batch: 13/17, train_dl_loss: 0.3867, train_bce_loss: 1.6701, train_bce_dl_loss: 0.3867, step time: 0.3849\n",
      "batch: 14/17, train_dl_loss: 0.3072, train_bce_loss: 1.6786, train_bce_dl_loss: 0.3072, step time: 0.4262\n",
      "batch: 15/17, train_dl_loss: 0.2652, train_bce_loss: 1.6733, train_bce_dl_loss: 0.2652, step time: 0.3913\n",
      "batch: 16/17, train_dl_loss: 0.2701, train_bce_loss: 1.6677, train_bce_dl_loss: 0.2701, step time: 0.4208\n",
      "batch: 17/17, train_dl_loss: 0.2885, train_bce_loss: 1.6532, train_bce_dl_loss: 0.2885, step time: 0.1152\n",
      "LOSS train DiceLoss: 0.3032, LOSS train BCE: 1.6602, LOSS train BCE-DiceLoss: 0.3032, LOSS val DiceLoss: 0.4320, LOSS val BCE: 1.6569, LOSS val BCE-DiceLoss: 0.4320, METRIC val: 0.5342\n",
      "time consuming of epoch 252 is: 455.4449\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2601, train_bce_loss: 1.6458, train_bce_dl_loss: 0.2601, step time: 0.4382\n",
      "batch: 1/17, train_dl_loss: 0.2798, train_bce_loss: 1.6612, train_bce_dl_loss: 0.2798, step time: 0.3771\n",
      "batch: 2/17, train_dl_loss: 0.3176, train_bce_loss: 1.6470, train_bce_dl_loss: 0.3176, step time: 0.4300\n",
      "batch: 3/17, train_dl_loss: 0.3637, train_bce_loss: 1.6608, train_bce_dl_loss: 0.3637, step time: 0.4151\n",
      "batch: 4/17, train_dl_loss: 0.2604, train_bce_loss: 1.6564, train_bce_dl_loss: 0.2604, step time: 0.4292\n",
      "batch: 5/17, train_dl_loss: 0.3002, train_bce_loss: 1.6467, train_bce_dl_loss: 0.3002, step time: 0.4257\n",
      "batch: 6/17, train_dl_loss: 0.3446, train_bce_loss: 1.6698, train_bce_dl_loss: 0.3446, step time: 0.4232\n",
      "batch: 7/17, train_dl_loss: 0.2633, train_bce_loss: 1.6470, train_bce_dl_loss: 0.2633, step time: 0.4304\n",
      "batch: 8/17, train_dl_loss: 0.2868, train_bce_loss: 1.6586, train_bce_dl_loss: 0.2868, step time: 0.4288\n",
      "batch: 9/17, train_dl_loss: 0.2842, train_bce_loss: 1.6718, train_bce_dl_loss: 0.2842, step time: 0.4318\n",
      "batch: 10/17, train_dl_loss: 0.3716, train_bce_loss: 1.6632, train_bce_dl_loss: 0.3716, step time: 0.4198\n",
      "batch: 11/17, train_dl_loss: 0.2492, train_bce_loss: 1.6315, train_bce_dl_loss: 0.2492, step time: 0.4349\n",
      "batch: 12/17, train_dl_loss: 0.3126, train_bce_loss: 1.6654, train_bce_dl_loss: 0.3126, step time: 0.4150\n",
      "batch: 13/17, train_dl_loss: 0.3730, train_bce_loss: 1.6690, train_bce_dl_loss: 0.3730, step time: 0.4217\n",
      "batch: 14/17, train_dl_loss: 0.3686, train_bce_loss: 1.6701, train_bce_dl_loss: 0.3686, step time: 0.4229\n",
      "batch: 15/17, train_dl_loss: 0.2652, train_bce_loss: 1.6650, train_bce_dl_loss: 0.2652, step time: 0.4296\n",
      "batch: 16/17, train_dl_loss: 0.2612, train_bce_loss: 1.6692, train_bce_dl_loss: 0.2612, step time: 0.4100\n",
      "batch: 17/17, train_dl_loss: 0.4013, train_bce_loss: 1.6778, train_bce_dl_loss: 0.4013, step time: 0.1117\n",
      "LOSS train DiceLoss: 0.3091, LOSS train BCE: 1.6598, LOSS train BCE-DiceLoss: 0.3091, LOSS val DiceLoss: 0.4320, LOSS val BCE: 1.6566, LOSS val BCE-DiceLoss: 0.4320, METRIC val: 0.5343\n",
      "time consuming of epoch 253 is: 767.4681\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2749, train_bce_loss: 1.6500, train_bce_dl_loss: 0.2749, step time: 0.4194\n",
      "batch: 1/17, train_dl_loss: 0.3039, train_bce_loss: 1.6640, train_bce_dl_loss: 0.3039, step time: 0.3786\n",
      "batch: 2/17, train_dl_loss: 0.3005, train_bce_loss: 1.6537, train_bce_dl_loss: 0.3005, step time: 0.4311\n",
      "batch: 3/17, train_dl_loss: 0.3834, train_bce_loss: 1.6554, train_bce_dl_loss: 0.3834, step time: 0.3798\n",
      "batch: 4/17, train_dl_loss: 0.2557, train_bce_loss: 1.6516, train_bce_dl_loss: 0.2557, step time: 0.4343\n",
      "batch: 5/17, train_dl_loss: 0.2588, train_bce_loss: 1.6510, train_bce_dl_loss: 0.2588, step time: 0.3792\n",
      "batch: 6/17, train_dl_loss: 0.3221, train_bce_loss: 1.6701, train_bce_dl_loss: 0.3221, step time: 0.4287\n",
      "batch: 7/17, train_dl_loss: 0.2639, train_bce_loss: 1.6432, train_bce_dl_loss: 0.2639, step time: 0.3830\n",
      "batch: 8/17, train_dl_loss: 0.2727, train_bce_loss: 1.6596, train_bce_dl_loss: 0.2727, step time: 0.4334\n",
      "batch: 9/17, train_dl_loss: 0.3269, train_bce_loss: 1.6640, train_bce_dl_loss: 0.3269, step time: 0.3798\n",
      "batch: 10/17, train_dl_loss: 0.3812, train_bce_loss: 1.6617, train_bce_dl_loss: 0.3812, step time: 0.4427\n",
      "batch: 11/17, train_dl_loss: 0.2833, train_bce_loss: 1.6483, train_bce_dl_loss: 0.2833, step time: 0.3858\n",
      "batch: 12/17, train_dl_loss: 0.3211, train_bce_loss: 1.6437, train_bce_dl_loss: 0.3211, step time: 0.4436\n",
      "batch: 13/17, train_dl_loss: 0.4215, train_bce_loss: 1.6687, train_bce_dl_loss: 0.4215, step time: 0.3758\n",
      "batch: 14/17, train_dl_loss: 0.2770, train_bce_loss: 1.6769, train_bce_dl_loss: 0.2770, step time: 0.4266\n",
      "batch: 15/17, train_dl_loss: 0.2644, train_bce_loss: 1.6618, train_bce_dl_loss: 0.2644, step time: 0.3738\n",
      "batch: 16/17, train_dl_loss: 0.2867, train_bce_loss: 1.6678, train_bce_dl_loss: 0.2867, step time: 0.4109\n",
      "batch: 17/17, train_dl_loss: 0.2393, train_bce_loss: 1.6658, train_bce_dl_loss: 0.2393, step time: 0.1119\n",
      "LOSS train DiceLoss: 0.3021, LOSS train BCE: 1.6587, LOSS train BCE-DiceLoss: 0.3021, LOSS val DiceLoss: 0.4319, LOSS val BCE: 1.6562, LOSS val BCE-DiceLoss: 0.4319, METRIC val: 0.5344\n",
      "time consuming of epoch 254 is: 441.8107\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2775, train_bce_loss: 1.6511, train_bce_dl_loss: 0.2775, step time: 0.4210\n",
      "batch: 1/17, train_dl_loss: 0.3147, train_bce_loss: 1.6662, train_bce_dl_loss: 0.3147, step time: 0.3874\n",
      "batch: 2/17, train_dl_loss: 0.3108, train_bce_loss: 1.6609, train_bce_dl_loss: 0.3108, step time: 0.4432\n",
      "batch: 3/17, train_dl_loss: 0.4335, train_bce_loss: 1.6709, train_bce_dl_loss: 0.4335, step time: 0.3865\n",
      "batch: 4/17, train_dl_loss: 0.2808, train_bce_loss: 1.6544, train_bce_dl_loss: 0.2808, step time: 0.4193\n",
      "batch: 5/17, train_dl_loss: 0.2924, train_bce_loss: 1.6650, train_bce_dl_loss: 0.2924, step time: 0.3853\n",
      "batch: 6/17, train_dl_loss: 0.3382, train_bce_loss: 1.6763, train_bce_dl_loss: 0.3382, step time: 0.4254\n",
      "batch: 7/17, train_dl_loss: 0.2989, train_bce_loss: 1.6511, train_bce_dl_loss: 0.2989, step time: 0.3840\n",
      "batch: 8/17, train_dl_loss: 0.2955, train_bce_loss: 1.6464, train_bce_dl_loss: 0.2955, step time: 0.4306\n",
      "batch: 9/17, train_dl_loss: 0.3633, train_bce_loss: 1.6737, train_bce_dl_loss: 0.3633, step time: 0.3875\n",
      "batch: 10/17, train_dl_loss: 0.4327, train_bce_loss: 1.6576, train_bce_dl_loss: 0.4327, step time: 0.4363\n",
      "batch: 11/17, train_dl_loss: 0.2785, train_bce_loss: 1.6421, train_bce_dl_loss: 0.2785, step time: 0.3768\n",
      "batch: 12/17, train_dl_loss: 0.2875, train_bce_loss: 1.6557, train_bce_dl_loss: 0.2875, step time: 0.4253\n",
      "batch: 13/17, train_dl_loss: 0.3790, train_bce_loss: 1.6700, train_bce_dl_loss: 0.3790, step time: 0.3798\n",
      "batch: 14/17, train_dl_loss: 0.2883, train_bce_loss: 1.6696, train_bce_dl_loss: 0.2883, step time: 0.4208\n",
      "batch: 15/17, train_dl_loss: 0.2837, train_bce_loss: 1.6662, train_bce_dl_loss: 0.2837, step time: 0.3717\n",
      "batch: 16/17, train_dl_loss: 0.2675, train_bce_loss: 1.6617, train_bce_dl_loss: 0.2675, step time: 0.4202\n",
      "batch: 17/17, train_dl_loss: 0.2273, train_bce_loss: 1.6542, train_bce_dl_loss: 0.2273, step time: 0.1116\n",
      "LOSS train DiceLoss: 0.3139, LOSS train BCE: 1.6607, LOSS train BCE-DiceLoss: 0.3139, LOSS val DiceLoss: 0.4318, LOSS val BCE: 1.6568, LOSS val BCE-DiceLoss: 0.4318, METRIC val: 0.5346\n",
      "time consuming of epoch 255 is: 491.2558\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2926, train_bce_loss: 1.6458, train_bce_dl_loss: 0.2926, step time: 0.4215\n",
      "batch: 1/17, train_dl_loss: 0.2850, train_bce_loss: 1.6648, train_bce_dl_loss: 0.2850, step time: 0.3775\n",
      "batch: 2/17, train_dl_loss: 0.2921, train_bce_loss: 1.6509, train_bce_dl_loss: 0.2921, step time: 0.4344\n",
      "batch: 3/17, train_dl_loss: 0.3581, train_bce_loss: 1.6586, train_bce_dl_loss: 0.3581, step time: 0.3787\n",
      "batch: 4/17, train_dl_loss: 0.2642, train_bce_loss: 1.6496, train_bce_dl_loss: 0.2642, step time: 0.4291\n",
      "batch: 5/17, train_dl_loss: 0.2578, train_bce_loss: 1.6537, train_bce_dl_loss: 0.2578, step time: 0.3856\n",
      "batch: 6/17, train_dl_loss: 0.3314, train_bce_loss: 1.6633, train_bce_dl_loss: 0.3314, step time: 0.4368\n",
      "batch: 7/17, train_dl_loss: 0.2657, train_bce_loss: 1.6549, train_bce_dl_loss: 0.2657, step time: 0.3874\n",
      "batch: 8/17, train_dl_loss: 0.2766, train_bce_loss: 1.6630, train_bce_dl_loss: 0.2766, step time: 0.4289\n",
      "batch: 9/17, train_dl_loss: 0.3397, train_bce_loss: 1.6746, train_bce_dl_loss: 0.3397, step time: 0.3836\n",
      "batch: 10/17, train_dl_loss: 0.3641, train_bce_loss: 1.6594, train_bce_dl_loss: 0.3641, step time: 0.4126\n",
      "batch: 11/17, train_dl_loss: 0.3021, train_bce_loss: 1.6522, train_bce_dl_loss: 0.3021, step time: 0.4260\n",
      "batch: 12/17, train_dl_loss: 0.2896, train_bce_loss: 1.6560, train_bce_dl_loss: 0.2896, step time: 0.4334\n",
      "batch: 13/17, train_dl_loss: 0.3873, train_bce_loss: 1.6684, train_bce_dl_loss: 0.3873, step time: 0.3782\n",
      "batch: 14/17, train_dl_loss: 0.2867, train_bce_loss: 1.6762, train_bce_dl_loss: 0.2867, step time: 0.4202\n",
      "batch: 15/17, train_dl_loss: 0.2697, train_bce_loss: 1.6669, train_bce_dl_loss: 0.2697, step time: 0.3745\n",
      "batch: 16/17, train_dl_loss: 0.2488, train_bce_loss: 1.6685, train_bce_dl_loss: 0.2488, step time: 0.4089\n",
      "batch: 17/17, train_dl_loss: 0.2243, train_bce_loss: 1.6718, train_bce_dl_loss: 0.2243, step time: 0.1120\n",
      "LOSS train DiceLoss: 0.2964, LOSS train BCE: 1.6610, LOSS train BCE-DiceLoss: 0.2964, LOSS val DiceLoss: 0.4316, LOSS val BCE: 1.6567, LOSS val BCE-DiceLoss: 0.4316, METRIC val: 0.5348\n",
      "time consuming of epoch 256 is: 524.3757\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2740, train_bce_loss: 1.6489, train_bce_dl_loss: 0.2740, step time: 0.4202\n",
      "batch: 1/17, train_dl_loss: 0.3144, train_bce_loss: 1.6615, train_bce_dl_loss: 0.3144, step time: 0.3758\n",
      "batch: 2/17, train_dl_loss: 0.2979, train_bce_loss: 1.6641, train_bce_dl_loss: 0.2979, step time: 0.4386\n",
      "batch: 3/17, train_dl_loss: 0.3388, train_bce_loss: 1.6631, train_bce_dl_loss: 0.3388, step time: 0.3816\n",
      "batch: 4/17, train_dl_loss: 0.2880, train_bce_loss: 1.6650, train_bce_dl_loss: 0.2880, step time: 0.4855\n",
      "batch: 5/17, train_dl_loss: 0.2571, train_bce_loss: 1.6530, train_bce_dl_loss: 0.2571, step time: 0.4308\n",
      "batch: 6/17, train_dl_loss: 0.3487, train_bce_loss: 1.6730, train_bce_dl_loss: 0.3487, step time: 0.4325\n",
      "batch: 7/17, train_dl_loss: 0.3133, train_bce_loss: 1.6608, train_bce_dl_loss: 0.3133, step time: 0.3785\n",
      "batch: 8/17, train_dl_loss: 0.2750, train_bce_loss: 1.6479, train_bce_dl_loss: 0.2750, step time: 0.4264\n",
      "batch: 9/17, train_dl_loss: 0.3146, train_bce_loss: 1.6602, train_bce_dl_loss: 0.3146, step time: 0.3799\n",
      "batch: 10/17, train_dl_loss: 0.3956, train_bce_loss: 1.6627, train_bce_dl_loss: 0.3956, step time: 0.4231\n",
      "batch: 11/17, train_dl_loss: 0.2778, train_bce_loss: 1.6403, train_bce_dl_loss: 0.2778, step time: 0.4410\n",
      "batch: 12/17, train_dl_loss: 0.3154, train_bce_loss: 1.6520, train_bce_dl_loss: 0.3154, step time: 0.4324\n",
      "batch: 13/17, train_dl_loss: 0.3643, train_bce_loss: 1.6687, train_bce_dl_loss: 0.3643, step time: 0.4253\n",
      "batch: 14/17, train_dl_loss: 0.3082, train_bce_loss: 1.6711, train_bce_dl_loss: 0.3082, step time: 0.4342\n",
      "batch: 15/17, train_dl_loss: 0.2631, train_bce_loss: 1.6713, train_bce_dl_loss: 0.2631, step time: 0.4229\n",
      "batch: 16/17, train_dl_loss: 0.2721, train_bce_loss: 1.6572, train_bce_dl_loss: 0.2721, step time: 0.4218\n",
      "batch: 17/17, train_dl_loss: 0.2241, train_bce_loss: 1.6720, train_bce_dl_loss: 0.2241, step time: 0.1111\n",
      "LOSS train DiceLoss: 0.3024, LOSS train BCE: 1.6607, LOSS train BCE-DiceLoss: 0.3024, LOSS val DiceLoss: 0.4302, LOSS val BCE: 1.6561, LOSS val BCE-DiceLoss: 0.4302, METRIC val: 0.5361\n",
      "time consuming of epoch 257 is: 529.8335\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2813, train_bce_loss: 1.6463, train_bce_dl_loss: 0.2813, step time: 0.4423\n",
      "batch: 1/17, train_dl_loss: 0.2816, train_bce_loss: 1.6720, train_bce_dl_loss: 0.2816, step time: 0.3768\n",
      "batch: 2/17, train_dl_loss: 0.3228, train_bce_loss: 1.6570, train_bce_dl_loss: 0.3228, step time: 0.4351\n",
      "batch: 3/17, train_dl_loss: 0.3550, train_bce_loss: 1.6627, train_bce_dl_loss: 0.3550, step time: 0.3846\n",
      "batch: 4/17, train_dl_loss: 0.2523, train_bce_loss: 1.6673, train_bce_dl_loss: 0.2523, step time: 0.4320\n",
      "batch: 5/17, train_dl_loss: 0.2809, train_bce_loss: 1.6424, train_bce_dl_loss: 0.2809, step time: 0.4009\n",
      "batch: 6/17, train_dl_loss: 0.3299, train_bce_loss: 1.6690, train_bce_dl_loss: 0.3299, step time: 0.4254\n",
      "batch: 7/17, train_dl_loss: 0.2527, train_bce_loss: 1.6556, train_bce_dl_loss: 0.2527, step time: 0.3793\n",
      "batch: 8/17, train_dl_loss: 0.3027, train_bce_loss: 1.6520, train_bce_dl_loss: 0.3027, step time: 0.4204\n",
      "batch: 9/17, train_dl_loss: 0.2650, train_bce_loss: 1.6717, train_bce_dl_loss: 0.2650, step time: 0.3753\n",
      "batch: 10/17, train_dl_loss: 0.3812, train_bce_loss: 1.6567, train_bce_dl_loss: 0.3812, step time: 0.4242\n",
      "batch: 11/17, train_dl_loss: 0.3370, train_bce_loss: 1.6465, train_bce_dl_loss: 0.3370, step time: 0.4415\n",
      "batch: 12/17, train_dl_loss: 0.2954, train_bce_loss: 1.6519, train_bce_dl_loss: 0.2954, step time: 0.4955\n",
      "batch: 13/17, train_dl_loss: 0.3700, train_bce_loss: 1.6698, train_bce_dl_loss: 0.3700, step time: 0.3794\n",
      "batch: 14/17, train_dl_loss: 0.2923, train_bce_loss: 1.6722, train_bce_dl_loss: 0.2923, step time: 0.4186\n",
      "batch: 15/17, train_dl_loss: 0.3143, train_bce_loss: 1.6615, train_bce_dl_loss: 0.3143, step time: 0.3802\n",
      "batch: 16/17, train_dl_loss: 0.2679, train_bce_loss: 1.6649, train_bce_dl_loss: 0.2679, step time: 0.4262\n",
      "batch: 17/17, train_dl_loss: 0.2886, train_bce_loss: 1.6532, train_bce_dl_loss: 0.2886, step time: 0.1117\n",
      "LOSS train DiceLoss: 0.3039, LOSS train BCE: 1.6596, LOSS train BCE-DiceLoss: 0.3039, LOSS val DiceLoss: 0.4314, LOSS val BCE: 1.6563, LOSS val BCE-DiceLoss: 0.4314, METRIC val: 0.5356\n",
      "time consuming of epoch 258 is: 449.6830\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2677, train_bce_loss: 1.6687, train_bce_dl_loss: 0.2677, step time: 0.4375\n",
      "batch: 1/17, train_dl_loss: 0.2780, train_bce_loss: 1.6715, train_bce_dl_loss: 0.2780, step time: 0.3829\n",
      "batch: 2/17, train_dl_loss: 0.2750, train_bce_loss: 1.6595, train_bce_dl_loss: 0.2750, step time: 0.4400\n",
      "batch: 3/17, train_dl_loss: 0.3689, train_bce_loss: 1.6540, train_bce_dl_loss: 0.3689, step time: 0.3726\n",
      "batch: 4/17, train_dl_loss: 0.2934, train_bce_loss: 1.6668, train_bce_dl_loss: 0.2934, step time: 0.4144\n",
      "batch: 5/17, train_dl_loss: 0.2766, train_bce_loss: 1.6542, train_bce_dl_loss: 0.2766, step time: 0.3735\n",
      "batch: 6/17, train_dl_loss: 0.3462, train_bce_loss: 1.6700, train_bce_dl_loss: 0.3462, step time: 0.4257\n",
      "batch: 7/17, train_dl_loss: 0.2835, train_bce_loss: 1.6555, train_bce_dl_loss: 0.2835, step time: 0.3795\n",
      "batch: 8/17, train_dl_loss: 0.2951, train_bce_loss: 1.6465, train_bce_dl_loss: 0.2951, step time: 0.4257\n",
      "batch: 9/17, train_dl_loss: 0.2985, train_bce_loss: 1.6627, train_bce_dl_loss: 0.2985, step time: 0.3814\n",
      "batch: 10/17, train_dl_loss: 0.3918, train_bce_loss: 1.6638, train_bce_dl_loss: 0.3918, step time: 0.4331\n",
      "batch: 11/17, train_dl_loss: 0.2598, train_bce_loss: 1.6398, train_bce_dl_loss: 0.2598, step time: 0.4162\n",
      "batch: 12/17, train_dl_loss: 0.3764, train_bce_loss: 1.6483, train_bce_dl_loss: 0.3764, step time: 0.4163\n",
      "batch: 13/17, train_dl_loss: 0.3797, train_bce_loss: 1.6710, train_bce_dl_loss: 0.3797, step time: 0.3868\n",
      "batch: 14/17, train_dl_loss: 0.3193, train_bce_loss: 1.6688, train_bce_dl_loss: 0.3193, step time: 0.4416\n",
      "batch: 15/17, train_dl_loss: 0.2693, train_bce_loss: 1.6676, train_bce_dl_loss: 0.2693, step time: 0.3872\n",
      "batch: 16/17, train_dl_loss: 0.2770, train_bce_loss: 1.6679, train_bce_dl_loss: 0.2770, step time: 0.4212\n",
      "batch: 17/17, train_dl_loss: 0.2362, train_bce_loss: 1.6546, train_bce_dl_loss: 0.2362, step time: 0.1120\n",
      "LOSS train DiceLoss: 0.3051, LOSS train BCE: 1.6606, LOSS train BCE-DiceLoss: 0.3051, LOSS val DiceLoss: 0.4309, LOSS val BCE: 1.6562, LOSS val BCE-DiceLoss: 0.4309, METRIC val: 0.5355\n",
      "time consuming of epoch 259 is: 453.4373\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3986, train_bce_loss: 1.6723, train_bce_dl_loss: 0.3986, step time: 0.4238\n",
      "batch: 1/17, train_dl_loss: 0.2809, train_bce_loss: 1.6606, train_bce_dl_loss: 0.2809, step time: 0.3711\n",
      "batch: 2/17, train_dl_loss: 0.2887, train_bce_loss: 1.6629, train_bce_dl_loss: 0.2887, step time: 0.4325\n",
      "batch: 3/17, train_dl_loss: 0.3626, train_bce_loss: 1.6627, train_bce_dl_loss: 0.3626, step time: 0.3781\n",
      "batch: 4/17, train_dl_loss: 0.2800, train_bce_loss: 1.6482, train_bce_dl_loss: 0.2800, step time: 0.4341\n",
      "batch: 5/17, train_dl_loss: 0.2897, train_bce_loss: 1.6447, train_bce_dl_loss: 0.2897, step time: 0.3805\n",
      "batch: 6/17, train_dl_loss: 0.3639, train_bce_loss: 1.6651, train_bce_dl_loss: 0.3639, step time: 0.4463\n",
      "batch: 7/17, train_dl_loss: 0.2841, train_bce_loss: 1.6403, train_bce_dl_loss: 0.2841, step time: 0.3794\n",
      "batch: 8/17, train_dl_loss: 0.2711, train_bce_loss: 1.6598, train_bce_dl_loss: 0.2711, step time: 0.4433\n",
      "batch: 9/17, train_dl_loss: 0.3086, train_bce_loss: 1.6677, train_bce_dl_loss: 0.3086, step time: 0.3809\n",
      "batch: 10/17, train_dl_loss: 0.4219, train_bce_loss: 1.6569, train_bce_dl_loss: 0.4219, step time: 0.4407\n",
      "batch: 11/17, train_dl_loss: 0.2913, train_bce_loss: 1.6397, train_bce_dl_loss: 0.2913, step time: 0.3840\n",
      "batch: 12/17, train_dl_loss: 0.3181, train_bce_loss: 1.6466, train_bce_dl_loss: 0.3181, step time: 0.4153\n",
      "batch: 13/17, train_dl_loss: 0.4108, train_bce_loss: 1.6694, train_bce_dl_loss: 0.4108, step time: 0.3874\n",
      "batch: 14/17, train_dl_loss: 0.3173, train_bce_loss: 1.6644, train_bce_dl_loss: 0.3173, step time: 0.4224\n",
      "batch: 15/17, train_dl_loss: 0.2849, train_bce_loss: 1.6637, train_bce_dl_loss: 0.2849, step time: 0.3750\n",
      "batch: 16/17, train_dl_loss: 0.2646, train_bce_loss: 1.6687, train_bce_dl_loss: 0.2646, step time: 0.4146\n",
      "batch: 17/17, train_dl_loss: 0.2677, train_bce_loss: 1.6665, train_bce_dl_loss: 0.2677, step time: 0.1116\n",
      "LOSS train DiceLoss: 0.3169, LOSS train BCE: 1.6589, LOSS train BCE-DiceLoss: 0.3169, LOSS val DiceLoss: 0.4307, LOSS val BCE: 1.6558, LOSS val BCE-DiceLoss: 0.4307, METRIC val: 0.5355\n",
      "time consuming of epoch 260 is: 445.0203\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2827, train_bce_loss: 1.6506, train_bce_dl_loss: 0.2827, step time: 0.4324\n",
      "batch: 1/17, train_dl_loss: 0.2734, train_bce_loss: 1.6747, train_bce_dl_loss: 0.2734, step time: 0.3756\n",
      "batch: 2/17, train_dl_loss: 0.2789, train_bce_loss: 1.6562, train_bce_dl_loss: 0.2789, step time: 0.4351\n",
      "batch: 3/17, train_dl_loss: 0.3808, train_bce_loss: 1.6615, train_bce_dl_loss: 0.3808, step time: 0.3759\n",
      "batch: 4/17, train_dl_loss: 0.2840, train_bce_loss: 1.6430, train_bce_dl_loss: 0.2840, step time: 0.4179\n",
      "batch: 5/17, train_dl_loss: 0.2674, train_bce_loss: 1.6482, train_bce_dl_loss: 0.2674, step time: 0.3827\n",
      "batch: 6/17, train_dl_loss: 0.3321, train_bce_loss: 1.6759, train_bce_dl_loss: 0.3321, step time: 0.4346\n",
      "batch: 7/17, train_dl_loss: 0.2685, train_bce_loss: 1.6482, train_bce_dl_loss: 0.2685, step time: 0.3766\n",
      "batch: 8/17, train_dl_loss: 0.2892, train_bce_loss: 1.6637, train_bce_dl_loss: 0.2892, step time: 0.4342\n",
      "batch: 9/17, train_dl_loss: 0.3059, train_bce_loss: 1.6594, train_bce_dl_loss: 0.3059, step time: 0.3793\n",
      "batch: 10/17, train_dl_loss: 0.3785, train_bce_loss: 1.6646, train_bce_dl_loss: 0.3785, step time: 0.4206\n",
      "batch: 11/17, train_dl_loss: 0.2778, train_bce_loss: 1.6474, train_bce_dl_loss: 0.2778, step time: 0.4358\n",
      "batch: 12/17, train_dl_loss: 0.2766, train_bce_loss: 1.6463, train_bce_dl_loss: 0.2766, step time: 0.4393\n",
      "batch: 13/17, train_dl_loss: 0.4332, train_bce_loss: 1.6759, train_bce_dl_loss: 0.4332, step time: 0.3827\n",
      "batch: 14/17, train_dl_loss: 0.2615, train_bce_loss: 1.6762, train_bce_dl_loss: 0.2615, step time: 0.4208\n",
      "batch: 15/17, train_dl_loss: 0.3313, train_bce_loss: 1.6655, train_bce_dl_loss: 0.3313, step time: 0.4038\n",
      "batch: 16/17, train_dl_loss: 0.2681, train_bce_loss: 1.6591, train_bce_dl_loss: 0.2681, step time: 0.4173\n",
      "batch: 17/17, train_dl_loss: 0.2868, train_bce_loss: 1.6525, train_bce_dl_loss: 0.2868, step time: 0.1170\n",
      "LOSS train DiceLoss: 0.3043, LOSS train BCE: 1.6594, LOSS train BCE-DiceLoss: 0.3043, LOSS val DiceLoss: 0.4297, LOSS val BCE: 1.6554, LOSS val BCE-DiceLoss: 0.4297, METRIC val: 0.5369\n",
      "time consuming of epoch 261 is: 449.3291\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2923, train_bce_loss: 1.6421, train_bce_dl_loss: 0.2923, step time: 0.4231\n",
      "batch: 1/17, train_dl_loss: 0.3077, train_bce_loss: 1.6627, train_bce_dl_loss: 0.3077, step time: 0.3749\n",
      "batch: 2/17, train_dl_loss: 0.3011, train_bce_loss: 1.6687, train_bce_dl_loss: 0.3011, step time: 0.4259\n",
      "batch: 3/17, train_dl_loss: 0.3976, train_bce_loss: 1.6670, train_bce_dl_loss: 0.3976, step time: 0.3871\n",
      "batch: 4/17, train_dl_loss: 0.2845, train_bce_loss: 1.6515, train_bce_dl_loss: 0.2845, step time: 0.4283\n",
      "batch: 5/17, train_dl_loss: 0.2638, train_bce_loss: 1.6581, train_bce_dl_loss: 0.2638, step time: 0.4314\n",
      "batch: 6/17, train_dl_loss: 0.3319, train_bce_loss: 1.6709, train_bce_dl_loss: 0.3319, step time: 0.4462\n",
      "batch: 7/17, train_dl_loss: 0.2487, train_bce_loss: 1.6414, train_bce_dl_loss: 0.2487, step time: 0.3922\n",
      "batch: 8/17, train_dl_loss: 0.2955, train_bce_loss: 1.6400, train_bce_dl_loss: 0.2955, step time: 0.4356\n",
      "batch: 9/17, train_dl_loss: 0.2833, train_bce_loss: 1.6610, train_bce_dl_loss: 0.2833, step time: 0.4338\n",
      "batch: 10/17, train_dl_loss: 0.4188, train_bce_loss: 1.6652, train_bce_dl_loss: 0.4188, step time: 0.4365\n",
      "batch: 11/17, train_dl_loss: 0.3183, train_bce_loss: 1.6353, train_bce_dl_loss: 0.3183, step time: 0.4437\n",
      "batch: 12/17, train_dl_loss: 0.2788, train_bce_loss: 1.6675, train_bce_dl_loss: 0.2788, step time: 0.4656\n",
      "batch: 13/17, train_dl_loss: 0.4179, train_bce_loss: 1.6700, train_bce_dl_loss: 0.4179, step time: 0.3785\n",
      "batch: 14/17, train_dl_loss: 0.2926, train_bce_loss: 1.6759, train_bce_dl_loss: 0.2926, step time: 0.4297\n",
      "batch: 15/17, train_dl_loss: 0.2918, train_bce_loss: 1.6555, train_bce_dl_loss: 0.2918, step time: 0.3813\n",
      "batch: 16/17, train_dl_loss: 0.2929, train_bce_loss: 1.6744, train_bce_dl_loss: 0.2929, step time: 0.4100\n",
      "batch: 17/17, train_dl_loss: 0.2379, train_bce_loss: 1.6675, train_bce_dl_loss: 0.2379, step time: 0.1112\n",
      "LOSS train DiceLoss: 0.3086, LOSS train BCE: 1.6597, LOSS train BCE-DiceLoss: 0.3086, LOSS val DiceLoss: 0.4308, LOSS val BCE: 1.6555, LOSS val BCE-DiceLoss: 0.4308, METRIC val: 0.5354\n",
      "time consuming of epoch 262 is: 423.9357\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2926, train_bce_loss: 1.6661, train_bce_dl_loss: 0.2926, step time: 0.4865\n",
      "batch: 1/17, train_dl_loss: 0.2864, train_bce_loss: 1.6561, train_bce_dl_loss: 0.2864, step time: 0.4103\n",
      "batch: 2/17, train_dl_loss: 0.3180, train_bce_loss: 1.6661, train_bce_dl_loss: 0.3180, step time: 0.4277\n",
      "batch: 3/17, train_dl_loss: 0.3653, train_bce_loss: 1.6683, train_bce_dl_loss: 0.3653, step time: 0.3775\n",
      "batch: 4/17, train_dl_loss: 0.2465, train_bce_loss: 1.6654, train_bce_dl_loss: 0.2465, step time: 0.4385\n",
      "batch: 5/17, train_dl_loss: 0.2738, train_bce_loss: 1.6594, train_bce_dl_loss: 0.2738, step time: 0.3858\n",
      "batch: 6/17, train_dl_loss: 0.3211, train_bce_loss: 1.6664, train_bce_dl_loss: 0.3211, step time: 0.4273\n",
      "batch: 7/17, train_dl_loss: 0.2973, train_bce_loss: 1.6424, train_bce_dl_loss: 0.2973, step time: 0.3818\n",
      "batch: 8/17, train_dl_loss: 0.3156, train_bce_loss: 1.6413, train_bce_dl_loss: 0.3156, step time: 0.4324\n",
      "batch: 9/17, train_dl_loss: 0.3071, train_bce_loss: 1.6717, train_bce_dl_loss: 0.3071, step time: 0.3769\n",
      "batch: 10/17, train_dl_loss: 0.3829, train_bce_loss: 1.6559, train_bce_dl_loss: 0.3829, step time: 0.4436\n",
      "batch: 11/17, train_dl_loss: 0.2756, train_bce_loss: 1.6442, train_bce_dl_loss: 0.2756, step time: 0.4325\n",
      "batch: 12/17, train_dl_loss: 0.2586, train_bce_loss: 1.6455, train_bce_dl_loss: 0.2586, step time: 0.4168\n",
      "batch: 13/17, train_dl_loss: 0.4069, train_bce_loss: 1.6719, train_bce_dl_loss: 0.4069, step time: 0.3794\n",
      "batch: 14/17, train_dl_loss: 0.3191, train_bce_loss: 1.6675, train_bce_dl_loss: 0.3191, step time: 0.4348\n",
      "batch: 15/17, train_dl_loss: 0.2874, train_bce_loss: 1.6571, train_bce_dl_loss: 0.2874, step time: 0.3718\n",
      "batch: 16/17, train_dl_loss: 0.2772, train_bce_loss: 1.6668, train_bce_dl_loss: 0.2772, step time: 0.4242\n",
      "batch: 17/17, train_dl_loss: 0.2414, train_bce_loss: 1.6595, train_bce_dl_loss: 0.2414, step time: 0.1117\n",
      "LOSS train DiceLoss: 0.3040, LOSS train BCE: 1.6595, LOSS train BCE-DiceLoss: 0.3040, LOSS val DiceLoss: 0.4312, LOSS val BCE: 1.6547, LOSS val BCE-DiceLoss: 0.4312, METRIC val: 0.5352\n",
      "time consuming of epoch 263 is: 461.2672\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2795, train_bce_loss: 1.6461, train_bce_dl_loss: 0.2795, step time: 0.4329\n",
      "batch: 1/17, train_dl_loss: 0.2984, train_bce_loss: 1.6553, train_bce_dl_loss: 0.2984, step time: 0.3804\n",
      "batch: 2/17, train_dl_loss: 0.3621, train_bce_loss: 1.6655, train_bce_dl_loss: 0.3621, step time: 0.4182\n",
      "batch: 3/17, train_dl_loss: 0.3533, train_bce_loss: 1.6653, train_bce_dl_loss: 0.3533, step time: 0.3770\n",
      "batch: 4/17, train_dl_loss: 0.2456, train_bce_loss: 1.6556, train_bce_dl_loss: 0.2456, step time: 0.4218\n",
      "batch: 5/17, train_dl_loss: 0.2492, train_bce_loss: 1.6424, train_bce_dl_loss: 0.2492, step time: 0.3953\n",
      "batch: 6/17, train_dl_loss: 0.3737, train_bce_loss: 1.6779, train_bce_dl_loss: 0.3737, step time: 0.4294\n",
      "batch: 7/17, train_dl_loss: 0.2951, train_bce_loss: 1.6657, train_bce_dl_loss: 0.2951, step time: 0.4316\n",
      "batch: 8/17, train_dl_loss: 0.2820, train_bce_loss: 1.6538, train_bce_dl_loss: 0.2820, step time: 0.4236\n",
      "batch: 9/17, train_dl_loss: 0.3301, train_bce_loss: 1.6645, train_bce_dl_loss: 0.3301, step time: 0.4306\n",
      "batch: 10/17, train_dl_loss: 0.3748, train_bce_loss: 1.6486, train_bce_dl_loss: 0.3748, step time: 0.4363\n",
      "batch: 11/17, train_dl_loss: 0.2682, train_bce_loss: 1.6540, train_bce_dl_loss: 0.2682, step time: 0.4383\n",
      "batch: 12/17, train_dl_loss: 0.3119, train_bce_loss: 1.6580, train_bce_dl_loss: 0.3119, step time: 0.4230\n",
      "batch: 13/17, train_dl_loss: 0.3717, train_bce_loss: 1.6676, train_bce_dl_loss: 0.3717, step time: 0.4405\n",
      "batch: 14/17, train_dl_loss: 0.3231, train_bce_loss: 1.6673, train_bce_dl_loss: 0.3231, step time: 0.4285\n",
      "batch: 15/17, train_dl_loss: 0.2818, train_bce_loss: 1.6629, train_bce_dl_loss: 0.2818, step time: 0.3979\n",
      "batch: 16/17, train_dl_loss: 0.2910, train_bce_loss: 1.6615, train_bce_dl_loss: 0.2910, step time: 0.4219\n",
      "batch: 17/17, train_dl_loss: 0.2888, train_bce_loss: 1.6510, train_bce_dl_loss: 0.2888, step time: 0.1117\n",
      "LOSS train DiceLoss: 0.3100, LOSS train BCE: 1.6590, LOSS train BCE-DiceLoss: 0.3100, LOSS val DiceLoss: 0.4285, LOSS val BCE: 1.6544, LOSS val BCE-DiceLoss: 0.4285, METRIC val: 0.5379\n",
      "time consuming of epoch 264 is: 448.2155\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3248, train_bce_loss: 1.6589, train_bce_dl_loss: 0.3248, step time: 0.4188\n",
      "batch: 1/17, train_dl_loss: 0.3580, train_bce_loss: 1.6668, train_bce_dl_loss: 0.3580, step time: 0.3829\n",
      "batch: 2/17, train_dl_loss: 0.2892, train_bce_loss: 1.6568, train_bce_dl_loss: 0.2892, step time: 0.4249\n",
      "batch: 3/17, train_dl_loss: 0.3904, train_bce_loss: 1.6794, train_bce_dl_loss: 0.3904, step time: 0.3808\n",
      "batch: 4/17, train_dl_loss: 0.2995, train_bce_loss: 1.6629, train_bce_dl_loss: 0.2995, step time: 0.4343\n",
      "batch: 5/17, train_dl_loss: 0.2708, train_bce_loss: 1.6536, train_bce_dl_loss: 0.2708, step time: 0.3855\n",
      "batch: 6/17, train_dl_loss: 0.3475, train_bce_loss: 1.6726, train_bce_dl_loss: 0.3475, step time: 0.4318\n",
      "batch: 7/17, train_dl_loss: 0.2515, train_bce_loss: 1.6459, train_bce_dl_loss: 0.2515, step time: 0.3866\n",
      "batch: 8/17, train_dl_loss: 0.2828, train_bce_loss: 1.6476, train_bce_dl_loss: 0.2828, step time: 0.4287\n",
      "batch: 9/17, train_dl_loss: 0.3107, train_bce_loss: 1.6696, train_bce_dl_loss: 0.3107, step time: 0.4195\n",
      "batch: 10/17, train_dl_loss: 0.3934, train_bce_loss: 1.6547, train_bce_dl_loss: 0.3934, step time: 0.4317\n",
      "batch: 11/17, train_dl_loss: 0.2832, train_bce_loss: 1.6469, train_bce_dl_loss: 0.2832, step time: 0.4331\n",
      "batch: 12/17, train_dl_loss: 0.3019, train_bce_loss: 1.6493, train_bce_dl_loss: 0.3019, step time: 0.4284\n",
      "batch: 13/17, train_dl_loss: 0.3984, train_bce_loss: 1.6773, train_bce_dl_loss: 0.3984, step time: 0.4262\n",
      "batch: 14/17, train_dl_loss: 0.2946, train_bce_loss: 1.6769, train_bce_dl_loss: 0.2946, step time: 0.4225\n",
      "batch: 15/17, train_dl_loss: 0.2700, train_bce_loss: 1.6563, train_bce_dl_loss: 0.2700, step time: 0.4292\n",
      "batch: 16/17, train_dl_loss: 0.2867, train_bce_loss: 1.6644, train_bce_dl_loss: 0.2867, step time: 0.4123\n",
      "batch: 17/17, train_dl_loss: 0.2876, train_bce_loss: 1.6505, train_bce_dl_loss: 0.2876, step time: 0.1139\n",
      "LOSS train DiceLoss: 0.3134, LOSS train BCE: 1.6606, LOSS train BCE-DiceLoss: 0.3134, LOSS val DiceLoss: 0.4297, LOSS val BCE: 1.6553, LOSS val BCE-DiceLoss: 0.4297, METRIC val: 0.5374\n",
      "time consuming of epoch 265 is: 458.9512\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3064, train_bce_loss: 1.6452, train_bce_dl_loss: 0.3064, step time: 0.4476\n",
      "batch: 1/17, train_dl_loss: 0.3054, train_bce_loss: 1.6651, train_bce_dl_loss: 0.3054, step time: 0.3986\n",
      "batch: 2/17, train_dl_loss: 0.2905, train_bce_loss: 1.6655, train_bce_dl_loss: 0.2905, step time: 0.4397\n",
      "batch: 3/17, train_dl_loss: 0.3513, train_bce_loss: 1.6592, train_bce_dl_loss: 0.3513, step time: 0.3784\n",
      "batch: 4/17, train_dl_loss: 0.2841, train_bce_loss: 1.6454, train_bce_dl_loss: 0.2841, step time: 0.4390\n",
      "batch: 5/17, train_dl_loss: 0.2669, train_bce_loss: 1.6660, train_bce_dl_loss: 0.2669, step time: 0.4514\n",
      "batch: 6/17, train_dl_loss: 0.3161, train_bce_loss: 1.6730, train_bce_dl_loss: 0.3161, step time: 0.4240\n",
      "batch: 7/17, train_dl_loss: 0.2790, train_bce_loss: 1.6390, train_bce_dl_loss: 0.2790, step time: 0.3841\n",
      "batch: 8/17, train_dl_loss: 0.2625, train_bce_loss: 1.6535, train_bce_dl_loss: 0.2625, step time: 0.4301\n",
      "batch: 9/17, train_dl_loss: 0.2705, train_bce_loss: 1.6659, train_bce_dl_loss: 0.2705, step time: 0.4504\n",
      "batch: 10/17, train_dl_loss: 0.4389, train_bce_loss: 1.6536, train_bce_dl_loss: 0.4389, step time: 0.4344\n",
      "batch: 11/17, train_dl_loss: 0.2925, train_bce_loss: 1.6545, train_bce_dl_loss: 0.2925, step time: 0.4255\n",
      "batch: 12/17, train_dl_loss: 0.3401, train_bce_loss: 1.6794, train_bce_dl_loss: 0.3401, step time: 0.4364\n",
      "batch: 13/17, train_dl_loss: 0.4110, train_bce_loss: 1.6676, train_bce_dl_loss: 0.4110, step time: 0.3776\n",
      "batch: 14/17, train_dl_loss: 0.2790, train_bce_loss: 1.6669, train_bce_dl_loss: 0.2790, step time: 0.4242\n",
      "batch: 15/17, train_dl_loss: 0.2787, train_bce_loss: 1.6675, train_bce_dl_loss: 0.2787, step time: 0.3910\n",
      "batch: 16/17, train_dl_loss: 0.2725, train_bce_loss: 1.6677, train_bce_dl_loss: 0.2725, step time: 0.4255\n",
      "batch: 17/17, train_dl_loss: 0.2303, train_bce_loss: 1.6505, train_bce_dl_loss: 0.2303, step time: 0.1143\n",
      "LOSS train DiceLoss: 0.3042, LOSS train BCE: 1.6603, LOSS train BCE-DiceLoss: 0.3042, LOSS val DiceLoss: 0.4290, LOSS val BCE: 1.6547, LOSS val BCE-DiceLoss: 0.4290, METRIC val: 0.5379\n",
      "time consuming of epoch 266 is: 445.6313\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2955, train_bce_loss: 1.6529, train_bce_dl_loss: 0.2955, step time: 0.4399\n",
      "batch: 1/17, train_dl_loss: 0.2910, train_bce_loss: 1.6604, train_bce_dl_loss: 0.2910, step time: 0.3803\n",
      "batch: 2/17, train_dl_loss: 0.2726, train_bce_loss: 1.6598, train_bce_dl_loss: 0.2726, step time: 0.4253\n",
      "batch: 3/17, train_dl_loss: 0.3509, train_bce_loss: 1.6650, train_bce_dl_loss: 0.3509, step time: 0.3740\n",
      "batch: 4/17, train_dl_loss: 0.2751, train_bce_loss: 1.6518, train_bce_dl_loss: 0.2751, step time: 0.4414\n",
      "batch: 5/17, train_dl_loss: 0.2939, train_bce_loss: 1.6514, train_bce_dl_loss: 0.2939, step time: 0.3805\n",
      "batch: 6/17, train_dl_loss: 0.3819, train_bce_loss: 1.6600, train_bce_dl_loss: 0.3819, step time: 0.4368\n",
      "batch: 7/17, train_dl_loss: 0.2763, train_bce_loss: 1.6659, train_bce_dl_loss: 0.2763, step time: 0.3817\n",
      "batch: 8/17, train_dl_loss: 0.3138, train_bce_loss: 1.6548, train_bce_dl_loss: 0.3138, step time: 0.4324\n",
      "batch: 9/17, train_dl_loss: 0.2976, train_bce_loss: 1.6683, train_bce_dl_loss: 0.2976, step time: 0.3874\n",
      "batch: 10/17, train_dl_loss: 0.3919, train_bce_loss: 1.6642, train_bce_dl_loss: 0.3919, step time: 0.4452\n",
      "batch: 11/17, train_dl_loss: 0.3176, train_bce_loss: 1.6543, train_bce_dl_loss: 0.3176, step time: 0.4491\n",
      "batch: 12/17, train_dl_loss: 0.2677, train_bce_loss: 1.6572, train_bce_dl_loss: 0.2677, step time: 0.4155\n",
      "batch: 13/17, train_dl_loss: 0.3657, train_bce_loss: 1.6735, train_bce_dl_loss: 0.3657, step time: 0.3913\n",
      "batch: 14/17, train_dl_loss: 0.3411, train_bce_loss: 1.6672, train_bce_dl_loss: 0.3411, step time: 0.4251\n",
      "batch: 15/17, train_dl_loss: 0.2924, train_bce_loss: 1.6675, train_bce_dl_loss: 0.2924, step time: 0.3884\n",
      "batch: 16/17, train_dl_loss: 0.2768, train_bce_loss: 1.6601, train_bce_dl_loss: 0.2768, step time: 0.4085\n",
      "batch: 17/17, train_dl_loss: 0.2879, train_bce_loss: 1.6498, train_bce_dl_loss: 0.2879, step time: 0.1106\n",
      "LOSS train DiceLoss: 0.3105, LOSS train BCE: 1.6602, LOSS train BCE-DiceLoss: 0.3105, LOSS val DiceLoss: 0.4290, LOSS val BCE: 1.6547, LOSS val BCE-DiceLoss: 0.4290, METRIC val: 0.5365\n",
      "time consuming of epoch 267 is: 463.3872\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3529, train_bce_loss: 1.6609, train_bce_dl_loss: 0.3529, step time: 0.4300\n",
      "batch: 1/17, train_dl_loss: 0.2638, train_bce_loss: 1.6643, train_bce_dl_loss: 0.2638, step time: 0.3710\n",
      "batch: 2/17, train_dl_loss: 0.2915, train_bce_loss: 1.6583, train_bce_dl_loss: 0.2915, step time: 0.4456\n",
      "batch: 3/17, train_dl_loss: 0.3617, train_bce_loss: 1.6647, train_bce_dl_loss: 0.3617, step time: 0.3737\n",
      "batch: 4/17, train_dl_loss: 0.2735, train_bce_loss: 1.6475, train_bce_dl_loss: 0.2735, step time: 0.4228\n",
      "batch: 5/17, train_dl_loss: 0.2801, train_bce_loss: 1.6556, train_bce_dl_loss: 0.2801, step time: 0.3898\n",
      "batch: 6/17, train_dl_loss: 0.3363, train_bce_loss: 1.6652, train_bce_dl_loss: 0.3363, step time: 0.4328\n",
      "batch: 7/17, train_dl_loss: 0.2814, train_bce_loss: 1.6635, train_bce_dl_loss: 0.2814, step time: 0.3901\n",
      "batch: 8/17, train_dl_loss: 0.2685, train_bce_loss: 1.6529, train_bce_dl_loss: 0.2685, step time: 0.4349\n",
      "batch: 9/17, train_dl_loss: 0.2881, train_bce_loss: 1.6599, train_bce_dl_loss: 0.2881, step time: 0.3862\n",
      "batch: 10/17, train_dl_loss: 0.4087, train_bce_loss: 1.6707, train_bce_dl_loss: 0.4087, step time: 0.4268\n",
      "batch: 11/17, train_dl_loss: 0.2952, train_bce_loss: 1.6319, train_bce_dl_loss: 0.2952, step time: 0.3825\n",
      "batch: 12/17, train_dl_loss: 0.3170, train_bce_loss: 1.6489, train_bce_dl_loss: 0.3170, step time: 0.4391\n",
      "batch: 13/17, train_dl_loss: 0.4205, train_bce_loss: 1.6748, train_bce_dl_loss: 0.4205, step time: 0.3828\n",
      "batch: 14/17, train_dl_loss: 0.3052, train_bce_loss: 1.6717, train_bce_dl_loss: 0.3052, step time: 0.4211\n",
      "batch: 15/17, train_dl_loss: 0.3061, train_bce_loss: 1.6584, train_bce_dl_loss: 0.3061, step time: 0.3838\n",
      "batch: 16/17, train_dl_loss: 0.2660, train_bce_loss: 1.6662, train_bce_dl_loss: 0.2660, step time: 0.4246\n",
      "batch: 17/17, train_dl_loss: 0.2371, train_bce_loss: 1.6676, train_bce_dl_loss: 0.2371, step time: 0.1102\n",
      "LOSS train DiceLoss: 0.3086, LOSS train BCE: 1.6602, LOSS train BCE-DiceLoss: 0.3086, LOSS val DiceLoss: 0.4283, LOSS val BCE: 1.6554, LOSS val BCE-DiceLoss: 0.4283, METRIC val: 0.5372\n",
      "time consuming of epoch 268 is: 451.8573\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2749, train_bce_loss: 1.6543, train_bce_dl_loss: 0.2749, step time: 0.4276\n",
      "batch: 1/17, train_dl_loss: 0.2849, train_bce_loss: 1.6543, train_bce_dl_loss: 0.2849, step time: 0.3744\n",
      "batch: 2/17, train_dl_loss: 0.3031, train_bce_loss: 1.6610, train_bce_dl_loss: 0.3031, step time: 0.4132\n",
      "batch: 3/17, train_dl_loss: 0.3637, train_bce_loss: 1.6751, train_bce_dl_loss: 0.3637, step time: 0.3853\n",
      "batch: 4/17, train_dl_loss: 0.2781, train_bce_loss: 1.6621, train_bce_dl_loss: 0.2781, step time: 0.4545\n",
      "batch: 5/17, train_dl_loss: 0.2721, train_bce_loss: 1.6564, train_bce_dl_loss: 0.2721, step time: 0.4406\n",
      "batch: 6/17, train_dl_loss: 0.3568, train_bce_loss: 1.6656, train_bce_dl_loss: 0.3568, step time: 0.4381\n",
      "batch: 7/17, train_dl_loss: 0.2883, train_bce_loss: 1.6509, train_bce_dl_loss: 0.2883, step time: 0.4306\n",
      "batch: 8/17, train_dl_loss: 0.2968, train_bce_loss: 1.6395, train_bce_dl_loss: 0.2968, step time: 0.4245\n",
      "batch: 9/17, train_dl_loss: 0.2829, train_bce_loss: 1.6586, train_bce_dl_loss: 0.2829, step time: 0.4247\n",
      "batch: 10/17, train_dl_loss: 0.3677, train_bce_loss: 1.6491, train_bce_dl_loss: 0.3677, step time: 0.4344\n",
      "batch: 11/17, train_dl_loss: 0.2844, train_bce_loss: 1.6522, train_bce_dl_loss: 0.2844, step time: 0.4375\n",
      "batch: 12/17, train_dl_loss: 0.2724, train_bce_loss: 1.6455, train_bce_dl_loss: 0.2724, step time: 0.4268\n",
      "batch: 13/17, train_dl_loss: 0.3767, train_bce_loss: 1.6675, train_bce_dl_loss: 0.3767, step time: 0.4277\n",
      "batch: 14/17, train_dl_loss: 0.3196, train_bce_loss: 1.6675, train_bce_dl_loss: 0.3196, step time: 0.4250\n",
      "batch: 15/17, train_dl_loss: 0.2711, train_bce_loss: 1.6599, train_bce_dl_loss: 0.2711, step time: 0.4187\n",
      "batch: 16/17, train_dl_loss: 0.2751, train_bce_loss: 1.6742, train_bce_dl_loss: 0.2751, step time: 0.4132\n",
      "batch: 17/17, train_dl_loss: 0.2160, train_bce_loss: 1.6703, train_bce_dl_loss: 0.2160, step time: 0.1110\n",
      "LOSS train DiceLoss: 0.2991, LOSS train BCE: 1.6591, LOSS train BCE-DiceLoss: 0.2991, LOSS val DiceLoss: 0.4295, LOSS val BCE: 1.6560, LOSS val BCE-DiceLoss: 0.4295, METRIC val: 0.5370\n",
      "time consuming of epoch 269 is: 379.2642\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3302, train_bce_loss: 1.6503, train_bce_dl_loss: 0.3302, step time: 0.4209\n",
      "batch: 1/17, train_dl_loss: 0.2966, train_bce_loss: 1.6532, train_bce_dl_loss: 0.2966, step time: 0.3804\n",
      "batch: 2/17, train_dl_loss: 0.3203, train_bce_loss: 1.6590, train_bce_dl_loss: 0.3203, step time: 0.4309\n",
      "batch: 3/17, train_dl_loss: 0.3608, train_bce_loss: 1.6451, train_bce_dl_loss: 0.3608, step time: 0.3816\n",
      "batch: 4/17, train_dl_loss: 0.2745, train_bce_loss: 1.6420, train_bce_dl_loss: 0.2745, step time: 0.4217\n",
      "batch: 5/17, train_dl_loss: 0.2613, train_bce_loss: 1.6648, train_bce_dl_loss: 0.2613, step time: 0.4247\n",
      "batch: 6/17, train_dl_loss: 0.3517, train_bce_loss: 1.6686, train_bce_dl_loss: 0.3517, step time: 0.4460\n",
      "batch: 7/17, train_dl_loss: 0.2903, train_bce_loss: 1.6557, train_bce_dl_loss: 0.2903, step time: 0.4288\n",
      "batch: 8/17, train_dl_loss: 0.2697, train_bce_loss: 1.6641, train_bce_dl_loss: 0.2697, step time: 0.4255\n",
      "batch: 9/17, train_dl_loss: 0.2671, train_bce_loss: 1.6735, train_bce_dl_loss: 0.2671, step time: 0.4291\n",
      "batch: 10/17, train_dl_loss: 0.3789, train_bce_loss: 1.6570, train_bce_dl_loss: 0.3789, step time: 0.4233\n",
      "batch: 11/17, train_dl_loss: 0.2662, train_bce_loss: 1.6603, train_bce_dl_loss: 0.2662, step time: 0.4296\n",
      "batch: 12/17, train_dl_loss: 0.2556, train_bce_loss: 1.6589, train_bce_dl_loss: 0.2556, step time: 0.4417\n",
      "batch: 13/17, train_dl_loss: 0.3715, train_bce_loss: 1.6793, train_bce_dl_loss: 0.3715, step time: 0.4081\n",
      "batch: 14/17, train_dl_loss: 0.3174, train_bce_loss: 1.6700, train_bce_dl_loss: 0.3174, step time: 0.4330\n",
      "batch: 15/17, train_dl_loss: 0.2775, train_bce_loss: 1.6641, train_bce_dl_loss: 0.2775, step time: 0.3795\n",
      "batch: 16/17, train_dl_loss: 0.2845, train_bce_loss: 1.6636, train_bce_dl_loss: 0.2845, step time: 0.4162\n",
      "batch: 17/17, train_dl_loss: 0.2304, train_bce_loss: 1.6551, train_bce_dl_loss: 0.2304, step time: 0.1134\n",
      "LOSS train DiceLoss: 0.3002, LOSS train BCE: 1.6602, LOSS train BCE-DiceLoss: 0.3002, LOSS val DiceLoss: 0.4342, LOSS val BCE: 1.6578, LOSS val BCE-DiceLoss: 0.4342, METRIC val: 0.5320\n",
      "time consuming of epoch 270 is: 424.1636\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2776, train_bce_loss: 1.6499, train_bce_dl_loss: 0.2776, step time: 0.4341\n",
      "batch: 1/17, train_dl_loss: 0.3312, train_bce_loss: 1.6580, train_bce_dl_loss: 0.3312, step time: 0.4300\n",
      "batch: 2/17, train_dl_loss: 0.2820, train_bce_loss: 1.6529, train_bce_dl_loss: 0.2820, step time: 0.4247\n",
      "batch: 3/17, train_dl_loss: 0.3482, train_bce_loss: 1.6715, train_bce_dl_loss: 0.3482, step time: 0.3808\n",
      "batch: 4/17, train_dl_loss: 0.2837, train_bce_loss: 1.6470, train_bce_dl_loss: 0.2837, step time: 0.4452\n",
      "batch: 5/17, train_dl_loss: 0.3073, train_bce_loss: 1.6546, train_bce_dl_loss: 0.3073, step time: 0.4014\n",
      "batch: 6/17, train_dl_loss: 0.3672, train_bce_loss: 1.6796, train_bce_dl_loss: 0.3672, step time: 0.4198\n",
      "batch: 7/17, train_dl_loss: 0.2742, train_bce_loss: 1.6562, train_bce_dl_loss: 0.2742, step time: 0.3762\n",
      "batch: 8/17, train_dl_loss: 0.2885, train_bce_loss: 1.6519, train_bce_dl_loss: 0.2885, step time: 0.4303\n",
      "batch: 9/17, train_dl_loss: 0.3010, train_bce_loss: 1.6720, train_bce_dl_loss: 0.3010, step time: 0.3763\n",
      "batch: 10/17, train_dl_loss: 0.3719, train_bce_loss: 1.6488, train_bce_dl_loss: 0.3719, step time: 0.4292\n",
      "batch: 11/17, train_dl_loss: 0.2896, train_bce_loss: 1.6388, train_bce_dl_loss: 0.2896, step time: 0.3776\n",
      "batch: 12/17, train_dl_loss: 0.2687, train_bce_loss: 1.6525, train_bce_dl_loss: 0.2687, step time: 0.4202\n",
      "batch: 13/17, train_dl_loss: 0.3819, train_bce_loss: 1.6682, train_bce_dl_loss: 0.3819, step time: 0.3773\n",
      "batch: 14/17, train_dl_loss: 0.2900, train_bce_loss: 1.6822, train_bce_dl_loss: 0.2900, step time: 0.4265\n",
      "batch: 15/17, train_dl_loss: 0.2724, train_bce_loss: 1.6633, train_bce_dl_loss: 0.2724, step time: 0.3797\n",
      "batch: 16/17, train_dl_loss: 0.2742, train_bce_loss: 1.6707, train_bce_dl_loss: 0.2742, step time: 0.4122\n",
      "batch: 17/17, train_dl_loss: 0.2379, train_bce_loss: 1.6735, train_bce_dl_loss: 0.2379, step time: 0.1115\n",
      "LOSS train DiceLoss: 0.3026, LOSS train BCE: 1.6606, LOSS train BCE-DiceLoss: 0.3026, LOSS val DiceLoss: 0.4328, LOSS val BCE: 1.6602, LOSS val BCE-DiceLoss: 0.4328, METRIC val: 0.5334\n",
      "time consuming of epoch 271 is: 422.5454\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2907, train_bce_loss: 1.6502, train_bce_dl_loss: 0.2907, step time: 0.4254\n",
      "batch: 1/17, train_dl_loss: 0.2696, train_bce_loss: 1.6786, train_bce_dl_loss: 0.2696, step time: 0.3812\n",
      "batch: 2/17, train_dl_loss: 0.3214, train_bce_loss: 1.6666, train_bce_dl_loss: 0.3214, step time: 0.4249\n",
      "batch: 3/17, train_dl_loss: 0.3722, train_bce_loss: 1.6778, train_bce_dl_loss: 0.3722, step time: 0.3781\n",
      "batch: 4/17, train_dl_loss: 0.2930, train_bce_loss: 1.6709, train_bce_dl_loss: 0.2930, step time: 0.4345\n",
      "batch: 5/17, train_dl_loss: 0.3068, train_bce_loss: 1.6617, train_bce_dl_loss: 0.3068, step time: 0.4267\n",
      "batch: 6/17, train_dl_loss: 0.3513, train_bce_loss: 1.6696, train_bce_dl_loss: 0.3513, step time: 0.4282\n",
      "batch: 7/17, train_dl_loss: 0.2684, train_bce_loss: 1.6594, train_bce_dl_loss: 0.2684, step time: 0.4467\n",
      "batch: 8/17, train_dl_loss: 0.2673, train_bce_loss: 1.6605, train_bce_dl_loss: 0.2673, step time: 0.4191\n",
      "batch: 9/17, train_dl_loss: 0.3969, train_bce_loss: 1.6759, train_bce_dl_loss: 0.3969, step time: 0.4433\n",
      "batch: 10/17, train_dl_loss: 0.3645, train_bce_loss: 1.6549, train_bce_dl_loss: 0.3645, step time: 0.4329\n",
      "batch: 11/17, train_dl_loss: 0.2620, train_bce_loss: 1.6560, train_bce_dl_loss: 0.2620, step time: 0.4375\n",
      "batch: 12/17, train_dl_loss: 0.2811, train_bce_loss: 1.6572, train_bce_dl_loss: 0.2811, step time: 0.4611\n",
      "batch: 13/17, train_dl_loss: 0.3920, train_bce_loss: 1.6757, train_bce_dl_loss: 0.3920, step time: 0.4525\n",
      "batch: 14/17, train_dl_loss: 0.3271, train_bce_loss: 1.6680, train_bce_dl_loss: 0.3271, step time: 0.4358\n",
      "batch: 15/17, train_dl_loss: 0.2852, train_bce_loss: 1.6612, train_bce_dl_loss: 0.2852, step time: 0.4328\n",
      "batch: 16/17, train_dl_loss: 0.3171, train_bce_loss: 1.6779, train_bce_dl_loss: 0.3171, step time: 0.4211\n",
      "batch: 17/17, train_dl_loss: 0.2341, train_bce_loss: 1.6693, train_bce_dl_loss: 0.2341, step time: 0.1117\n",
      "LOSS train DiceLoss: 0.3111, LOSS train BCE: 1.6662, LOSS train BCE-DiceLoss: 0.3111, LOSS val DiceLoss: 0.4320, LOSS val BCE: 1.6596, LOSS val BCE-DiceLoss: 0.4320, METRIC val: 0.5343\n",
      "time consuming of epoch 272 is: 395.6983\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2855, train_bce_loss: 1.6568, train_bce_dl_loss: 0.2855, step time: 0.4300\n",
      "batch: 1/17, train_dl_loss: 0.2773, train_bce_loss: 1.6646, train_bce_dl_loss: 0.2773, step time: 0.3838\n",
      "batch: 2/17, train_dl_loss: 0.2908, train_bce_loss: 1.6636, train_bce_dl_loss: 0.2908, step time: 0.4372\n",
      "batch: 3/17, train_dl_loss: 0.4358, train_bce_loss: 1.6582, train_bce_dl_loss: 0.4358, step time: 0.3866\n",
      "batch: 4/17, train_dl_loss: 0.2619, train_bce_loss: 1.6666, train_bce_dl_loss: 0.2619, step time: 0.4144\n",
      "batch: 5/17, train_dl_loss: 0.2748, train_bce_loss: 1.6707, train_bce_dl_loss: 0.2748, step time: 0.4369\n",
      "batch: 6/17, train_dl_loss: 0.3754, train_bce_loss: 1.6703, train_bce_dl_loss: 0.3754, step time: 0.4257\n",
      "batch: 7/17, train_dl_loss: 0.2769, train_bce_loss: 1.6472, train_bce_dl_loss: 0.2769, step time: 0.3859\n",
      "batch: 8/17, train_dl_loss: 0.2790, train_bce_loss: 1.6504, train_bce_dl_loss: 0.2790, step time: 0.4258\n",
      "batch: 9/17, train_dl_loss: 0.2818, train_bce_loss: 1.6615, train_bce_dl_loss: 0.2818, step time: 0.4359\n",
      "batch: 10/17, train_dl_loss: 0.3900, train_bce_loss: 1.6506, train_bce_dl_loss: 0.3900, step time: 0.4313\n",
      "batch: 11/17, train_dl_loss: 0.2655, train_bce_loss: 1.6563, train_bce_dl_loss: 0.2655, step time: 0.4416\n",
      "batch: 12/17, train_dl_loss: 0.2629, train_bce_loss: 1.6455, train_bce_dl_loss: 0.2629, step time: 0.4208\n",
      "batch: 13/17, train_dl_loss: 0.3844, train_bce_loss: 1.6739, train_bce_dl_loss: 0.3844, step time: 0.3759\n",
      "batch: 14/17, train_dl_loss: 0.3273, train_bce_loss: 1.6784, train_bce_dl_loss: 0.3273, step time: 0.4345\n",
      "batch: 15/17, train_dl_loss: 0.2965, train_bce_loss: 1.6789, train_bce_dl_loss: 0.2965, step time: 0.3862\n",
      "batch: 16/17, train_dl_loss: 0.2693, train_bce_loss: 1.6668, train_bce_dl_loss: 0.2693, step time: 0.4254\n",
      "batch: 17/17, train_dl_loss: 0.2279, train_bce_loss: 1.6562, train_bce_dl_loss: 0.2279, step time: 0.1137\n",
      "LOSS train DiceLoss: 0.3035, LOSS train BCE: 1.6620, LOSS train BCE-DiceLoss: 0.3035, LOSS val DiceLoss: 0.4326, LOSS val BCE: 1.6598, LOSS val BCE-DiceLoss: 0.4326, METRIC val: 0.5334\n",
      "time consuming of epoch 273 is: 422.3210\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2925, train_bce_loss: 1.6665, train_bce_dl_loss: 0.2925, step time: 0.4241\n",
      "batch: 1/17, train_dl_loss: 0.2958, train_bce_loss: 1.6693, train_bce_dl_loss: 0.2958, step time: 0.3752\n",
      "batch: 2/17, train_dl_loss: 0.3080, train_bce_loss: 1.6674, train_bce_dl_loss: 0.3080, step time: 0.4193\n",
      "batch: 3/17, train_dl_loss: 0.3786, train_bce_loss: 1.6760, train_bce_dl_loss: 0.3786, step time: 0.3836\n",
      "batch: 4/17, train_dl_loss: 0.2808, train_bce_loss: 1.6570, train_bce_dl_loss: 0.2808, step time: 0.4264\n",
      "batch: 5/17, train_dl_loss: 0.2796, train_bce_loss: 1.6575, train_bce_dl_loss: 0.2796, step time: 0.3785\n",
      "batch: 6/17, train_dl_loss: 0.3378, train_bce_loss: 1.6620, train_bce_dl_loss: 0.3378, step time: 0.4141\n",
      "batch: 7/17, train_dl_loss: 0.2772, train_bce_loss: 1.6588, train_bce_dl_loss: 0.2772, step time: 0.3726\n",
      "batch: 8/17, train_dl_loss: 0.2868, train_bce_loss: 1.6601, train_bce_dl_loss: 0.2868, step time: 0.4308\n",
      "batch: 9/17, train_dl_loss: 0.2862, train_bce_loss: 1.6634, train_bce_dl_loss: 0.2862, step time: 0.3833\n",
      "batch: 10/17, train_dl_loss: 0.3744, train_bce_loss: 1.6539, train_bce_dl_loss: 0.3744, step time: 0.4315\n",
      "batch: 11/17, train_dl_loss: 0.2683, train_bce_loss: 1.6552, train_bce_dl_loss: 0.2683, step time: 0.4414\n",
      "batch: 12/17, train_dl_loss: 0.2741, train_bce_loss: 1.6532, train_bce_dl_loss: 0.2741, step time: 0.4204\n",
      "batch: 13/17, train_dl_loss: 0.4280, train_bce_loss: 1.6769, train_bce_dl_loss: 0.4280, step time: 0.3815\n",
      "batch: 14/17, train_dl_loss: 0.3215, train_bce_loss: 1.6813, train_bce_dl_loss: 0.3215, step time: 0.4301\n",
      "batch: 15/17, train_dl_loss: 0.2781, train_bce_loss: 1.6706, train_bce_dl_loss: 0.2781, step time: 0.3763\n",
      "batch: 16/17, train_dl_loss: 0.2736, train_bce_loss: 1.6655, train_bce_dl_loss: 0.2736, step time: 0.4164\n",
      "batch: 17/17, train_dl_loss: 0.2320, train_bce_loss: 1.6534, train_bce_dl_loss: 0.2320, step time: 0.1131\n",
      "LOSS train DiceLoss: 0.3041, LOSS train BCE: 1.6638, LOSS train BCE-DiceLoss: 0.3041, LOSS val DiceLoss: 0.4333, LOSS val BCE: 1.6570, LOSS val BCE-DiceLoss: 0.4333, METRIC val: 0.5332\n",
      "time consuming of epoch 274 is: 547.1338\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2781, train_bce_loss: 1.6574, train_bce_dl_loss: 0.2781, step time: 0.4290\n",
      "batch: 1/17, train_dl_loss: 0.2761, train_bce_loss: 1.6597, train_bce_dl_loss: 0.2761, step time: 0.3723\n",
      "batch: 2/17, train_dl_loss: 0.3278, train_bce_loss: 1.6556, train_bce_dl_loss: 0.3278, step time: 0.4177\n",
      "batch: 3/17, train_dl_loss: 0.3244, train_bce_loss: 1.6707, train_bce_dl_loss: 0.3244, step time: 0.3762\n",
      "batch: 4/17, train_dl_loss: 0.2520, train_bce_loss: 1.6622, train_bce_dl_loss: 0.2520, step time: 0.4747\n",
      "batch: 5/17, train_dl_loss: 0.2879, train_bce_loss: 1.6512, train_bce_dl_loss: 0.2879, step time: 0.4272\n",
      "batch: 6/17, train_dl_loss: 0.3281, train_bce_loss: 1.6725, train_bce_dl_loss: 0.3281, step time: 0.4409\n",
      "batch: 7/17, train_dl_loss: 0.2917, train_bce_loss: 1.6511, train_bce_dl_loss: 0.2917, step time: 0.4367\n",
      "batch: 8/17, train_dl_loss: 0.2955, train_bce_loss: 1.6539, train_bce_dl_loss: 0.2955, step time: 0.4327\n",
      "batch: 9/17, train_dl_loss: 0.2933, train_bce_loss: 1.6719, train_bce_dl_loss: 0.2933, step time: 0.4260\n",
      "batch: 10/17, train_dl_loss: 0.3879, train_bce_loss: 1.6666, train_bce_dl_loss: 0.3879, step time: 0.4239\n",
      "batch: 11/17, train_dl_loss: 0.2653, train_bce_loss: 1.6566, train_bce_dl_loss: 0.2653, step time: 0.5026\n",
      "batch: 12/17, train_dl_loss: 0.3190, train_bce_loss: 1.6524, train_bce_dl_loss: 0.3190, step time: 0.4458\n",
      "batch: 13/17, train_dl_loss: 0.3875, train_bce_loss: 1.6817, train_bce_dl_loss: 0.3875, step time: 0.4312\n",
      "batch: 14/17, train_dl_loss: 0.3090, train_bce_loss: 1.6738, train_bce_dl_loss: 0.3090, step time: 0.4272\n",
      "batch: 15/17, train_dl_loss: 0.3037, train_bce_loss: 1.6606, train_bce_dl_loss: 0.3037, step time: 0.4400\n",
      "batch: 16/17, train_dl_loss: 0.2778, train_bce_loss: 1.6628, train_bce_dl_loss: 0.2778, step time: 0.4148\n",
      "batch: 17/17, train_dl_loss: 0.2965, train_bce_loss: 1.6554, train_bce_dl_loss: 0.2965, step time: 0.1122\n",
      "LOSS train DiceLoss: 0.3057, LOSS train BCE: 1.6620, LOSS train BCE-DiceLoss: 0.3057, LOSS val DiceLoss: 0.4386, LOSS val BCE: 1.6587, LOSS val BCE-DiceLoss: 0.4386, METRIC val: 0.5287\n",
      "time consuming of epoch 275 is: 577.2781\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2983, train_bce_loss: 1.6490, train_bce_dl_loss: 0.2983, step time: 0.4381\n",
      "batch: 1/17, train_dl_loss: 0.3227, train_bce_loss: 1.6644, train_bce_dl_loss: 0.3227, step time: 0.3857\n",
      "batch: 2/17, train_dl_loss: 0.2730, train_bce_loss: 1.6635, train_bce_dl_loss: 0.2730, step time: 0.4258\n",
      "batch: 3/17, train_dl_loss: 0.3477, train_bce_loss: 1.6707, train_bce_dl_loss: 0.3477, step time: 0.3933\n",
      "batch: 4/17, train_dl_loss: 0.2720, train_bce_loss: 1.6595, train_bce_dl_loss: 0.2720, step time: 0.4372\n",
      "batch: 5/17, train_dl_loss: 0.2936, train_bce_loss: 1.6519, train_bce_dl_loss: 0.2936, step time: 0.3882\n",
      "batch: 6/17, train_dl_loss: 0.3377, train_bce_loss: 1.6792, train_bce_dl_loss: 0.3377, step time: 0.4162\n",
      "batch: 7/17, train_dl_loss: 0.2625, train_bce_loss: 1.6563, train_bce_dl_loss: 0.2625, step time: 0.3850\n",
      "batch: 8/17, train_dl_loss: 0.2987, train_bce_loss: 1.6447, train_bce_dl_loss: 0.2987, step time: 0.4330\n",
      "batch: 9/17, train_dl_loss: 0.3027, train_bce_loss: 1.6699, train_bce_dl_loss: 0.3027, step time: 0.3833\n",
      "batch: 10/17, train_dl_loss: 0.4109, train_bce_loss: 1.6665, train_bce_dl_loss: 0.4109, step time: 0.4356\n",
      "batch: 11/17, train_dl_loss: 0.3014, train_bce_loss: 1.6426, train_bce_dl_loss: 0.3014, step time: 0.4310\n",
      "batch: 12/17, train_dl_loss: 0.3081, train_bce_loss: 1.6598, train_bce_dl_loss: 0.3081, step time: 0.4232\n",
      "batch: 13/17, train_dl_loss: 0.4106, train_bce_loss: 1.6653, train_bce_dl_loss: 0.4106, step time: 0.3783\n",
      "batch: 14/17, train_dl_loss: 0.3168, train_bce_loss: 1.6705, train_bce_dl_loss: 0.3168, step time: 0.4378\n",
      "batch: 15/17, train_dl_loss: 0.2930, train_bce_loss: 1.6690, train_bce_dl_loss: 0.2930, step time: 0.3832\n",
      "batch: 16/17, train_dl_loss: 0.2747, train_bce_loss: 1.6688, train_bce_dl_loss: 0.2747, step time: 0.4184\n",
      "batch: 17/17, train_dl_loss: 0.2338, train_bce_loss: 1.6551, train_bce_dl_loss: 0.2338, step time: 0.1161\n",
      "LOSS train DiceLoss: 0.3088, LOSS train BCE: 1.6615, LOSS train BCE-DiceLoss: 0.3088, LOSS val DiceLoss: 0.4380, LOSS val BCE: 1.6608, LOSS val BCE-DiceLoss: 0.4380, METRIC val: 0.5298\n",
      "time consuming of epoch 276 is: 488.5823\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2707, train_bce_loss: 1.6644, train_bce_dl_loss: 0.2707, step time: 0.4327\n",
      "batch: 1/17, train_dl_loss: 0.3546, train_bce_loss: 1.6677, train_bce_dl_loss: 0.3546, step time: 0.3848\n",
      "batch: 2/17, train_dl_loss: 0.3161, train_bce_loss: 1.6739, train_bce_dl_loss: 0.3161, step time: 0.4251\n",
      "batch: 3/17, train_dl_loss: 0.3811, train_bce_loss: 1.6640, train_bce_dl_loss: 0.3811, step time: 0.3823\n",
      "batch: 4/17, train_dl_loss: 0.2581, train_bce_loss: 1.6547, train_bce_dl_loss: 0.2581, step time: 0.4200\n",
      "batch: 5/17, train_dl_loss: 0.2813, train_bce_loss: 1.6738, train_bce_dl_loss: 0.2813, step time: 0.3857\n",
      "batch: 6/17, train_dl_loss: 0.3583, train_bce_loss: 1.6760, train_bce_dl_loss: 0.3583, step time: 0.4295\n",
      "batch: 7/17, train_dl_loss: 0.3071, train_bce_loss: 1.6509, train_bce_dl_loss: 0.3071, step time: 0.3814\n",
      "batch: 8/17, train_dl_loss: 0.2869, train_bce_loss: 1.6521, train_bce_dl_loss: 0.2869, step time: 0.4434\n",
      "batch: 9/17, train_dl_loss: 0.2821, train_bce_loss: 1.6654, train_bce_dl_loss: 0.2821, step time: 0.3783\n",
      "batch: 10/17, train_dl_loss: 0.4019, train_bce_loss: 1.6504, train_bce_dl_loss: 0.4019, step time: 0.4294\n",
      "batch: 11/17, train_dl_loss: 0.2912, train_bce_loss: 1.6447, train_bce_dl_loss: 0.2912, step time: 0.3832\n",
      "batch: 12/17, train_dl_loss: 0.2828, train_bce_loss: 1.6529, train_bce_dl_loss: 0.2828, step time: 0.4348\n",
      "batch: 13/17, train_dl_loss: 0.4234, train_bce_loss: 1.6733, train_bce_dl_loss: 0.4234, step time: 0.3820\n",
      "batch: 14/17, train_dl_loss: 0.2926, train_bce_loss: 1.6771, train_bce_dl_loss: 0.2926, step time: 0.4289\n",
      "batch: 15/17, train_dl_loss: 0.2835, train_bce_loss: 1.6706, train_bce_dl_loss: 0.2835, step time: 0.4102\n",
      "batch: 16/17, train_dl_loss: 0.3078, train_bce_loss: 1.6780, train_bce_dl_loss: 0.3078, step time: 0.4176\n",
      "batch: 17/17, train_dl_loss: 0.2341, train_bce_loss: 1.6580, train_bce_dl_loss: 0.2341, step time: 0.1112\n",
      "LOSS train DiceLoss: 0.3119, LOSS train BCE: 1.6638, LOSS train BCE-DiceLoss: 0.3119, LOSS val DiceLoss: 0.4471, LOSS val BCE: 1.6618, LOSS val BCE-DiceLoss: 0.4471, METRIC val: 0.5215\n",
      "time consuming of epoch 277 is: 473.7866\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3081, train_bce_loss: 1.6499, train_bce_dl_loss: 0.3081, step time: 0.4212\n",
      "batch: 1/17, train_dl_loss: 0.2979, train_bce_loss: 1.6815, train_bce_dl_loss: 0.2979, step time: 0.3811\n",
      "batch: 2/17, train_dl_loss: 0.2944, train_bce_loss: 1.6534, train_bce_dl_loss: 0.2944, step time: 0.4195\n",
      "batch: 3/17, train_dl_loss: 0.3951, train_bce_loss: 1.6763, train_bce_dl_loss: 0.3951, step time: 0.4463\n",
      "batch: 4/17, train_dl_loss: 0.3062, train_bce_loss: 1.6528, train_bce_dl_loss: 0.3062, step time: 0.4389\n",
      "batch: 5/17, train_dl_loss: 0.2889, train_bce_loss: 1.6564, train_bce_dl_loss: 0.2889, step time: 0.3833\n",
      "batch: 6/17, train_dl_loss: 0.3737, train_bce_loss: 1.6702, train_bce_dl_loss: 0.3737, step time: 0.4366\n",
      "batch: 7/17, train_dl_loss: 0.2553, train_bce_loss: 1.6515, train_bce_dl_loss: 0.2553, step time: 0.4345\n",
      "batch: 8/17, train_dl_loss: 0.2937, train_bce_loss: 1.6539, train_bce_dl_loss: 0.2937, step time: 0.4214\n",
      "batch: 9/17, train_dl_loss: 0.2734, train_bce_loss: 1.6709, train_bce_dl_loss: 0.2734, step time: 0.4492\n",
      "batch: 10/17, train_dl_loss: 0.3867, train_bce_loss: 1.6699, train_bce_dl_loss: 0.3867, step time: 0.4292\n",
      "batch: 11/17, train_dl_loss: 0.2861, train_bce_loss: 1.6610, train_bce_dl_loss: 0.2861, step time: 0.4302\n",
      "batch: 12/17, train_dl_loss: 0.3207, train_bce_loss: 1.6578, train_bce_dl_loss: 0.3207, step time: 0.4410\n",
      "batch: 13/17, train_dl_loss: 0.4290, train_bce_loss: 1.6732, train_bce_dl_loss: 0.4290, step time: 0.4249\n",
      "batch: 14/17, train_dl_loss: 0.2894, train_bce_loss: 1.6791, train_bce_dl_loss: 0.2894, step time: 0.4389\n",
      "batch: 15/17, train_dl_loss: 0.2650, train_bce_loss: 1.6661, train_bce_dl_loss: 0.2650, step time: 0.3841\n",
      "batch: 16/17, train_dl_loss: 0.2977, train_bce_loss: 1.6593, train_bce_dl_loss: 0.2977, step time: 0.4201\n",
      "batch: 17/17, train_dl_loss: 0.2359, train_bce_loss: 1.6736, train_bce_dl_loss: 0.2359, step time: 0.1116\n",
      "LOSS train DiceLoss: 0.3110, LOSS train BCE: 1.6643, LOSS train BCE-DiceLoss: 0.3110, LOSS val DiceLoss: 0.4332, LOSS val BCE: 1.6583, LOSS val BCE-DiceLoss: 0.4332, METRIC val: 0.5347\n",
      "time consuming of epoch 278 is: 424.2363\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2853, train_bce_loss: 1.6489, train_bce_dl_loss: 0.2853, step time: 0.4222\n",
      "batch: 1/17, train_dl_loss: 0.2884, train_bce_loss: 1.6628, train_bce_dl_loss: 0.2884, step time: 0.3770\n",
      "batch: 2/17, train_dl_loss: 0.2945, train_bce_loss: 1.6587, train_bce_dl_loss: 0.2945, step time: 0.4277\n",
      "batch: 3/17, train_dl_loss: 0.3690, train_bce_loss: 1.6561, train_bce_dl_loss: 0.3690, step time: 0.3790\n",
      "batch: 4/17, train_dl_loss: 0.3177, train_bce_loss: 1.6721, train_bce_dl_loss: 0.3177, step time: 0.4383\n",
      "batch: 5/17, train_dl_loss: 0.3400, train_bce_loss: 1.6601, train_bce_dl_loss: 0.3400, step time: 0.3828\n",
      "batch: 6/17, train_dl_loss: 0.3561, train_bce_loss: 1.6747, train_bce_dl_loss: 0.3561, step time: 0.4195\n",
      "batch: 7/17, train_dl_loss: 0.2762, train_bce_loss: 1.6440, train_bce_dl_loss: 0.2762, step time: 0.3774\n",
      "batch: 8/17, train_dl_loss: 0.2687, train_bce_loss: 1.6568, train_bce_dl_loss: 0.2687, step time: 0.4187\n",
      "batch: 9/17, train_dl_loss: 0.3339, train_bce_loss: 1.6698, train_bce_dl_loss: 0.3339, step time: 0.3767\n",
      "batch: 10/17, train_dl_loss: 0.3751, train_bce_loss: 1.6636, train_bce_dl_loss: 0.3751, step time: 0.4384\n",
      "batch: 11/17, train_dl_loss: 0.3040, train_bce_loss: 1.6474, train_bce_dl_loss: 0.3040, step time: 0.4248\n",
      "batch: 12/17, train_dl_loss: 0.2671, train_bce_loss: 1.6588, train_bce_dl_loss: 0.2671, step time: 0.4280\n",
      "batch: 13/17, train_dl_loss: 0.4015, train_bce_loss: 1.6808, train_bce_dl_loss: 0.4015, step time: 0.3792\n",
      "batch: 14/17, train_dl_loss: 0.3365, train_bce_loss: 1.6784, train_bce_dl_loss: 0.3365, step time: 0.4235\n",
      "batch: 15/17, train_dl_loss: 0.2851, train_bce_loss: 1.6603, train_bce_dl_loss: 0.2851, step time: 0.3800\n",
      "batch: 16/17, train_dl_loss: 0.2810, train_bce_loss: 1.6751, train_bce_dl_loss: 0.2810, step time: 0.4215\n",
      "batch: 17/17, train_dl_loss: 0.2978, train_bce_loss: 1.6560, train_bce_dl_loss: 0.2978, step time: 0.1119\n",
      "LOSS train DiceLoss: 0.3154, LOSS train BCE: 1.6625, LOSS train BCE-DiceLoss: 0.3154, LOSS val DiceLoss: 0.4382, LOSS val BCE: 1.6599, LOSS val BCE-DiceLoss: 0.4382, METRIC val: 0.5276\n",
      "time consuming of epoch 279 is: 435.7548\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3385, train_bce_loss: 1.6483, train_bce_dl_loss: 0.3385, step time: 0.4408\n",
      "batch: 1/17, train_dl_loss: 0.2778, train_bce_loss: 1.6683, train_bce_dl_loss: 0.2778, step time: 0.3763\n",
      "batch: 2/17, train_dl_loss: 0.3216, train_bce_loss: 1.6566, train_bce_dl_loss: 0.3216, step time: 0.4225\n",
      "batch: 3/17, train_dl_loss: 0.3604, train_bce_loss: 1.6617, train_bce_dl_loss: 0.3604, step time: 0.3848\n",
      "batch: 4/17, train_dl_loss: 0.2811, train_bce_loss: 1.6444, train_bce_dl_loss: 0.2811, step time: 0.4266\n",
      "batch: 5/17, train_dl_loss: 0.2846, train_bce_loss: 1.6695, train_bce_dl_loss: 0.2846, step time: 0.4193\n",
      "batch: 6/17, train_dl_loss: 0.3423, train_bce_loss: 1.6641, train_bce_dl_loss: 0.3423, step time: 0.4433\n",
      "batch: 7/17, train_dl_loss: 0.3022, train_bce_loss: 1.6687, train_bce_dl_loss: 0.3022, step time: 0.4222\n",
      "batch: 8/17, train_dl_loss: 0.2834, train_bce_loss: 1.6531, train_bce_dl_loss: 0.2834, step time: 0.4372\n",
      "batch: 9/17, train_dl_loss: 0.2960, train_bce_loss: 1.6682, train_bce_dl_loss: 0.2960, step time: 0.4311\n",
      "batch: 10/17, train_dl_loss: 0.3845, train_bce_loss: 1.6638, train_bce_dl_loss: 0.3845, step time: 0.4178\n",
      "batch: 11/17, train_dl_loss: 0.2565, train_bce_loss: 1.6385, train_bce_dl_loss: 0.2565, step time: 0.4260\n",
      "batch: 12/17, train_dl_loss: 0.2867, train_bce_loss: 1.6664, train_bce_dl_loss: 0.2867, step time: 0.4266\n",
      "batch: 13/17, train_dl_loss: 0.3803, train_bce_loss: 1.6820, train_bce_dl_loss: 0.3803, step time: 0.3842\n",
      "batch: 14/17, train_dl_loss: 0.2850, train_bce_loss: 1.6823, train_bce_dl_loss: 0.2850, step time: 0.4265\n",
      "batch: 15/17, train_dl_loss: 0.3168, train_bce_loss: 1.6671, train_bce_dl_loss: 0.3168, step time: 0.3728\n",
      "batch: 16/17, train_dl_loss: 0.3241, train_bce_loss: 1.6694, train_bce_dl_loss: 0.3241, step time: 0.4193\n",
      "batch: 17/17, train_dl_loss: 0.2364, train_bce_loss: 1.6574, train_bce_dl_loss: 0.2364, step time: 0.1125\n",
      "LOSS train DiceLoss: 0.3088, LOSS train BCE: 1.6628, LOSS train BCE-DiceLoss: 0.3088, LOSS val DiceLoss: 0.4396, LOSS val BCE: 1.6616, LOSS val BCE-DiceLoss: 0.4396, METRIC val: 0.5235\n",
      "time consuming of epoch 280 is: 464.8506\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2678, train_bce_loss: 1.6706, train_bce_dl_loss: 0.2678, step time: 0.4193\n",
      "batch: 1/17, train_dl_loss: 0.2781, train_bce_loss: 1.6643, train_bce_dl_loss: 0.2781, step time: 0.3912\n",
      "batch: 2/17, train_dl_loss: 0.3790, train_bce_loss: 1.6696, train_bce_dl_loss: 0.3790, step time: 0.4169\n",
      "batch: 3/17, train_dl_loss: 0.4080, train_bce_loss: 1.6859, train_bce_dl_loss: 0.4080, step time: 0.3693\n",
      "batch: 4/17, train_dl_loss: 0.2638, train_bce_loss: 1.6653, train_bce_dl_loss: 0.2638, step time: 0.4284\n",
      "batch: 5/17, train_dl_loss: 0.3341, train_bce_loss: 1.6582, train_bce_dl_loss: 0.3341, step time: 0.3776\n",
      "batch: 6/17, train_dl_loss: 0.3679, train_bce_loss: 1.6785, train_bce_dl_loss: 0.3679, step time: 0.4225\n",
      "batch: 7/17, train_dl_loss: 0.2641, train_bce_loss: 1.6594, train_bce_dl_loss: 0.2641, step time: 0.3847\n",
      "batch: 8/17, train_dl_loss: 0.2865, train_bce_loss: 1.6664, train_bce_dl_loss: 0.2865, step time: 0.4292\n",
      "batch: 9/17, train_dl_loss: 0.2938, train_bce_loss: 1.6816, train_bce_dl_loss: 0.2938, step time: 0.4281\n",
      "batch: 10/17, train_dl_loss: 0.4070, train_bce_loss: 1.6568, train_bce_dl_loss: 0.4070, step time: 0.4410\n",
      "batch: 11/17, train_dl_loss: 0.2920, train_bce_loss: 1.6478, train_bce_dl_loss: 0.2920, step time: 0.4197\n",
      "batch: 12/17, train_dl_loss: 0.2773, train_bce_loss: 1.6742, train_bce_dl_loss: 0.2773, step time: 0.4215\n",
      "batch: 13/17, train_dl_loss: 0.4010, train_bce_loss: 1.6736, train_bce_dl_loss: 0.4010, step time: 0.3789\n",
      "batch: 14/17, train_dl_loss: 0.3183, train_bce_loss: 1.6779, train_bce_dl_loss: 0.3183, step time: 0.4117\n",
      "batch: 15/17, train_dl_loss: 0.2714, train_bce_loss: 1.6650, train_bce_dl_loss: 0.2714, step time: 0.3767\n",
      "batch: 16/17, train_dl_loss: 0.2871, train_bce_loss: 1.6715, train_bce_dl_loss: 0.2871, step time: 0.4163\n",
      "batch: 17/17, train_dl_loss: 0.2298, train_bce_loss: 1.6572, train_bce_dl_loss: 0.2298, step time: 0.1112\n",
      "LOSS train DiceLoss: 0.3126, LOSS train BCE: 1.6680, LOSS train BCE-DiceLoss: 0.3126, LOSS val DiceLoss: 0.4463, LOSS val BCE: 1.6619, LOSS val BCE-DiceLoss: 0.4463, METRIC val: 0.5161\n",
      "time consuming of epoch 281 is: 422.2223\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2979, train_bce_loss: 1.6517, train_bce_dl_loss: 0.2979, step time: 0.4182\n",
      "batch: 1/17, train_dl_loss: 0.2851, train_bce_loss: 1.6708, train_bce_dl_loss: 0.2851, step time: 0.3796\n",
      "batch: 2/17, train_dl_loss: 0.3120, train_bce_loss: 1.6743, train_bce_dl_loss: 0.3120, step time: 0.4129\n",
      "batch: 3/17, train_dl_loss: 0.3233, train_bce_loss: 1.6693, train_bce_dl_loss: 0.3233, step time: 0.3747\n",
      "batch: 4/17, train_dl_loss: 0.2713, train_bce_loss: 1.6734, train_bce_dl_loss: 0.2713, step time: 0.4143\n",
      "batch: 5/17, train_dl_loss: 0.3169, train_bce_loss: 1.6678, train_bce_dl_loss: 0.3169, step time: 0.4332\n",
      "batch: 6/17, train_dl_loss: 0.3817, train_bce_loss: 1.6824, train_bce_dl_loss: 0.3817, step time: 0.4255\n",
      "batch: 7/17, train_dl_loss: 0.2649, train_bce_loss: 1.6520, train_bce_dl_loss: 0.2649, step time: 0.4394\n",
      "batch: 8/17, train_dl_loss: 0.3067, train_bce_loss: 1.6594, train_bce_dl_loss: 0.3067, step time: 0.4349\n",
      "batch: 9/17, train_dl_loss: 0.3066, train_bce_loss: 1.6698, train_bce_dl_loss: 0.3066, step time: 0.4350\n",
      "batch: 10/17, train_dl_loss: 0.3900, train_bce_loss: 1.6497, train_bce_dl_loss: 0.3900, step time: 0.4143\n",
      "batch: 11/17, train_dl_loss: 0.2762, train_bce_loss: 1.6519, train_bce_dl_loss: 0.2762, step time: 0.4226\n",
      "batch: 12/17, train_dl_loss: 0.2747, train_bce_loss: 1.6653, train_bce_dl_loss: 0.2747, step time: 0.4198\n",
      "batch: 13/17, train_dl_loss: 0.3745, train_bce_loss: 1.6795, train_bce_dl_loss: 0.3745, step time: 0.4371\n",
      "batch: 14/17, train_dl_loss: 0.3414, train_bce_loss: 1.6836, train_bce_dl_loss: 0.3414, step time: 0.4239\n",
      "batch: 15/17, train_dl_loss: 0.2854, train_bce_loss: 1.6649, train_bce_dl_loss: 0.2854, step time: 0.4155\n",
      "batch: 16/17, train_dl_loss: 0.2795, train_bce_loss: 1.6725, train_bce_dl_loss: 0.2795, step time: 0.4316\n",
      "batch: 17/17, train_dl_loss: 0.2247, train_bce_loss: 1.6779, train_bce_dl_loss: 0.2247, step time: 0.1110\n",
      "LOSS train DiceLoss: 0.3063, LOSS train BCE: 1.6676, LOSS train BCE-DiceLoss: 0.3063, LOSS val DiceLoss: 0.4483, LOSS val BCE: 1.6583, LOSS val BCE-DiceLoss: 0.4483, METRIC val: 0.5136\n",
      "time consuming of epoch 282 is: 508.1470\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3077, train_bce_loss: 1.6449, train_bce_dl_loss: 0.3077, step time: 0.4124\n",
      "batch: 1/17, train_dl_loss: 0.3292, train_bce_loss: 1.6647, train_bce_dl_loss: 0.3292, step time: 0.3739\n",
      "batch: 2/17, train_dl_loss: 0.3472, train_bce_loss: 1.6719, train_bce_dl_loss: 0.3472, step time: 0.4307\n",
      "batch: 3/17, train_dl_loss: 0.3664, train_bce_loss: 1.6827, train_bce_dl_loss: 0.3664, step time: 0.3761\n",
      "batch: 4/17, train_dl_loss: 0.2560, train_bce_loss: 1.6602, train_bce_dl_loss: 0.2560, step time: 0.4355\n",
      "batch: 5/17, train_dl_loss: 0.2873, train_bce_loss: 1.6471, train_bce_dl_loss: 0.2873, step time: 0.3761\n",
      "batch: 6/17, train_dl_loss: 0.3352, train_bce_loss: 1.6734, train_bce_dl_loss: 0.3352, step time: 0.4371\n",
      "batch: 7/17, train_dl_loss: 0.2802, train_bce_loss: 1.6466, train_bce_dl_loss: 0.2802, step time: 0.3816\n",
      "batch: 8/17, train_dl_loss: 0.2957, train_bce_loss: 1.6408, train_bce_dl_loss: 0.2957, step time: 0.4398\n",
      "batch: 9/17, train_dl_loss: 0.2985, train_bce_loss: 1.6752, train_bce_dl_loss: 0.2985, step time: 0.3840\n",
      "batch: 10/17, train_dl_loss: 0.3666, train_bce_loss: 1.6588, train_bce_dl_loss: 0.3666, step time: 0.4446\n",
      "batch: 11/17, train_dl_loss: 0.2992, train_bce_loss: 1.6560, train_bce_dl_loss: 0.2992, step time: 0.4354\n",
      "batch: 12/17, train_dl_loss: 0.2781, train_bce_loss: 1.6556, train_bce_dl_loss: 0.2781, step time: 0.4325\n",
      "batch: 13/17, train_dl_loss: 0.3681, train_bce_loss: 1.6753, train_bce_dl_loss: 0.3681, step time: 0.3800\n",
      "batch: 14/17, train_dl_loss: 0.3299, train_bce_loss: 1.6825, train_bce_dl_loss: 0.3299, step time: 0.4344\n",
      "batch: 15/17, train_dl_loss: 0.2867, train_bce_loss: 1.6676, train_bce_dl_loss: 0.2867, step time: 0.3827\n",
      "batch: 16/17, train_dl_loss: 0.2904, train_bce_loss: 1.6664, train_bce_dl_loss: 0.2904, step time: 0.4175\n",
      "batch: 17/17, train_dl_loss: 0.2882, train_bce_loss: 1.6580, train_bce_dl_loss: 0.2882, step time: 0.1152\n",
      "LOSS train DiceLoss: 0.3117, LOSS train BCE: 1.6627, LOSS train BCE-DiceLoss: 0.3117, LOSS val DiceLoss: 0.4342, LOSS val BCE: 1.6597, LOSS val BCE-DiceLoss: 0.4342, METRIC val: 0.5300\n",
      "time consuming of epoch 283 is: 499.2136\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3124, train_bce_loss: 1.6719, train_bce_dl_loss: 0.3124, step time: 0.4389\n",
      "batch: 1/17, train_dl_loss: 0.3034, train_bce_loss: 1.6720, train_bce_dl_loss: 0.3034, step time: 0.3796\n",
      "batch: 2/17, train_dl_loss: 0.3612, train_bce_loss: 1.6609, train_bce_dl_loss: 0.3612, step time: 0.4297\n",
      "batch: 3/17, train_dl_loss: 0.3906, train_bce_loss: 1.6507, train_bce_dl_loss: 0.3906, step time: 0.3860\n",
      "batch: 4/17, train_dl_loss: 0.2637, train_bce_loss: 1.6699, train_bce_dl_loss: 0.2637, step time: 0.4191\n",
      "batch: 5/17, train_dl_loss: 0.2818, train_bce_loss: 1.6489, train_bce_dl_loss: 0.2818, step time: 0.3773\n",
      "batch: 6/17, train_dl_loss: 0.3368, train_bce_loss: 1.6738, train_bce_dl_loss: 0.3368, step time: 0.4516\n",
      "batch: 7/17, train_dl_loss: 0.2673, train_bce_loss: 1.6496, train_bce_dl_loss: 0.2673, step time: 0.3931\n",
      "batch: 8/17, train_dl_loss: 0.3008, train_bce_loss: 1.6643, train_bce_dl_loss: 0.3008, step time: 0.4241\n",
      "batch: 9/17, train_dl_loss: 0.2757, train_bce_loss: 1.6653, train_bce_dl_loss: 0.2757, step time: 0.4339\n",
      "batch: 10/17, train_dl_loss: 0.3629, train_bce_loss: 1.6712, train_bce_dl_loss: 0.3629, step time: 0.4522\n",
      "batch: 11/17, train_dl_loss: 0.2668, train_bce_loss: 1.6523, train_bce_dl_loss: 0.2668, step time: 0.4699\n",
      "batch: 12/17, train_dl_loss: 0.2958, train_bce_loss: 1.6489, train_bce_dl_loss: 0.2958, step time: 0.4267\n",
      "batch: 13/17, train_dl_loss: 0.4324, train_bce_loss: 1.6769, train_bce_dl_loss: 0.4324, step time: 0.3766\n",
      "batch: 14/17, train_dl_loss: 0.3176, train_bce_loss: 1.6856, train_bce_dl_loss: 0.3176, step time: 0.4223\n",
      "batch: 15/17, train_dl_loss: 0.2756, train_bce_loss: 1.6681, train_bce_dl_loss: 0.2756, step time: 0.3691\n",
      "batch: 16/17, train_dl_loss: 0.2926, train_bce_loss: 1.6717, train_bce_dl_loss: 0.2926, step time: 0.4131\n",
      "batch: 17/17, train_dl_loss: 0.2332, train_bce_loss: 1.6657, train_bce_dl_loss: 0.2332, step time: 0.1130\n",
      "LOSS train DiceLoss: 0.3095, LOSS train BCE: 1.6649, LOSS train BCE-DiceLoss: 0.3095, LOSS val DiceLoss: 0.4341, LOSS val BCE: 1.6627, LOSS val BCE-DiceLoss: 0.4341, METRIC val: 0.5319\n",
      "time consuming of epoch 284 is: 467.5823\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2807, train_bce_loss: 1.6655, train_bce_dl_loss: 0.2807, step time: 0.4297\n",
      "batch: 1/17, train_dl_loss: 0.3243, train_bce_loss: 1.6647, train_bce_dl_loss: 0.3243, step time: 0.3830\n",
      "batch: 2/17, train_dl_loss: 0.2846, train_bce_loss: 1.6614, train_bce_dl_loss: 0.2846, step time: 0.4266\n",
      "batch: 3/17, train_dl_loss: 0.3731, train_bce_loss: 1.6652, train_bce_dl_loss: 0.3731, step time: 0.3786\n",
      "batch: 4/17, train_dl_loss: 0.2723, train_bce_loss: 1.6496, train_bce_dl_loss: 0.2723, step time: 0.4265\n",
      "batch: 5/17, train_dl_loss: 0.3073, train_bce_loss: 1.6532, train_bce_dl_loss: 0.3073, step time: 0.3926\n",
      "batch: 6/17, train_dl_loss: 0.3303, train_bce_loss: 1.6712, train_bce_dl_loss: 0.3303, step time: 0.4315\n",
      "batch: 7/17, train_dl_loss: 0.2898, train_bce_loss: 1.6455, train_bce_dl_loss: 0.2898, step time: 0.3793\n",
      "batch: 8/17, train_dl_loss: 0.2840, train_bce_loss: 1.6566, train_bce_dl_loss: 0.2840, step time: 0.4321\n",
      "batch: 9/17, train_dl_loss: 0.2742, train_bce_loss: 1.6624, train_bce_dl_loss: 0.2742, step time: 0.3800\n",
      "batch: 10/17, train_dl_loss: 0.4627, train_bce_loss: 1.6662, train_bce_dl_loss: 0.4627, step time: 0.4326\n",
      "batch: 11/17, train_dl_loss: 0.3279, train_bce_loss: 1.6475, train_bce_dl_loss: 0.3279, step time: 0.3899\n",
      "batch: 12/17, train_dl_loss: 0.2926, train_bce_loss: 1.6552, train_bce_dl_loss: 0.2926, step time: 0.4325\n",
      "batch: 13/17, train_dl_loss: 0.4069, train_bce_loss: 1.6837, train_bce_dl_loss: 0.4069, step time: 0.3887\n",
      "batch: 14/17, train_dl_loss: 0.3101, train_bce_loss: 1.6743, train_bce_dl_loss: 0.3101, step time: 0.4137\n",
      "batch: 15/17, train_dl_loss: 0.2510, train_bce_loss: 1.6643, train_bce_dl_loss: 0.2510, step time: 0.3812\n",
      "batch: 16/17, train_dl_loss: 0.3154, train_bce_loss: 1.6724, train_bce_dl_loss: 0.3154, step time: 0.4175\n",
      "batch: 17/17, train_dl_loss: 0.2421, train_bce_loss: 1.6598, train_bce_dl_loss: 0.2421, step time: 0.1142\n",
      "LOSS train DiceLoss: 0.3127, LOSS train BCE: 1.6621, LOSS train BCE-DiceLoss: 0.3127, LOSS val DiceLoss: 0.4368, LOSS val BCE: 1.6630, LOSS val BCE-DiceLoss: 0.4368, METRIC val: 0.5272\n",
      "time consuming of epoch 285 is: 431.7676\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3072, train_bce_loss: 1.6714, train_bce_dl_loss: 0.3072, step time: 0.4296\n",
      "batch: 1/17, train_dl_loss: 0.2711, train_bce_loss: 1.6693, train_bce_dl_loss: 0.2711, step time: 0.3865\n",
      "batch: 2/17, train_dl_loss: 0.2912, train_bce_loss: 1.6695, train_bce_dl_loss: 0.2912, step time: 0.4212\n",
      "batch: 3/17, train_dl_loss: 0.3373, train_bce_loss: 1.6692, train_bce_dl_loss: 0.3373, step time: 0.3714\n",
      "batch: 4/17, train_dl_loss: 0.2749, train_bce_loss: 1.6698, train_bce_dl_loss: 0.2749, step time: 0.4337\n",
      "batch: 5/17, train_dl_loss: 0.2628, train_bce_loss: 1.6634, train_bce_dl_loss: 0.2628, step time: 0.4344\n",
      "batch: 6/17, train_dl_loss: 0.3329, train_bce_loss: 1.6724, train_bce_dl_loss: 0.3329, step time: 0.4317\n",
      "batch: 7/17, train_dl_loss: 0.2629, train_bce_loss: 1.6570, train_bce_dl_loss: 0.2629, step time: 0.4242\n",
      "batch: 8/17, train_dl_loss: 0.2721, train_bce_loss: 1.6683, train_bce_dl_loss: 0.2721, step time: 0.4396\n",
      "batch: 9/17, train_dl_loss: 0.3017, train_bce_loss: 1.6660, train_bce_dl_loss: 0.3017, step time: 0.4421\n",
      "batch: 10/17, train_dl_loss: 0.4082, train_bce_loss: 1.6765, train_bce_dl_loss: 0.4082, step time: 0.4330\n",
      "batch: 11/17, train_dl_loss: 0.2984, train_bce_loss: 1.6600, train_bce_dl_loss: 0.2984, step time: 0.4231\n",
      "batch: 12/17, train_dl_loss: 0.2866, train_bce_loss: 1.6569, train_bce_dl_loss: 0.2866, step time: 0.4191\n",
      "batch: 13/17, train_dl_loss: 0.3868, train_bce_loss: 1.6794, train_bce_dl_loss: 0.3868, step time: 0.4258\n",
      "batch: 14/17, train_dl_loss: 0.3293, train_bce_loss: 1.6758, train_bce_dl_loss: 0.3293, step time: 0.4257\n",
      "batch: 15/17, train_dl_loss: 0.2968, train_bce_loss: 1.6812, train_bce_dl_loss: 0.2968, step time: 0.3877\n",
      "batch: 16/17, train_dl_loss: 0.2634, train_bce_loss: 1.6696, train_bce_dl_loss: 0.2634, step time: 0.4253\n",
      "batch: 17/17, train_dl_loss: 0.3949, train_bce_loss: 1.6850, train_bce_dl_loss: 0.3949, step time: 0.1128\n",
      "LOSS train DiceLoss: 0.3099, LOSS train BCE: 1.6700, LOSS train BCE-DiceLoss: 0.3099, LOSS val DiceLoss: 0.4275, LOSS val BCE: 1.6636, LOSS val BCE-DiceLoss: 0.4275, METRIC val: 0.5380\n",
      "time consuming of epoch 286 is: 454.6043\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3581, train_bce_loss: 1.6855, train_bce_dl_loss: 0.3581, step time: 0.4290\n",
      "batch: 1/17, train_dl_loss: 0.2899, train_bce_loss: 1.6593, train_bce_dl_loss: 0.2899, step time: 0.3834\n",
      "batch: 2/17, train_dl_loss: 0.2715, train_bce_loss: 1.6595, train_bce_dl_loss: 0.2715, step time: 0.4182\n",
      "batch: 3/17, train_dl_loss: 0.3391, train_bce_loss: 1.6675, train_bce_dl_loss: 0.3391, step time: 0.3901\n",
      "batch: 4/17, train_dl_loss: 0.2782, train_bce_loss: 1.6730, train_bce_dl_loss: 0.2782, step time: 0.4278\n",
      "batch: 5/17, train_dl_loss: 0.2547, train_bce_loss: 1.6580, train_bce_dl_loss: 0.2547, step time: 0.3907\n",
      "batch: 6/17, train_dl_loss: 0.3453, train_bce_loss: 1.6678, train_bce_dl_loss: 0.3453, step time: 0.4646\n",
      "batch: 7/17, train_dl_loss: 0.2515, train_bce_loss: 1.6630, train_bce_dl_loss: 0.2515, step time: 0.3859\n",
      "batch: 8/17, train_dl_loss: 0.2721, train_bce_loss: 1.6467, train_bce_dl_loss: 0.2721, step time: 0.4382\n",
      "batch: 9/17, train_dl_loss: 0.3668, train_bce_loss: 1.6729, train_bce_dl_loss: 0.3668, step time: 0.3787\n",
      "batch: 10/17, train_dl_loss: 0.3939, train_bce_loss: 1.6721, train_bce_dl_loss: 0.3939, step time: 0.4291\n",
      "batch: 11/17, train_dl_loss: 0.2775, train_bce_loss: 1.6555, train_bce_dl_loss: 0.2775, step time: 0.3877\n",
      "batch: 12/17, train_dl_loss: 0.2616, train_bce_loss: 1.6538, train_bce_dl_loss: 0.2616, step time: 0.4367\n",
      "batch: 13/17, train_dl_loss: 0.3666, train_bce_loss: 1.6724, train_bce_dl_loss: 0.3666, step time: 0.3753\n",
      "batch: 14/17, train_dl_loss: 0.2774, train_bce_loss: 1.6816, train_bce_dl_loss: 0.2774, step time: 0.4358\n",
      "batch: 15/17, train_dl_loss: 0.2921, train_bce_loss: 1.6735, train_bce_dl_loss: 0.2921, step time: 0.3760\n",
      "batch: 16/17, train_dl_loss: 0.2653, train_bce_loss: 1.6732, train_bce_dl_loss: 0.2653, step time: 0.4195\n",
      "batch: 17/17, train_dl_loss: 0.2843, train_bce_loss: 1.6590, train_bce_dl_loss: 0.2843, step time: 0.1142\n",
      "LOSS train DiceLoss: 0.3025, LOSS train BCE: 1.6664, LOSS train BCE-DiceLoss: 0.3025, LOSS val DiceLoss: 0.4352, LOSS val BCE: 1.6638, LOSS val BCE-DiceLoss: 0.4352, METRIC val: 0.5306\n",
      "time consuming of epoch 287 is: 468.5896\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2718, train_bce_loss: 1.6549, train_bce_dl_loss: 0.2718, step time: 0.4746\n",
      "batch: 1/17, train_dl_loss: 0.2926, train_bce_loss: 1.6622, train_bce_dl_loss: 0.2926, step time: 0.3804\n",
      "batch: 2/17, train_dl_loss: 0.3148, train_bce_loss: 1.6695, train_bce_dl_loss: 0.3148, step time: 0.4309\n",
      "batch: 3/17, train_dl_loss: 0.3595, train_bce_loss: 1.6642, train_bce_dl_loss: 0.3595, step time: 0.3759\n",
      "batch: 4/17, train_dl_loss: 0.2711, train_bce_loss: 1.6803, train_bce_dl_loss: 0.2711, step time: 0.4317\n",
      "batch: 5/17, train_dl_loss: 0.2940, train_bce_loss: 1.6478, train_bce_dl_loss: 0.2940, step time: 0.4011\n",
      "batch: 6/17, train_dl_loss: 0.3873, train_bce_loss: 1.6691, train_bce_dl_loss: 0.3873, step time: 0.4366\n",
      "batch: 7/17, train_dl_loss: 0.2698, train_bce_loss: 1.6476, train_bce_dl_loss: 0.2698, step time: 0.3913\n",
      "batch: 8/17, train_dl_loss: 0.3256, train_bce_loss: 1.6681, train_bce_dl_loss: 0.3256, step time: 0.4205\n",
      "batch: 9/17, train_dl_loss: 0.2709, train_bce_loss: 1.6719, train_bce_dl_loss: 0.2709, step time: 0.3760\n",
      "batch: 10/17, train_dl_loss: 0.3597, train_bce_loss: 1.6591, train_bce_dl_loss: 0.3597, step time: 0.4217\n",
      "batch: 11/17, train_dl_loss: 0.2520, train_bce_loss: 1.6537, train_bce_dl_loss: 0.2520, step time: 0.4431\n",
      "batch: 12/17, train_dl_loss: 0.2768, train_bce_loss: 1.6672, train_bce_dl_loss: 0.2768, step time: 0.4404\n",
      "batch: 13/17, train_dl_loss: 0.3804, train_bce_loss: 1.6847, train_bce_dl_loss: 0.3804, step time: 0.3920\n",
      "batch: 14/17, train_dl_loss: 0.3085, train_bce_loss: 1.6832, train_bce_dl_loss: 0.3085, step time: 0.4322\n",
      "batch: 15/17, train_dl_loss: 0.2897, train_bce_loss: 1.6815, train_bce_dl_loss: 0.2897, step time: 0.3711\n",
      "batch: 16/17, train_dl_loss: 0.2824, train_bce_loss: 1.6750, train_bce_dl_loss: 0.2824, step time: 0.4329\n",
      "batch: 17/17, train_dl_loss: 0.2583, train_bce_loss: 1.6812, train_bce_dl_loss: 0.2583, step time: 0.1120\n",
      "LOSS train DiceLoss: 0.3036, LOSS train BCE: 1.6678, LOSS train BCE-DiceLoss: 0.3036, LOSS val DiceLoss: 0.4514, LOSS val BCE: 1.6657, LOSS val BCE-DiceLoss: 0.4514, METRIC val: 0.5089\n",
      "time consuming of epoch 288 is: 449.6145\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2750, train_bce_loss: 1.6665, train_bce_dl_loss: 0.2750, step time: 0.4306\n",
      "batch: 1/17, train_dl_loss: 0.3622, train_bce_loss: 1.6658, train_bce_dl_loss: 0.3622, step time: 0.3832\n",
      "batch: 2/17, train_dl_loss: 0.3095, train_bce_loss: 1.6687, train_bce_dl_loss: 0.3095, step time: 0.4152\n",
      "batch: 3/17, train_dl_loss: 0.4046, train_bce_loss: 1.6633, train_bce_dl_loss: 0.4046, step time: 0.3797\n",
      "batch: 4/17, train_dl_loss: 0.2728, train_bce_loss: 1.6592, train_bce_dl_loss: 0.2728, step time: 0.4454\n",
      "batch: 5/17, train_dl_loss: 0.2625, train_bce_loss: 1.6648, train_bce_dl_loss: 0.2625, step time: 0.3821\n",
      "batch: 6/17, train_dl_loss: 0.3539, train_bce_loss: 1.6720, train_bce_dl_loss: 0.3539, step time: 0.4298\n",
      "batch: 7/17, train_dl_loss: 0.2789, train_bce_loss: 1.6427, train_bce_dl_loss: 0.2789, step time: 0.3724\n",
      "batch: 8/17, train_dl_loss: 0.2922, train_bce_loss: 1.6624, train_bce_dl_loss: 0.2922, step time: 0.4215\n",
      "batch: 9/17, train_dl_loss: 0.3155, train_bce_loss: 1.6608, train_bce_dl_loss: 0.3155, step time: 0.3992\n",
      "batch: 10/17, train_dl_loss: 0.4638, train_bce_loss: 1.6777, train_bce_dl_loss: 0.4638, step time: 0.4462\n",
      "batch: 11/17, train_dl_loss: 0.2831, train_bce_loss: 1.6701, train_bce_dl_loss: 0.2831, step time: 0.3918\n",
      "batch: 12/17, train_dl_loss: 0.2772, train_bce_loss: 1.6601, train_bce_dl_loss: 0.2772, step time: 0.4477\n",
      "batch: 13/17, train_dl_loss: 0.4178, train_bce_loss: 1.6825, train_bce_dl_loss: 0.4178, step time: 0.3859\n",
      "batch: 14/17, train_dl_loss: 0.3312, train_bce_loss: 1.6707, train_bce_dl_loss: 0.3312, step time: 0.4318\n",
      "batch: 15/17, train_dl_loss: 0.2953, train_bce_loss: 1.6689, train_bce_dl_loss: 0.2953, step time: 0.4037\n",
      "batch: 16/17, train_dl_loss: 0.3039, train_bce_loss: 1.6712, train_bce_dl_loss: 0.3039, step time: 0.4136\n",
      "batch: 17/17, train_dl_loss: 0.2317, train_bce_loss: 1.6543, train_bce_dl_loss: 0.2317, step time: 0.1137\n",
      "LOSS train DiceLoss: 0.3184, LOSS train BCE: 1.6656, LOSS train BCE-DiceLoss: 0.3184, LOSS val DiceLoss: 0.4408, LOSS val BCE: 1.6621, LOSS val BCE-DiceLoss: 0.4408, METRIC val: 0.5239\n",
      "time consuming of epoch 289 is: 443.5112\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2919, train_bce_loss: 1.6650, train_bce_dl_loss: 0.2919, step time: 0.4437\n",
      "batch: 1/17, train_dl_loss: 0.3963, train_bce_loss: 1.6821, train_bce_dl_loss: 0.3963, step time: 0.3899\n",
      "batch: 2/17, train_dl_loss: 0.3030, train_bce_loss: 1.6760, train_bce_dl_loss: 0.3030, step time: 0.4209\n",
      "batch: 3/17, train_dl_loss: 0.3405, train_bce_loss: 1.6688, train_bce_dl_loss: 0.3405, step time: 0.3864\n",
      "batch: 4/17, train_dl_loss: 0.2797, train_bce_loss: 1.6540, train_bce_dl_loss: 0.2797, step time: 0.4322\n",
      "batch: 5/17, train_dl_loss: 0.2890, train_bce_loss: 1.6611, train_bce_dl_loss: 0.2890, step time: 0.3831\n",
      "batch: 6/17, train_dl_loss: 0.3688, train_bce_loss: 1.6785, train_bce_dl_loss: 0.3688, step time: 0.4260\n",
      "batch: 7/17, train_dl_loss: 0.3041, train_bce_loss: 1.6690, train_bce_dl_loss: 0.3041, step time: 0.3891\n",
      "batch: 8/17, train_dl_loss: 0.2968, train_bce_loss: 1.6452, train_bce_dl_loss: 0.2968, step time: 0.4222\n",
      "batch: 9/17, train_dl_loss: 0.2789, train_bce_loss: 1.6670, train_bce_dl_loss: 0.2789, step time: 0.4407\n",
      "batch: 10/17, train_dl_loss: 0.3612, train_bce_loss: 1.6547, train_bce_dl_loss: 0.3612, step time: 0.4296\n",
      "batch: 11/17, train_dl_loss: 0.2727, train_bce_loss: 1.6472, train_bce_dl_loss: 0.2727, step time: 0.4463\n",
      "batch: 12/17, train_dl_loss: 0.3193, train_bce_loss: 1.6660, train_bce_dl_loss: 0.3193, step time: 0.4265\n",
      "batch: 13/17, train_dl_loss: 0.4538, train_bce_loss: 1.6740, train_bce_dl_loss: 0.4538, step time: 0.3821\n",
      "batch: 14/17, train_dl_loss: 0.2834, train_bce_loss: 1.6809, train_bce_dl_loss: 0.2834, step time: 0.4202\n",
      "batch: 15/17, train_dl_loss: 0.3081, train_bce_loss: 1.6651, train_bce_dl_loss: 0.3081, step time: 0.3794\n",
      "batch: 16/17, train_dl_loss: 0.2932, train_bce_loss: 1.6748, train_bce_dl_loss: 0.2932, step time: 0.4241\n",
      "batch: 17/17, train_dl_loss: 0.2381, train_bce_loss: 1.6807, train_bce_dl_loss: 0.2381, step time: 0.1137\n",
      "LOSS train DiceLoss: 0.3155, LOSS train BCE: 1.6672, LOSS train BCE-DiceLoss: 0.3155, LOSS val DiceLoss: 0.4375, LOSS val BCE: 1.6658, LOSS val BCE-DiceLoss: 0.4375, METRIC val: 0.5262\n",
      "time consuming of epoch 290 is: 502.9575\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2737, train_bce_loss: 1.6655, train_bce_dl_loss: 0.2737, step time: 0.4506\n",
      "batch: 1/17, train_dl_loss: 0.2886, train_bce_loss: 1.6677, train_bce_dl_loss: 0.2886, step time: 0.4196\n",
      "batch: 2/17, train_dl_loss: 0.2836, train_bce_loss: 1.6633, train_bce_dl_loss: 0.2836, step time: 0.4398\n",
      "batch: 3/17, train_dl_loss: 0.3413, train_bce_loss: 1.6715, train_bce_dl_loss: 0.3413, step time: 0.3846\n",
      "batch: 4/17, train_dl_loss: 0.2809, train_bce_loss: 1.6660, train_bce_dl_loss: 0.2809, step time: 0.4314\n",
      "batch: 5/17, train_dl_loss: 0.2632, train_bce_loss: 1.6654, train_bce_dl_loss: 0.2632, step time: 0.3861\n",
      "batch: 6/17, train_dl_loss: 0.3626, train_bce_loss: 1.6728, train_bce_dl_loss: 0.3626, step time: 0.4205\n",
      "batch: 7/17, train_dl_loss: 0.2622, train_bce_loss: 1.6493, train_bce_dl_loss: 0.2622, step time: 0.3882\n",
      "batch: 8/17, train_dl_loss: 0.2654, train_bce_loss: 1.6518, train_bce_dl_loss: 0.2654, step time: 0.4177\n",
      "batch: 9/17, train_dl_loss: 0.3126, train_bce_loss: 1.6694, train_bce_dl_loss: 0.3126, step time: 0.3771\n",
      "batch: 10/17, train_dl_loss: 0.3797, train_bce_loss: 1.6574, train_bce_dl_loss: 0.3797, step time: 0.4400\n",
      "batch: 11/17, train_dl_loss: 0.2876, train_bce_loss: 1.6687, train_bce_dl_loss: 0.2876, step time: 0.4424\n",
      "batch: 12/17, train_dl_loss: 0.2887, train_bce_loss: 1.6589, train_bce_dl_loss: 0.2887, step time: 0.4395\n",
      "batch: 13/17, train_dl_loss: 0.4653, train_bce_loss: 1.6815, train_bce_dl_loss: 0.4653, step time: 0.3867\n",
      "batch: 14/17, train_dl_loss: 0.3098, train_bce_loss: 1.6877, train_bce_dl_loss: 0.3098, step time: 0.4343\n",
      "batch: 15/17, train_dl_loss: 0.2982, train_bce_loss: 1.6679, train_bce_dl_loss: 0.2982, step time: 0.3755\n",
      "batch: 16/17, train_dl_loss: 0.2940, train_bce_loss: 1.6752, train_bce_dl_loss: 0.2940, step time: 0.4208\n",
      "batch: 17/17, train_dl_loss: 0.2466, train_bce_loss: 1.6700, train_bce_dl_loss: 0.2466, step time: 0.1140\n",
      "LOSS train DiceLoss: 0.3058, LOSS train BCE: 1.6672, LOSS train BCE-DiceLoss: 0.3058, LOSS val DiceLoss: 0.4473, LOSS val BCE: 1.6655, LOSS val BCE-DiceLoss: 0.4473, METRIC val: 0.5154\n",
      "time consuming of epoch 291 is: 480.2800\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3638, train_bce_loss: 1.6731, train_bce_dl_loss: 0.3638, step time: 0.4458\n",
      "batch: 1/17, train_dl_loss: 0.3012, train_bce_loss: 1.6631, train_bce_dl_loss: 0.3012, step time: 0.3771\n",
      "batch: 2/17, train_dl_loss: 0.3434, train_bce_loss: 1.6653, train_bce_dl_loss: 0.3434, step time: 0.4165\n",
      "batch: 3/17, train_dl_loss: 0.3542, train_bce_loss: 1.6560, train_bce_dl_loss: 0.3542, step time: 0.3822\n",
      "batch: 4/17, train_dl_loss: 0.2687, train_bce_loss: 1.6657, train_bce_dl_loss: 0.2687, step time: 0.4259\n",
      "batch: 5/17, train_dl_loss: 0.3146, train_bce_loss: 1.6561, train_bce_dl_loss: 0.3146, step time: 0.3843\n",
      "batch: 6/17, train_dl_loss: 0.3524, train_bce_loss: 1.6685, train_bce_dl_loss: 0.3524, step time: 0.4357\n",
      "batch: 7/17, train_dl_loss: 0.2680, train_bce_loss: 1.6474, train_bce_dl_loss: 0.2680, step time: 0.3807\n",
      "batch: 8/17, train_dl_loss: 0.3093, train_bce_loss: 1.6476, train_bce_dl_loss: 0.3093, step time: 0.4306\n",
      "batch: 9/17, train_dl_loss: 0.3219, train_bce_loss: 1.6882, train_bce_dl_loss: 0.3219, step time: 0.3810\n",
      "batch: 10/17, train_dl_loss: 0.4089, train_bce_loss: 1.6743, train_bce_dl_loss: 0.4089, step time: 0.4260\n",
      "batch: 11/17, train_dl_loss: 0.2854, train_bce_loss: 1.6704, train_bce_dl_loss: 0.2854, step time: 0.4290\n",
      "batch: 12/17, train_dl_loss: 0.2703, train_bce_loss: 1.6605, train_bce_dl_loss: 0.2703, step time: 0.4327\n",
      "batch: 13/17, train_dl_loss: 0.4184, train_bce_loss: 1.6826, train_bce_dl_loss: 0.4184, step time: 0.3826\n",
      "batch: 14/17, train_dl_loss: 0.2746, train_bce_loss: 1.6808, train_bce_dl_loss: 0.2746, step time: 0.4253\n",
      "batch: 15/17, train_dl_loss: 0.2918, train_bce_loss: 1.6664, train_bce_dl_loss: 0.2918, step time: 0.3805\n",
      "batch: 16/17, train_dl_loss: 0.2840, train_bce_loss: 1.6663, train_bce_dl_loss: 0.2840, step time: 0.4192\n",
      "batch: 17/17, train_dl_loss: 0.3576, train_bce_loss: 1.6820, train_bce_dl_loss: 0.3576, step time: 0.1136\n",
      "LOSS train DiceLoss: 0.3216, LOSS train BCE: 1.6675, LOSS train BCE-DiceLoss: 0.3216, LOSS val DiceLoss: 0.4414, LOSS val BCE: 1.6618, LOSS val BCE-DiceLoss: 0.4414, METRIC val: 0.5213\n",
      "time consuming of epoch 292 is: 455.9387\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2947, train_bce_loss: 1.6680, train_bce_dl_loss: 0.2947, step time: 0.4249\n",
      "batch: 1/17, train_dl_loss: 0.2840, train_bce_loss: 1.6836, train_bce_dl_loss: 0.2840, step time: 0.3777\n",
      "batch: 2/17, train_dl_loss: 0.3332, train_bce_loss: 1.6782, train_bce_dl_loss: 0.3332, step time: 0.4287\n",
      "batch: 3/17, train_dl_loss: 0.4021, train_bce_loss: 1.6780, train_bce_dl_loss: 0.4021, step time: 0.3739\n",
      "batch: 4/17, train_dl_loss: 0.2884, train_bce_loss: 1.6847, train_bce_dl_loss: 0.2884, step time: 0.4166\n",
      "batch: 5/17, train_dl_loss: 0.2776, train_bce_loss: 1.6599, train_bce_dl_loss: 0.2776, step time: 0.3854\n",
      "batch: 6/17, train_dl_loss: 0.3685, train_bce_loss: 1.6699, train_bce_dl_loss: 0.3685, step time: 0.4413\n",
      "batch: 7/17, train_dl_loss: 0.2623, train_bce_loss: 1.6591, train_bce_dl_loss: 0.2623, step time: 0.3816\n",
      "batch: 8/17, train_dl_loss: 0.3134, train_bce_loss: 1.6488, train_bce_dl_loss: 0.3134, step time: 0.4286\n",
      "batch: 9/17, train_dl_loss: 0.3177, train_bce_loss: 1.6594, train_bce_dl_loss: 0.3177, step time: 0.3857\n",
      "batch: 10/17, train_dl_loss: 0.3547, train_bce_loss: 1.6728, train_bce_dl_loss: 0.3547, step time: 0.4538\n",
      "batch: 11/17, train_dl_loss: 0.3067, train_bce_loss: 1.6491, train_bce_dl_loss: 0.3067, step time: 0.4220\n",
      "batch: 12/17, train_dl_loss: 0.3625, train_bce_loss: 1.6733, train_bce_dl_loss: 0.3625, step time: 0.4231\n",
      "batch: 13/17, train_dl_loss: 0.3778, train_bce_loss: 1.6775, train_bce_dl_loss: 0.3778, step time: 0.3847\n",
      "batch: 14/17, train_dl_loss: 0.2884, train_bce_loss: 1.6805, train_bce_dl_loss: 0.2884, step time: 0.4336\n",
      "batch: 15/17, train_dl_loss: 0.3294, train_bce_loss: 1.6740, train_bce_dl_loss: 0.3294, step time: 0.3798\n",
      "batch: 16/17, train_dl_loss: 0.2712, train_bce_loss: 1.6806, train_bce_dl_loss: 0.2712, step time: 0.4144\n",
      "batch: 17/17, train_dl_loss: 0.3647, train_bce_loss: 1.6850, train_bce_dl_loss: 0.3647, step time: 0.1138\n",
      "LOSS train DiceLoss: 0.3221, LOSS train BCE: 1.6712, LOSS train BCE-DiceLoss: 0.3221, LOSS val DiceLoss: 0.4368, LOSS val BCE: 1.6676, LOSS val BCE-DiceLoss: 0.4368, METRIC val: 0.5284\n",
      "time consuming of epoch 293 is: 449.9687\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2762, train_bce_loss: 1.6665, train_bce_dl_loss: 0.2762, step time: 0.4445\n",
      "batch: 1/17, train_dl_loss: 0.3219, train_bce_loss: 1.6861, train_bce_dl_loss: 0.3219, step time: 0.3799\n",
      "batch: 2/17, train_dl_loss: 0.3171, train_bce_loss: 1.6654, train_bce_dl_loss: 0.3171, step time: 0.4222\n",
      "batch: 3/17, train_dl_loss: 0.3558, train_bce_loss: 1.6722, train_bce_dl_loss: 0.3558, step time: 0.4139\n",
      "batch: 4/17, train_dl_loss: 0.2785, train_bce_loss: 1.6558, train_bce_dl_loss: 0.2785, step time: 0.4326\n",
      "batch: 5/17, train_dl_loss: 0.3124, train_bce_loss: 1.6544, train_bce_dl_loss: 0.3124, step time: 0.3869\n",
      "batch: 6/17, train_dl_loss: 0.3411, train_bce_loss: 1.6787, train_bce_dl_loss: 0.3411, step time: 0.4228\n",
      "batch: 7/17, train_dl_loss: 0.2812, train_bce_loss: 1.6600, train_bce_dl_loss: 0.2812, step time: 0.3834\n",
      "batch: 8/17, train_dl_loss: 0.3047, train_bce_loss: 1.6678, train_bce_dl_loss: 0.3047, step time: 0.4210\n",
      "batch: 9/17, train_dl_loss: 0.3138, train_bce_loss: 1.6718, train_bce_dl_loss: 0.3138, step time: 0.3832\n",
      "batch: 10/17, train_dl_loss: 0.3802, train_bce_loss: 1.6749, train_bce_dl_loss: 0.3802, step time: 0.5119\n",
      "batch: 11/17, train_dl_loss: 0.2964, train_bce_loss: 1.6458, train_bce_dl_loss: 0.2964, step time: 0.4389\n",
      "batch: 12/17, train_dl_loss: 0.3102, train_bce_loss: 1.6567, train_bce_dl_loss: 0.3102, step time: 0.4270\n",
      "batch: 13/17, train_dl_loss: 0.4344, train_bce_loss: 1.6746, train_bce_dl_loss: 0.4344, step time: 0.3799\n",
      "batch: 14/17, train_dl_loss: 0.3330, train_bce_loss: 1.6854, train_bce_dl_loss: 0.3330, step time: 0.4340\n",
      "batch: 15/17, train_dl_loss: 0.2930, train_bce_loss: 1.6658, train_bce_dl_loss: 0.2930, step time: 0.3732\n",
      "batch: 16/17, train_dl_loss: 0.2639, train_bce_loss: 1.6797, train_bce_dl_loss: 0.2639, step time: 0.4272\n",
      "batch: 17/17, train_dl_loss: 0.2905, train_bce_loss: 1.6648, train_bce_dl_loss: 0.2905, step time: 0.1141\n",
      "LOSS train DiceLoss: 0.3169, LOSS train BCE: 1.6681, LOSS train BCE-DiceLoss: 0.3169, LOSS val DiceLoss: 0.4381, LOSS val BCE: 1.6692, LOSS val BCE-DiceLoss: 0.4381, METRIC val: 0.5257\n",
      "time consuming of epoch 294 is: 453.9172\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3148, train_bce_loss: 1.6758, train_bce_dl_loss: 0.3148, step time: 0.4397\n",
      "batch: 1/17, train_dl_loss: 0.3340, train_bce_loss: 1.6694, train_bce_dl_loss: 0.3340, step time: 0.3796\n",
      "batch: 2/17, train_dl_loss: 0.3221, train_bce_loss: 1.6807, train_bce_dl_loss: 0.3221, step time: 0.4712\n",
      "batch: 3/17, train_dl_loss: 0.3821, train_bce_loss: 1.6806, train_bce_dl_loss: 0.3821, step time: 0.3849\n",
      "batch: 4/17, train_dl_loss: 0.2410, train_bce_loss: 1.6659, train_bce_dl_loss: 0.2410, step time: 0.4256\n",
      "batch: 5/17, train_dl_loss: 0.2857, train_bce_loss: 1.6556, train_bce_dl_loss: 0.2857, step time: 0.3837\n",
      "batch: 6/17, train_dl_loss: 0.3548, train_bce_loss: 1.6760, train_bce_dl_loss: 0.3548, step time: 0.4253\n",
      "batch: 7/17, train_dl_loss: 0.2890, train_bce_loss: 1.6749, train_bce_dl_loss: 0.2890, step time: 0.3815\n",
      "batch: 8/17, train_dl_loss: 0.2889, train_bce_loss: 1.6682, train_bce_dl_loss: 0.2889, step time: 0.4277\n",
      "batch: 9/17, train_dl_loss: 0.2939, train_bce_loss: 1.6813, train_bce_dl_loss: 0.2939, step time: 0.3794\n",
      "batch: 10/17, train_dl_loss: 0.4047, train_bce_loss: 1.6731, train_bce_dl_loss: 0.4047, step time: 0.4351\n",
      "batch: 11/17, train_dl_loss: 0.3465, train_bce_loss: 1.6595, train_bce_dl_loss: 0.3465, step time: 0.3866\n",
      "batch: 12/17, train_dl_loss: 0.2922, train_bce_loss: 1.6686, train_bce_dl_loss: 0.2922, step time: 0.4273\n",
      "batch: 13/17, train_dl_loss: 0.4013, train_bce_loss: 1.6721, train_bce_dl_loss: 0.4013, step time: 0.4114\n",
      "batch: 14/17, train_dl_loss: 0.3042, train_bce_loss: 1.6741, train_bce_dl_loss: 0.3042, step time: 0.4223\n",
      "batch: 15/17, train_dl_loss: 0.3202, train_bce_loss: 1.6753, train_bce_dl_loss: 0.3202, step time: 0.3804\n",
      "batch: 16/17, train_dl_loss: 0.3140, train_bce_loss: 1.6751, train_bce_dl_loss: 0.3140, step time: 0.4166\n",
      "batch: 17/17, train_dl_loss: 0.2383, train_bce_loss: 1.6674, train_bce_dl_loss: 0.2383, step time: 0.1141\n",
      "LOSS train DiceLoss: 0.3182, LOSS train BCE: 1.6719, LOSS train BCE-DiceLoss: 0.3182, LOSS val DiceLoss: 0.4335, LOSS val BCE: 1.6659, LOSS val BCE-DiceLoss: 0.4335, METRIC val: 0.5303\n",
      "time consuming of epoch 295 is: 492.7668\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2825, train_bce_loss: 1.6709, train_bce_dl_loss: 0.2825, step time: 0.4287\n",
      "batch: 1/17, train_dl_loss: 0.2963, train_bce_loss: 1.6842, train_bce_dl_loss: 0.2963, step time: 0.3781\n",
      "batch: 2/17, train_dl_loss: 0.2885, train_bce_loss: 1.6789, train_bce_dl_loss: 0.2885, step time: 0.4376\n",
      "batch: 3/17, train_dl_loss: 0.3736, train_bce_loss: 1.6693, train_bce_dl_loss: 0.3736, step time: 0.3797\n",
      "batch: 4/17, train_dl_loss: 0.2628, train_bce_loss: 1.6697, train_bce_dl_loss: 0.2628, step time: 0.4405\n",
      "batch: 5/17, train_dl_loss: 0.2971, train_bce_loss: 1.6736, train_bce_dl_loss: 0.2971, step time: 0.3908\n",
      "batch: 6/17, train_dl_loss: 0.3545, train_bce_loss: 1.6849, train_bce_dl_loss: 0.3545, step time: 0.4335\n",
      "batch: 7/17, train_dl_loss: 0.2697, train_bce_loss: 1.6727, train_bce_dl_loss: 0.2697, step time: 0.3804\n",
      "batch: 8/17, train_dl_loss: 0.3173, train_bce_loss: 1.6491, train_bce_dl_loss: 0.3173, step time: 0.4292\n",
      "batch: 9/17, train_dl_loss: 0.2921, train_bce_loss: 1.6720, train_bce_dl_loss: 0.2921, step time: 0.3923\n",
      "batch: 10/17, train_dl_loss: 0.3869, train_bce_loss: 1.6694, train_bce_dl_loss: 0.3869, step time: 0.4307\n",
      "batch: 11/17, train_dl_loss: 0.3072, train_bce_loss: 1.6484, train_bce_dl_loss: 0.3072, step time: 0.4436\n",
      "batch: 12/17, train_dl_loss: 0.3119, train_bce_loss: 1.6528, train_bce_dl_loss: 0.3119, step time: 0.4424\n",
      "batch: 13/17, train_dl_loss: 0.4158, train_bce_loss: 1.6742, train_bce_dl_loss: 0.4158, step time: 0.3852\n",
      "batch: 14/17, train_dl_loss: 0.3525, train_bce_loss: 1.6796, train_bce_dl_loss: 0.3525, step time: 0.4227\n",
      "batch: 15/17, train_dl_loss: 0.2681, train_bce_loss: 1.6707, train_bce_dl_loss: 0.2681, step time: 0.3812\n",
      "batch: 16/17, train_dl_loss: 0.3075, train_bce_loss: 1.6807, train_bce_dl_loss: 0.3075, step time: 0.4214\n",
      "batch: 17/17, train_dl_loss: 0.2218, train_bce_loss: 1.6847, train_bce_dl_loss: 0.2218, step time: 0.1131\n",
      "LOSS train DiceLoss: 0.3115, LOSS train BCE: 1.6714, LOSS train BCE-DiceLoss: 0.3115, LOSS val DiceLoss: 0.4449, LOSS val BCE: 1.6702, LOSS val BCE-DiceLoss: 0.4449, METRIC val: 0.5212\n",
      "time consuming of epoch 296 is: 452.4646\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2867, train_bce_loss: 1.6586, train_bce_dl_loss: 0.2867, step time: 0.4343\n",
      "batch: 1/17, train_dl_loss: 0.2656, train_bce_loss: 1.6777, train_bce_dl_loss: 0.2656, step time: 0.3767\n",
      "batch: 2/17, train_dl_loss: 0.3421, train_bce_loss: 1.6742, train_bce_dl_loss: 0.3421, step time: 0.4208\n",
      "batch: 3/17, train_dl_loss: 0.3672, train_bce_loss: 1.6688, train_bce_dl_loss: 0.3672, step time: 0.3792\n",
      "batch: 4/17, train_dl_loss: 0.2615, train_bce_loss: 1.6591, train_bce_dl_loss: 0.2615, step time: 0.4204\n",
      "batch: 5/17, train_dl_loss: 0.2917, train_bce_loss: 1.6576, train_bce_dl_loss: 0.2917, step time: 0.3799\n",
      "batch: 6/17, train_dl_loss: 0.3390, train_bce_loss: 1.6842, train_bce_dl_loss: 0.3390, step time: 0.4248\n",
      "batch: 7/17, train_dl_loss: 0.2844, train_bce_loss: 1.6717, train_bce_dl_loss: 0.2844, step time: 0.3819\n",
      "batch: 8/17, train_dl_loss: 0.2895, train_bce_loss: 1.6617, train_bce_dl_loss: 0.2895, step time: 0.4271\n",
      "batch: 9/17, train_dl_loss: 0.3143, train_bce_loss: 1.6669, train_bce_dl_loss: 0.3143, step time: 0.3827\n",
      "batch: 10/17, train_dl_loss: 0.4027, train_bce_loss: 1.6673, train_bce_dl_loss: 0.4027, step time: 0.4445\n",
      "batch: 11/17, train_dl_loss: 0.2657, train_bce_loss: 1.6437, train_bce_dl_loss: 0.2657, step time: 0.4403\n",
      "batch: 12/17, train_dl_loss: 0.2974, train_bce_loss: 1.6682, train_bce_dl_loss: 0.2974, step time: 0.4257\n",
      "batch: 13/17, train_dl_loss: 0.3942, train_bce_loss: 1.6766, train_bce_dl_loss: 0.3942, step time: 0.3843\n",
      "batch: 14/17, train_dl_loss: 0.3305, train_bce_loss: 1.6820, train_bce_dl_loss: 0.3305, step time: 0.4291\n",
      "batch: 15/17, train_dl_loss: 0.2832, train_bce_loss: 1.6762, train_bce_dl_loss: 0.2832, step time: 0.3812\n",
      "batch: 16/17, train_dl_loss: 0.2778, train_bce_loss: 1.6717, train_bce_dl_loss: 0.2778, step time: 0.4232\n",
      "batch: 17/17, train_dl_loss: 0.2284, train_bce_loss: 1.6808, train_bce_dl_loss: 0.2284, step time: 0.1140\n",
      "LOSS train DiceLoss: 0.3068, LOSS train BCE: 1.6693, LOSS train BCE-DiceLoss: 0.3068, LOSS val DiceLoss: 0.4301, LOSS val BCE: 1.6677, LOSS val BCE-DiceLoss: 0.4301, METRIC val: 0.5363\n",
      "time consuming of epoch 297 is: 444.5012\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2990, train_bce_loss: 1.6833, train_bce_dl_loss: 0.2990, step time: 0.4149\n",
      "batch: 1/17, train_dl_loss: 0.3353, train_bce_loss: 1.6770, train_bce_dl_loss: 0.3353, step time: 0.3737\n",
      "batch: 2/17, train_dl_loss: 0.3194, train_bce_loss: 1.6583, train_bce_dl_loss: 0.3194, step time: 0.4176\n",
      "batch: 3/17, train_dl_loss: 0.3824, train_bce_loss: 1.6725, train_bce_dl_loss: 0.3824, step time: 0.3766\n",
      "batch: 4/17, train_dl_loss: 0.2620, train_bce_loss: 1.6737, train_bce_dl_loss: 0.2620, step time: 0.4303\n",
      "batch: 5/17, train_dl_loss: 0.2573, train_bce_loss: 1.6634, train_bce_dl_loss: 0.2573, step time: 0.3879\n",
      "batch: 6/17, train_dl_loss: 0.3521, train_bce_loss: 1.6824, train_bce_dl_loss: 0.3521, step time: 0.4409\n",
      "batch: 7/17, train_dl_loss: 0.2642, train_bce_loss: 1.6561, train_bce_dl_loss: 0.2642, step time: 0.3886\n",
      "batch: 8/17, train_dl_loss: 0.3091, train_bce_loss: 1.6594, train_bce_dl_loss: 0.3091, step time: 0.4261\n",
      "batch: 9/17, train_dl_loss: 0.3171, train_bce_loss: 1.6687, train_bce_dl_loss: 0.3171, step time: 0.3863\n",
      "batch: 10/17, train_dl_loss: 0.3867, train_bce_loss: 1.6665, train_bce_dl_loss: 0.3867, step time: 0.4288\n",
      "batch: 11/17, train_dl_loss: 0.3641, train_bce_loss: 1.6578, train_bce_dl_loss: 0.3641, step time: 0.4320\n",
      "batch: 12/17, train_dl_loss: 0.2756, train_bce_loss: 1.6609, train_bce_dl_loss: 0.2756, step time: 0.4270\n",
      "batch: 13/17, train_dl_loss: 0.4029, train_bce_loss: 1.6893, train_bce_dl_loss: 0.4029, step time: 0.4197\n",
      "batch: 14/17, train_dl_loss: 0.2963, train_bce_loss: 1.6866, train_bce_dl_loss: 0.2963, step time: 0.4371\n",
      "batch: 15/17, train_dl_loss: 0.2774, train_bce_loss: 1.6662, train_bce_dl_loss: 0.2774, step time: 0.3830\n",
      "batch: 16/17, train_dl_loss: 0.3143, train_bce_loss: 1.6814, train_bce_dl_loss: 0.3143, step time: 0.4261\n",
      "batch: 17/17, train_dl_loss: 0.2382, train_bce_loss: 1.6633, train_bce_dl_loss: 0.2382, step time: 0.1147\n",
      "LOSS train DiceLoss: 0.3141, LOSS train BCE: 1.6704, LOSS train BCE-DiceLoss: 0.3141, LOSS val DiceLoss: 0.4408, LOSS val BCE: 1.6678, LOSS val BCE-DiceLoss: 0.4408, METRIC val: 0.5230\n",
      "time consuming of epoch 298 is: 542.6939\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3053, train_bce_loss: 1.6667, train_bce_dl_loss: 0.3053, step time: 0.4350\n",
      "batch: 1/17, train_dl_loss: 0.3159, train_bce_loss: 1.6653, train_bce_dl_loss: 0.3159, step time: 0.3775\n",
      "batch: 2/17, train_dl_loss: 0.3016, train_bce_loss: 1.6650, train_bce_dl_loss: 0.3016, step time: 0.4198\n",
      "batch: 3/17, train_dl_loss: 0.3772, train_bce_loss: 1.6681, train_bce_dl_loss: 0.3772, step time: 0.3875\n",
      "batch: 4/17, train_dl_loss: 0.2691, train_bce_loss: 1.6637, train_bce_dl_loss: 0.2691, step time: 0.4211\n",
      "batch: 5/17, train_dl_loss: 0.2691, train_bce_loss: 1.6736, train_bce_dl_loss: 0.2691, step time: 0.3828\n",
      "batch: 6/17, train_dl_loss: 0.3798, train_bce_loss: 1.6743, train_bce_dl_loss: 0.3798, step time: 0.4415\n",
      "batch: 7/17, train_dl_loss: 0.3070, train_bce_loss: 1.6745, train_bce_dl_loss: 0.3070, step time: 0.4391\n",
      "batch: 8/17, train_dl_loss: 0.3162, train_bce_loss: 1.6663, train_bce_dl_loss: 0.3162, step time: 0.4358\n",
      "batch: 9/17, train_dl_loss: 0.2718, train_bce_loss: 1.6771, train_bce_dl_loss: 0.2718, step time: 0.4316\n",
      "batch: 10/17, train_dl_loss: 0.3858, train_bce_loss: 1.6653, train_bce_dl_loss: 0.3858, step time: 0.4389\n",
      "batch: 11/17, train_dl_loss: 0.3140, train_bce_loss: 1.6642, train_bce_dl_loss: 0.3140, step time: 0.4415\n",
      "batch: 12/17, train_dl_loss: 0.2983, train_bce_loss: 1.6635, train_bce_dl_loss: 0.2983, step time: 0.4285\n",
      "batch: 13/17, train_dl_loss: 0.4479, train_bce_loss: 1.6816, train_bce_dl_loss: 0.4479, step time: 0.3809\n",
      "batch: 14/17, train_dl_loss: 0.3080, train_bce_loss: 1.6924, train_bce_dl_loss: 0.3080, step time: 0.4384\n",
      "batch: 15/17, train_dl_loss: 0.2889, train_bce_loss: 1.6803, train_bce_dl_loss: 0.2889, step time: 0.3766\n",
      "batch: 16/17, train_dl_loss: 0.3296, train_bce_loss: 1.6909, train_bce_dl_loss: 0.3296, step time: 0.4118\n",
      "batch: 17/17, train_dl_loss: 0.2383, train_bce_loss: 1.6880, train_bce_dl_loss: 0.2383, step time: 0.1136\n",
      "LOSS train DiceLoss: 0.3180, LOSS train BCE: 1.6734, LOSS train BCE-DiceLoss: 0.3180, LOSS val DiceLoss: 0.4309, LOSS val BCE: 1.6688, LOSS val BCE-DiceLoss: 0.4309, METRIC val: 0.5334\n",
      "time consuming of epoch 299 is: 474.2009\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2949, train_bce_loss: 1.6583, train_bce_dl_loss: 0.2949, step time: 0.4395\n",
      "batch: 1/17, train_dl_loss: 0.2829, train_bce_loss: 1.6826, train_bce_dl_loss: 0.2829, step time: 0.3805\n",
      "batch: 2/17, train_dl_loss: 0.3328, train_bce_loss: 1.6742, train_bce_dl_loss: 0.3328, step time: 0.4132\n",
      "batch: 3/17, train_dl_loss: 0.3828, train_bce_loss: 1.6683, train_bce_dl_loss: 0.3828, step time: 0.3736\n",
      "batch: 4/17, train_dl_loss: 0.2539, train_bce_loss: 1.6645, train_bce_dl_loss: 0.2539, step time: 0.4539\n",
      "batch: 5/17, train_dl_loss: 0.2992, train_bce_loss: 1.6796, train_bce_dl_loss: 0.2992, step time: 0.3880\n",
      "batch: 6/17, train_dl_loss: 0.3758, train_bce_loss: 1.6772, train_bce_dl_loss: 0.3758, step time: 0.4357\n",
      "batch: 7/17, train_dl_loss: 0.2635, train_bce_loss: 1.6602, train_bce_dl_loss: 0.2635, step time: 0.4233\n",
      "batch: 8/17, train_dl_loss: 0.3078, train_bce_loss: 1.6565, train_bce_dl_loss: 0.3078, step time: 0.4350\n",
      "batch: 9/17, train_dl_loss: 0.3749, train_bce_loss: 1.6872, train_bce_dl_loss: 0.3749, step time: 0.4260\n",
      "batch: 10/17, train_dl_loss: 0.3738, train_bce_loss: 1.6689, train_bce_dl_loss: 0.3738, step time: 0.4288\n",
      "batch: 11/17, train_dl_loss: 0.2992, train_bce_loss: 1.6433, train_bce_dl_loss: 0.2992, step time: 0.4265\n",
      "batch: 12/17, train_dl_loss: 0.3216, train_bce_loss: 1.6601, train_bce_dl_loss: 0.3216, step time: 0.4324\n",
      "batch: 13/17, train_dl_loss: 0.4373, train_bce_loss: 1.6817, train_bce_dl_loss: 0.4373, step time: 0.4358\n",
      "batch: 14/17, train_dl_loss: 0.3580, train_bce_loss: 1.6974, train_bce_dl_loss: 0.3580, step time: 0.4283\n",
      "batch: 15/17, train_dl_loss: 0.2816, train_bce_loss: 1.6884, train_bce_dl_loss: 0.2816, step time: 0.3777\n",
      "batch: 16/17, train_dl_loss: 0.2864, train_bce_loss: 1.6784, train_bce_dl_loss: 0.2864, step time: 0.4184\n",
      "batch: 17/17, train_dl_loss: 0.2914, train_bce_loss: 1.6623, train_bce_dl_loss: 0.2914, step time: 0.1150\n",
      "LOSS train DiceLoss: 0.3232, LOSS train BCE: 1.6716, LOSS train BCE-DiceLoss: 0.3232, LOSS val DiceLoss: 0.4499, LOSS val BCE: 1.6648, LOSS val BCE-DiceLoss: 0.4499, METRIC val: 0.5128\n",
      "time consuming of epoch 300 is: 454.5189\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2824, train_bce_loss: 1.6690, train_bce_dl_loss: 0.2824, step time: 0.4332\n",
      "batch: 1/17, train_dl_loss: 0.3031, train_bce_loss: 1.6588, train_bce_dl_loss: 0.3031, step time: 0.3804\n",
      "batch: 2/17, train_dl_loss: 0.2937, train_bce_loss: 1.6600, train_bce_dl_loss: 0.2937, step time: 0.4260\n",
      "batch: 3/17, train_dl_loss: 0.3962, train_bce_loss: 1.6648, train_bce_dl_loss: 0.3962, step time: 0.3788\n",
      "batch: 4/17, train_dl_loss: 0.3175, train_bce_loss: 1.6720, train_bce_dl_loss: 0.3175, step time: 0.4451\n",
      "batch: 5/17, train_dl_loss: 0.2786, train_bce_loss: 1.6634, train_bce_dl_loss: 0.2786, step time: 0.4427\n",
      "batch: 6/17, train_dl_loss: 0.3300, train_bce_loss: 1.6770, train_bce_dl_loss: 0.3300, step time: 0.4761\n",
      "batch: 7/17, train_dl_loss: 0.3105, train_bce_loss: 1.6585, train_bce_dl_loss: 0.3105, step time: 0.4582\n",
      "batch: 8/17, train_dl_loss: 0.3101, train_bce_loss: 1.6635, train_bce_dl_loss: 0.3101, step time: 0.4138\n",
      "batch: 9/17, train_dl_loss: 0.2992, train_bce_loss: 1.6728, train_bce_dl_loss: 0.2992, step time: 0.3824\n",
      "batch: 10/17, train_dl_loss: 0.3810, train_bce_loss: 1.6674, train_bce_dl_loss: 0.3810, step time: 0.4345\n",
      "batch: 11/17, train_dl_loss: 0.2737, train_bce_loss: 1.6668, train_bce_dl_loss: 0.2737, step time: 0.3947\n",
      "batch: 12/17, train_dl_loss: 0.2837, train_bce_loss: 1.6620, train_bce_dl_loss: 0.2837, step time: 0.4190\n",
      "batch: 13/17, train_dl_loss: 0.4530, train_bce_loss: 1.6786, train_bce_dl_loss: 0.4530, step time: 0.3791\n",
      "batch: 14/17, train_dl_loss: 0.3327, train_bce_loss: 1.6818, train_bce_dl_loss: 0.3327, step time: 0.4469\n",
      "batch: 15/17, train_dl_loss: 0.3225, train_bce_loss: 1.6651, train_bce_dl_loss: 0.3225, step time: 0.3885\n",
      "batch: 16/17, train_dl_loss: 0.2931, train_bce_loss: 1.6747, train_bce_dl_loss: 0.2931, step time: 0.4167\n",
      "batch: 17/17, train_dl_loss: 0.3039, train_bce_loss: 1.6637, train_bce_dl_loss: 0.3039, step time: 0.1137\n",
      "LOSS train DiceLoss: 0.3203, LOSS train BCE: 1.6678, LOSS train BCE-DiceLoss: 0.3203, LOSS val DiceLoss: 0.4374, LOSS val BCE: 1.6682, LOSS val BCE-DiceLoss: 0.4374, METRIC val: 0.5278\n",
      "time consuming of epoch 301 is: 449.6715\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3009, train_bce_loss: 1.6698, train_bce_dl_loss: 0.3009, step time: 0.4271\n",
      "batch: 1/17, train_dl_loss: 0.3066, train_bce_loss: 1.6720, train_bce_dl_loss: 0.3066, step time: 0.3884\n",
      "batch: 2/17, train_dl_loss: 0.3021, train_bce_loss: 1.6647, train_bce_dl_loss: 0.3021, step time: 0.4363\n",
      "batch: 3/17, train_dl_loss: 0.3960, train_bce_loss: 1.6740, train_bce_dl_loss: 0.3960, step time: 0.3801\n",
      "batch: 4/17, train_dl_loss: 0.2674, train_bce_loss: 1.6711, train_bce_dl_loss: 0.2674, step time: 0.4167\n",
      "batch: 5/17, train_dl_loss: 0.3065, train_bce_loss: 1.6575, train_bce_dl_loss: 0.3065, step time: 0.3889\n",
      "batch: 6/17, train_dl_loss: 0.4136, train_bce_loss: 1.6720, train_bce_dl_loss: 0.4136, step time: 0.4374\n",
      "batch: 7/17, train_dl_loss: 0.2842, train_bce_loss: 1.6523, train_bce_dl_loss: 0.2842, step time: 0.3796\n",
      "batch: 8/17, train_dl_loss: 0.2863, train_bce_loss: 1.6603, train_bce_dl_loss: 0.2863, step time: 0.4308\n",
      "batch: 9/17, train_dl_loss: 0.3087, train_bce_loss: 1.6670, train_bce_dl_loss: 0.3087, step time: 0.3767\n",
      "batch: 10/17, train_dl_loss: 0.4599, train_bce_loss: 1.6736, train_bce_dl_loss: 0.4599, step time: 0.4310\n",
      "batch: 11/17, train_dl_loss: 0.4243, train_bce_loss: 1.6808, train_bce_dl_loss: 0.4243, step time: 0.3928\n",
      "batch: 12/17, train_dl_loss: 0.2958, train_bce_loss: 1.6742, train_bce_dl_loss: 0.2958, step time: 0.4205\n",
      "batch: 13/17, train_dl_loss: 0.4299, train_bce_loss: 1.6830, train_bce_dl_loss: 0.4299, step time: 0.3788\n",
      "batch: 14/17, train_dl_loss: 0.3069, train_bce_loss: 1.6932, train_bce_dl_loss: 0.3069, step time: 0.4334\n",
      "batch: 15/17, train_dl_loss: 0.2889, train_bce_loss: 1.6919, train_bce_dl_loss: 0.2889, step time: 0.3797\n",
      "batch: 16/17, train_dl_loss: 0.2923, train_bce_loss: 1.6881, train_bce_dl_loss: 0.2923, step time: 0.4109\n",
      "batch: 17/17, train_dl_loss: 0.2480, train_bce_loss: 1.6899, train_bce_dl_loss: 0.2480, step time: 0.1132\n",
      "LOSS train DiceLoss: 0.3288, LOSS train BCE: 1.6742, LOSS train BCE-DiceLoss: 0.3288, LOSS val DiceLoss: 0.4366, LOSS val BCE: 1.6734, LOSS val BCE-DiceLoss: 0.4366, METRIC val: 0.5268\n",
      "time consuming of epoch 302 is: 458.3353\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3754, train_bce_loss: 1.6805, train_bce_dl_loss: 0.3754, step time: 0.4413\n",
      "batch: 1/17, train_dl_loss: 0.3099, train_bce_loss: 1.6851, train_bce_dl_loss: 0.3099, step time: 0.3761\n",
      "batch: 2/17, train_dl_loss: 0.2886, train_bce_loss: 1.6879, train_bce_dl_loss: 0.2886, step time: 0.4169\n",
      "batch: 3/17, train_dl_loss: 0.4790, train_bce_loss: 1.6854, train_bce_dl_loss: 0.4790, step time: 0.3752\n",
      "batch: 4/17, train_dl_loss: 0.2931, train_bce_loss: 1.6679, train_bce_dl_loss: 0.2931, step time: 0.4269\n",
      "batch: 5/17, train_dl_loss: 0.2699, train_bce_loss: 1.6791, train_bce_dl_loss: 0.2699, step time: 0.3877\n",
      "batch: 6/17, train_dl_loss: 0.3458, train_bce_loss: 1.6871, train_bce_dl_loss: 0.3458, step time: 0.4300\n",
      "batch: 7/17, train_dl_loss: 0.2799, train_bce_loss: 1.6718, train_bce_dl_loss: 0.2799, step time: 0.3774\n",
      "batch: 8/17, train_dl_loss: 0.2805, train_bce_loss: 1.6595, train_bce_dl_loss: 0.2805, step time: 0.4315\n",
      "batch: 9/17, train_dl_loss: 0.3041, train_bce_loss: 1.6805, train_bce_dl_loss: 0.3041, step time: 0.3875\n",
      "batch: 10/17, train_dl_loss: 0.3695, train_bce_loss: 1.6600, train_bce_dl_loss: 0.3695, step time: 0.4320\n",
      "batch: 11/17, train_dl_loss: 0.3189, train_bce_loss: 1.6513, train_bce_dl_loss: 0.3189, step time: 0.4420\n",
      "batch: 12/17, train_dl_loss: 0.3536, train_bce_loss: 1.6751, train_bce_dl_loss: 0.3536, step time: 0.4242\n",
      "batch: 13/17, train_dl_loss: 0.4158, train_bce_loss: 1.6807, train_bce_dl_loss: 0.4158, step time: 0.3782\n",
      "batch: 14/17, train_dl_loss: 0.3596, train_bce_loss: 1.6956, train_bce_dl_loss: 0.3596, step time: 0.4433\n",
      "batch: 15/17, train_dl_loss: 0.2703, train_bce_loss: 1.6868, train_bce_dl_loss: 0.2703, step time: 0.3877\n",
      "batch: 16/17, train_dl_loss: 0.2733, train_bce_loss: 1.6859, train_bce_dl_loss: 0.2733, step time: 0.4136\n",
      "batch: 17/17, train_dl_loss: 0.2338, train_bce_loss: 1.6792, train_bce_dl_loss: 0.2338, step time: 0.1130\n",
      "LOSS train DiceLoss: 0.3234, LOSS train BCE: 1.6777, LOSS train BCE-DiceLoss: 0.3234, LOSS val DiceLoss: 0.4728, LOSS val BCE: 1.6716, LOSS val BCE-DiceLoss: 0.4728, METRIC val: 0.4889\n",
      "time consuming of epoch 303 is: 451.5118\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3175, train_bce_loss: 1.6722, train_bce_dl_loss: 0.3175, step time: 0.4343\n",
      "batch: 1/17, train_dl_loss: 0.3051, train_bce_loss: 1.6800, train_bce_dl_loss: 0.3051, step time: 0.3751\n",
      "batch: 2/17, train_dl_loss: 0.3254, train_bce_loss: 1.6845, train_bce_dl_loss: 0.3254, step time: 0.4249\n",
      "batch: 3/17, train_dl_loss: 0.3538, train_bce_loss: 1.6713, train_bce_dl_loss: 0.3538, step time: 0.3823\n",
      "batch: 4/17, train_dl_loss: 0.3043, train_bce_loss: 1.6497, train_bce_dl_loss: 0.3043, step time: 0.4305\n",
      "batch: 5/17, train_dl_loss: 0.2674, train_bce_loss: 1.6541, train_bce_dl_loss: 0.2674, step time: 0.3874\n",
      "batch: 6/17, train_dl_loss: 0.3875, train_bce_loss: 1.6674, train_bce_dl_loss: 0.3875, step time: 0.4355\n",
      "batch: 7/17, train_dl_loss: 0.2759, train_bce_loss: 1.6594, train_bce_dl_loss: 0.2759, step time: 0.3810\n",
      "batch: 8/17, train_dl_loss: 0.2813, train_bce_loss: 1.6682, train_bce_dl_loss: 0.2813, step time: 0.4400\n",
      "batch: 9/17, train_dl_loss: 0.4093, train_bce_loss: 1.6743, train_bce_dl_loss: 0.4093, step time: 0.3812\n",
      "batch: 10/17, train_dl_loss: 0.4027, train_bce_loss: 1.6645, train_bce_dl_loss: 0.4027, step time: 0.4469\n",
      "batch: 11/17, train_dl_loss: 0.3198, train_bce_loss: 1.6494, train_bce_dl_loss: 0.3198, step time: 0.3863\n",
      "batch: 12/17, train_dl_loss: 0.3330, train_bce_loss: 1.6636, train_bce_dl_loss: 0.3330, step time: 0.4244\n",
      "batch: 13/17, train_dl_loss: 0.5121, train_bce_loss: 1.6624, train_bce_dl_loss: 0.5121, step time: 0.3842\n",
      "batch: 14/17, train_dl_loss: 0.3205, train_bce_loss: 1.6732, train_bce_dl_loss: 0.3205, step time: 0.4241\n",
      "batch: 15/17, train_dl_loss: 0.3453, train_bce_loss: 1.6715, train_bce_dl_loss: 0.3453, step time: 0.3824\n",
      "batch: 16/17, train_dl_loss: 0.3187, train_bce_loss: 1.6728, train_bce_dl_loss: 0.3187, step time: 0.4225\n",
      "batch: 17/17, train_dl_loss: 0.2395, train_bce_loss: 1.6616, train_bce_dl_loss: 0.2395, step time: 0.1154\n",
      "LOSS train DiceLoss: 0.3344, LOSS train BCE: 1.6667, LOSS train BCE-DiceLoss: 0.3344, LOSS val DiceLoss: 0.4377, LOSS val BCE: 1.6697, LOSS val BCE-DiceLoss: 0.4377, METRIC val: 0.5265\n",
      "time consuming of epoch 304 is: 443.8115\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3203, train_bce_loss: 1.6631, train_bce_dl_loss: 0.3203, step time: 0.4262\n",
      "batch: 1/17, train_dl_loss: 0.2927, train_bce_loss: 1.6703, train_bce_dl_loss: 0.2927, step time: 0.3846\n",
      "batch: 2/17, train_dl_loss: 0.3537, train_bce_loss: 1.6624, train_bce_dl_loss: 0.3537, step time: 0.4265\n",
      "batch: 3/17, train_dl_loss: 0.3479, train_bce_loss: 1.6754, train_bce_dl_loss: 0.3479, step time: 0.3821\n",
      "batch: 4/17, train_dl_loss: 0.2702, train_bce_loss: 1.6694, train_bce_dl_loss: 0.2702, step time: 0.4395\n",
      "batch: 5/17, train_dl_loss: 0.3204, train_bce_loss: 1.6723, train_bce_dl_loss: 0.3204, step time: 0.3859\n",
      "batch: 6/17, train_dl_loss: 0.4367, train_bce_loss: 1.6829, train_bce_dl_loss: 0.4367, step time: 0.4390\n",
      "batch: 7/17, train_dl_loss: 0.2723, train_bce_loss: 1.6642, train_bce_dl_loss: 0.2723, step time: 0.4229\n",
      "batch: 8/17, train_dl_loss: 0.3088, train_bce_loss: 1.6519, train_bce_dl_loss: 0.3088, step time: 0.4257\n",
      "batch: 9/17, train_dl_loss: 0.3197, train_bce_loss: 1.6673, train_bce_dl_loss: 0.3197, step time: 0.4178\n",
      "batch: 10/17, train_dl_loss: 0.4290, train_bce_loss: 1.6803, train_bce_dl_loss: 0.4290, step time: 0.4171\n",
      "batch: 11/17, train_dl_loss: 0.2993, train_bce_loss: 1.6617, train_bce_dl_loss: 0.2993, step time: 0.4456\n",
      "batch: 12/17, train_dl_loss: 0.3562, train_bce_loss: 1.6726, train_bce_dl_loss: 0.3562, step time: 0.4401\n",
      "batch: 13/17, train_dl_loss: 0.4486, train_bce_loss: 1.6898, train_bce_dl_loss: 0.4486, step time: 0.3808\n",
      "batch: 14/17, train_dl_loss: 0.3846, train_bce_loss: 1.6932, train_bce_dl_loss: 0.3846, step time: 0.4416\n",
      "batch: 15/17, train_dl_loss: 0.3006, train_bce_loss: 1.6716, train_bce_dl_loss: 0.3006, step time: 0.3859\n",
      "batch: 16/17, train_dl_loss: 0.2812, train_bce_loss: 1.6835, train_bce_dl_loss: 0.2812, step time: 0.4349\n",
      "batch: 17/17, train_dl_loss: 0.2971, train_bce_loss: 1.6725, train_bce_dl_loss: 0.2971, step time: 0.1132\n",
      "LOSS train DiceLoss: 0.3355, LOSS train BCE: 1.6725, LOSS train BCE-DiceLoss: 0.3355, LOSS val DiceLoss: 0.4432, LOSS val BCE: 1.6721, LOSS val BCE-DiceLoss: 0.4432, METRIC val: 0.5197\n",
      "time consuming of epoch 305 is: 437.8659\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3753, train_bce_loss: 1.6717, train_bce_dl_loss: 0.3753, step time: 0.4290\n",
      "batch: 1/17, train_dl_loss: 0.2987, train_bce_loss: 1.6824, train_bce_dl_loss: 0.2987, step time: 0.3788\n",
      "batch: 2/17, train_dl_loss: 0.3507, train_bce_loss: 1.6852, train_bce_dl_loss: 0.3507, step time: 0.4327\n",
      "batch: 3/17, train_dl_loss: 0.4329, train_bce_loss: 1.6776, train_bce_dl_loss: 0.4329, step time: 0.3750\n",
      "batch: 4/17, train_dl_loss: 0.2829, train_bce_loss: 1.6738, train_bce_dl_loss: 0.2829, step time: 0.4411\n",
      "batch: 5/17, train_dl_loss: 0.3063, train_bce_loss: 1.6593, train_bce_dl_loss: 0.3063, step time: 0.3809\n",
      "batch: 6/17, train_dl_loss: 0.3226, train_bce_loss: 1.6809, train_bce_dl_loss: 0.3226, step time: 0.4300\n",
      "batch: 7/17, train_dl_loss: 0.2763, train_bce_loss: 1.6591, train_bce_dl_loss: 0.2763, step time: 0.3803\n",
      "batch: 8/17, train_dl_loss: 0.3006, train_bce_loss: 1.6745, train_bce_dl_loss: 0.3006, step time: 0.4162\n",
      "batch: 9/17, train_dl_loss: 0.2902, train_bce_loss: 1.6892, train_bce_dl_loss: 0.2902, step time: 0.3777\n",
      "batch: 10/17, train_dl_loss: 0.3950, train_bce_loss: 1.6620, train_bce_dl_loss: 0.3950, step time: 0.4280\n",
      "batch: 11/17, train_dl_loss: 0.3485, train_bce_loss: 1.6596, train_bce_dl_loss: 0.3485, step time: 0.4368\n",
      "batch: 12/17, train_dl_loss: 0.2868, train_bce_loss: 1.6688, train_bce_dl_loss: 0.2868, step time: 0.4300\n",
      "batch: 13/17, train_dl_loss: 0.4236, train_bce_loss: 1.6815, train_bce_dl_loss: 0.4236, step time: 0.4298\n",
      "batch: 14/17, train_dl_loss: 0.3261, train_bce_loss: 1.6808, train_bce_dl_loss: 0.3261, step time: 0.4224\n",
      "batch: 15/17, train_dl_loss: 0.2794, train_bce_loss: 1.6854, train_bce_dl_loss: 0.2794, step time: 0.3803\n",
      "batch: 16/17, train_dl_loss: 0.2627, train_bce_loss: 1.6903, train_bce_dl_loss: 0.2627, step time: 0.4233\n",
      "batch: 17/17, train_dl_loss: 0.2356, train_bce_loss: 1.6950, train_bce_dl_loss: 0.2356, step time: 0.1144\n",
      "LOSS train DiceLoss: 0.3219, LOSS train BCE: 1.6765, LOSS train BCE-DiceLoss: 0.3219, LOSS val DiceLoss: 0.4359, LOSS val BCE: 1.6741, LOSS val BCE-DiceLoss: 0.4359, METRIC val: 0.5290\n",
      "time consuming of epoch 306 is: 523.9438\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2861, train_bce_loss: 1.6576, train_bce_dl_loss: 0.2861, step time: 0.4371\n",
      "batch: 1/17, train_dl_loss: 0.2991, train_bce_loss: 1.6707, train_bce_dl_loss: 0.2991, step time: 0.3867\n",
      "batch: 2/17, train_dl_loss: 0.3016, train_bce_loss: 1.6810, train_bce_dl_loss: 0.3016, step time: 0.4125\n",
      "batch: 3/17, train_dl_loss: 0.3581, train_bce_loss: 1.6886, train_bce_dl_loss: 0.3581, step time: 0.3745\n",
      "batch: 4/17, train_dl_loss: 0.2775, train_bce_loss: 1.6681, train_bce_dl_loss: 0.2775, step time: 0.4449\n",
      "batch: 5/17, train_dl_loss: 0.2991, train_bce_loss: 1.6830, train_bce_dl_loss: 0.2991, step time: 0.3990\n",
      "batch: 6/17, train_dl_loss: 0.3317, train_bce_loss: 1.6892, train_bce_dl_loss: 0.3317, step time: 0.4178\n",
      "batch: 7/17, train_dl_loss: 0.3084, train_bce_loss: 1.6522, train_bce_dl_loss: 0.3084, step time: 0.3801\n",
      "batch: 8/17, train_dl_loss: 0.2863, train_bce_loss: 1.6656, train_bce_dl_loss: 0.2863, step time: 0.4300\n",
      "batch: 9/17, train_dl_loss: 0.3088, train_bce_loss: 1.6749, train_bce_dl_loss: 0.3088, step time: 0.3840\n",
      "batch: 10/17, train_dl_loss: 0.3824, train_bce_loss: 1.6581, train_bce_dl_loss: 0.3824, step time: 0.4165\n",
      "batch: 11/17, train_dl_loss: 0.2734, train_bce_loss: 1.6858, train_bce_dl_loss: 0.2734, step time: 0.4498\n",
      "batch: 12/17, train_dl_loss: 0.3399, train_bce_loss: 1.6841, train_bce_dl_loss: 0.3399, step time: 0.4337\n",
      "batch: 13/17, train_dl_loss: 0.3526, train_bce_loss: 1.6887, train_bce_dl_loss: 0.3526, step time: 0.3798\n",
      "batch: 14/17, train_dl_loss: 0.3296, train_bce_loss: 1.6866, train_bce_dl_loss: 0.3296, step time: 0.4367\n",
      "batch: 15/17, train_dl_loss: 0.2722, train_bce_loss: 1.6745, train_bce_dl_loss: 0.2722, step time: 0.3843\n",
      "batch: 16/17, train_dl_loss: 0.2974, train_bce_loss: 1.6782, train_bce_dl_loss: 0.2974, step time: 0.4316\n",
      "batch: 17/17, train_dl_loss: 0.2341, train_bce_loss: 1.6708, train_bce_dl_loss: 0.2341, step time: 0.1134\n",
      "LOSS train DiceLoss: 0.3077, LOSS train BCE: 1.6754, LOSS train BCE-DiceLoss: 0.3077, LOSS val DiceLoss: 0.4290, LOSS val BCE: 1.6728, LOSS val BCE-DiceLoss: 0.4290, METRIC val: 0.5371\n",
      "time consuming of epoch 307 is: 457.0548\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3099, train_bce_loss: 1.6739, train_bce_dl_loss: 0.3099, step time: 0.4386\n",
      "batch: 1/17, train_dl_loss: 0.3089, train_bce_loss: 1.6830, train_bce_dl_loss: 0.3089, step time: 0.3833\n",
      "batch: 2/17, train_dl_loss: 0.3108, train_bce_loss: 1.6731, train_bce_dl_loss: 0.3108, step time: 0.4242\n",
      "batch: 3/17, train_dl_loss: 0.4855, train_bce_loss: 1.6941, train_bce_dl_loss: 0.4855, step time: 0.3814\n",
      "batch: 4/17, train_dl_loss: 0.2446, train_bce_loss: 1.6649, train_bce_dl_loss: 0.2446, step time: 0.4310\n",
      "batch: 5/17, train_dl_loss: 0.4299, train_bce_loss: 1.6793, train_bce_dl_loss: 0.4299, step time: 0.3802\n",
      "batch: 6/17, train_dl_loss: 0.3606, train_bce_loss: 1.6753, train_bce_dl_loss: 0.3606, step time: 0.4349\n",
      "batch: 7/17, train_dl_loss: 0.3388, train_bce_loss: 1.6602, train_bce_dl_loss: 0.3388, step time: 0.3781\n",
      "batch: 8/17, train_dl_loss: 0.3586, train_bce_loss: 1.6542, train_bce_dl_loss: 0.3586, step time: 0.4356\n",
      "batch: 9/17, train_dl_loss: 0.3135, train_bce_loss: 1.6796, train_bce_dl_loss: 0.3135, step time: 0.4272\n",
      "batch: 10/17, train_dl_loss: 0.4540, train_bce_loss: 1.6650, train_bce_dl_loss: 0.4540, step time: 0.4192\n",
      "batch: 11/17, train_dl_loss: 0.3093, train_bce_loss: 1.6484, train_bce_dl_loss: 0.3093, step time: 0.4410\n",
      "batch: 12/17, train_dl_loss: 0.3023, train_bce_loss: 1.6705, train_bce_dl_loss: 0.3023, step time: 0.4465\n",
      "batch: 13/17, train_dl_loss: 0.4165, train_bce_loss: 1.6784, train_bce_dl_loss: 0.4165, step time: 0.3792\n",
      "batch: 14/17, train_dl_loss: 0.3008, train_bce_loss: 1.6821, train_bce_dl_loss: 0.3008, step time: 0.4217\n",
      "batch: 15/17, train_dl_loss: 0.3332, train_bce_loss: 1.6790, train_bce_dl_loss: 0.3332, step time: 0.3796\n",
      "batch: 16/17, train_dl_loss: 0.3101, train_bce_loss: 1.6945, train_bce_dl_loss: 0.3101, step time: 0.4336\n",
      "batch: 17/17, train_dl_loss: 0.2382, train_bce_loss: 1.6768, train_bce_dl_loss: 0.2382, step time: 0.1136\n",
      "LOSS train DiceLoss: 0.3403, LOSS train BCE: 1.6740, LOSS train BCE-DiceLoss: 0.3403, LOSS val DiceLoss: 0.4441, LOSS val BCE: 1.6758, LOSS val BCE-DiceLoss: 0.4441, METRIC val: 0.5202\n",
      "time consuming of epoch 308 is: 439.6485\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3159, train_bce_loss: 1.6698, train_bce_dl_loss: 0.3159, step time: 0.4435\n",
      "batch: 1/17, train_dl_loss: 0.3374, train_bce_loss: 1.6755, train_bce_dl_loss: 0.3374, step time: 0.3820\n",
      "batch: 2/17, train_dl_loss: 0.3247, train_bce_loss: 1.6790, train_bce_dl_loss: 0.3247, step time: 0.4256\n",
      "batch: 3/17, train_dl_loss: 0.3697, train_bce_loss: 1.6685, train_bce_dl_loss: 0.3697, step time: 0.3788\n",
      "batch: 4/17, train_dl_loss: 0.2804, train_bce_loss: 1.6701, train_bce_dl_loss: 0.2804, step time: 0.4459\n",
      "batch: 5/17, train_dl_loss: 0.3145, train_bce_loss: 1.6645, train_bce_dl_loss: 0.3145, step time: 0.3864\n",
      "batch: 6/17, train_dl_loss: 0.3511, train_bce_loss: 1.6769, train_bce_dl_loss: 0.3511, step time: 0.4463\n",
      "batch: 7/17, train_dl_loss: 0.2817, train_bce_loss: 1.6616, train_bce_dl_loss: 0.2817, step time: 0.3783\n",
      "batch: 8/17, train_dl_loss: 0.2957, train_bce_loss: 1.6733, train_bce_dl_loss: 0.2957, step time: 0.4357\n",
      "batch: 9/17, train_dl_loss: 0.2887, train_bce_loss: 1.6842, train_bce_dl_loss: 0.2887, step time: 0.3812\n",
      "batch: 10/17, train_dl_loss: 0.3892, train_bce_loss: 1.6720, train_bce_dl_loss: 0.3892, step time: 0.4236\n",
      "batch: 11/17, train_dl_loss: 0.3599, train_bce_loss: 1.6800, train_bce_dl_loss: 0.3599, step time: 0.4376\n",
      "batch: 12/17, train_dl_loss: 0.3050, train_bce_loss: 1.6708, train_bce_dl_loss: 0.3050, step time: 0.4280\n",
      "batch: 13/17, train_dl_loss: 0.4429, train_bce_loss: 1.6834, train_bce_dl_loss: 0.4429, step time: 0.3705\n",
      "batch: 14/17, train_dl_loss: 0.3145, train_bce_loss: 1.6766, train_bce_dl_loss: 0.3145, step time: 0.4351\n",
      "batch: 15/17, train_dl_loss: 0.3090, train_bce_loss: 1.6811, train_bce_dl_loss: 0.3090, step time: 0.3800\n",
      "batch: 16/17, train_dl_loss: 0.3036, train_bce_loss: 1.6753, train_bce_dl_loss: 0.3036, step time: 0.4134\n",
      "batch: 17/17, train_dl_loss: 0.2316, train_bce_loss: 1.6686, train_bce_dl_loss: 0.2316, step time: 0.1126\n",
      "LOSS train DiceLoss: 0.3231, LOSS train BCE: 1.6739, LOSS train BCE-DiceLoss: 0.3231, LOSS val DiceLoss: 0.4418, LOSS val BCE: 1.6743, LOSS val BCE-DiceLoss: 0.4418, METRIC val: 0.5229\n",
      "time consuming of epoch 309 is: 440.5951\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2866, train_bce_loss: 1.6827, train_bce_dl_loss: 0.2866, step time: 0.4421\n",
      "batch: 1/17, train_dl_loss: 0.3635, train_bce_loss: 1.6827, train_bce_dl_loss: 0.3635, step time: 0.3844\n",
      "batch: 2/17, train_dl_loss: 0.3825, train_bce_loss: 1.6892, train_bce_dl_loss: 0.3825, step time: 0.4143\n",
      "batch: 3/17, train_dl_loss: 0.3917, train_bce_loss: 1.6809, train_bce_dl_loss: 0.3917, step time: 0.3792\n",
      "batch: 4/17, train_dl_loss: 0.2775, train_bce_loss: 1.6670, train_bce_dl_loss: 0.2775, step time: 0.4263\n",
      "batch: 5/17, train_dl_loss: 0.3082, train_bce_loss: 1.6817, train_bce_dl_loss: 0.3082, step time: 0.3798\n",
      "batch: 6/17, train_dl_loss: 0.3297, train_bce_loss: 1.6811, train_bce_dl_loss: 0.3297, step time: 0.4369\n",
      "batch: 7/17, train_dl_loss: 0.2790, train_bce_loss: 1.6582, train_bce_dl_loss: 0.2790, step time: 0.3801\n",
      "batch: 8/17, train_dl_loss: 0.3007, train_bce_loss: 1.6531, train_bce_dl_loss: 0.3007, step time: 0.4254\n",
      "batch: 9/17, train_dl_loss: 0.3031, train_bce_loss: 1.6882, train_bce_dl_loss: 0.3031, step time: 0.4187\n",
      "batch: 10/17, train_dl_loss: 0.3468, train_bce_loss: 1.6838, train_bce_dl_loss: 0.3468, step time: 0.4332\n",
      "batch: 11/17, train_dl_loss: 0.3014, train_bce_loss: 1.6831, train_bce_dl_loss: 0.3014, step time: 0.4248\n",
      "batch: 12/17, train_dl_loss: 0.2803, train_bce_loss: 1.6656, train_bce_dl_loss: 0.2803, step time: 0.4218\n",
      "batch: 13/17, train_dl_loss: 0.3865, train_bce_loss: 1.6843, train_bce_dl_loss: 0.3865, step time: 0.3900\n",
      "batch: 14/17, train_dl_loss: 0.3103, train_bce_loss: 1.6855, train_bce_dl_loss: 0.3103, step time: 0.4219\n",
      "batch: 15/17, train_dl_loss: 0.2858, train_bce_loss: 1.6903, train_bce_dl_loss: 0.2858, step time: 0.3808\n",
      "batch: 16/17, train_dl_loss: 0.2864, train_bce_loss: 1.6782, train_bce_dl_loss: 0.2864, step time: 0.4238\n",
      "batch: 17/17, train_dl_loss: 0.2404, train_bce_loss: 1.6830, train_bce_dl_loss: 0.2404, step time: 0.1117\n",
      "LOSS train DiceLoss: 0.3145, LOSS train BCE: 1.6788, LOSS train BCE-DiceLoss: 0.3145, LOSS val DiceLoss: 0.4383, LOSS val BCE: 1.6722, LOSS val BCE-DiceLoss: 0.4383, METRIC val: 0.5274\n",
      "time consuming of epoch 310 is: 448.4283\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3095, train_bce_loss: 1.6647, train_bce_dl_loss: 0.3095, step time: 0.4236\n",
      "batch: 1/17, train_dl_loss: 0.2742, train_bce_loss: 1.6814, train_bce_dl_loss: 0.2742, step time: 0.3766\n",
      "batch: 2/17, train_dl_loss: 0.2777, train_bce_loss: 1.6707, train_bce_dl_loss: 0.2777, step time: 0.4407\n",
      "batch: 3/17, train_dl_loss: 0.4033, train_bce_loss: 1.6839, train_bce_dl_loss: 0.4033, step time: 0.4651\n",
      "batch: 4/17, train_dl_loss: 0.2722, train_bce_loss: 1.6731, train_bce_dl_loss: 0.2722, step time: 0.4315\n",
      "batch: 5/17, train_dl_loss: 0.2881, train_bce_loss: 1.6879, train_bce_dl_loss: 0.2881, step time: 0.4355\n",
      "batch: 6/17, train_dl_loss: 0.3664, train_bce_loss: 1.6938, train_bce_dl_loss: 0.3664, step time: 0.4446\n",
      "batch: 7/17, train_dl_loss: 0.2872, train_bce_loss: 1.6760, train_bce_dl_loss: 0.2872, step time: 0.4396\n",
      "batch: 8/17, train_dl_loss: 0.3242, train_bce_loss: 1.6639, train_bce_dl_loss: 0.3242, step time: 0.4321\n",
      "batch: 9/17, train_dl_loss: 0.3842, train_bce_loss: 1.6930, train_bce_dl_loss: 0.3842, step time: 0.4512\n",
      "batch: 10/17, train_dl_loss: 0.3875, train_bce_loss: 1.6634, train_bce_dl_loss: 0.3875, step time: 0.4208\n",
      "batch: 11/17, train_dl_loss: 0.3067, train_bce_loss: 1.6639, train_bce_dl_loss: 0.3067, step time: 0.4256\n",
      "batch: 12/17, train_dl_loss: 0.2714, train_bce_loss: 1.6635, train_bce_dl_loss: 0.2714, step time: 0.4331\n",
      "batch: 13/17, train_dl_loss: 0.4437, train_bce_loss: 1.6953, train_bce_dl_loss: 0.4437, step time: 0.4267\n",
      "batch: 14/17, train_dl_loss: 0.3037, train_bce_loss: 1.6813, train_bce_dl_loss: 0.3037, step time: 0.4196\n",
      "batch: 15/17, train_dl_loss: 0.3146, train_bce_loss: 1.6824, train_bce_dl_loss: 0.3146, step time: 0.3786\n",
      "batch: 16/17, train_dl_loss: 0.2690, train_bce_loss: 1.6903, train_bce_dl_loss: 0.2690, step time: 0.4103\n",
      "batch: 17/17, train_dl_loss: 0.2234, train_bce_loss: 1.6812, train_bce_dl_loss: 0.2234, step time: 0.1139\n",
      "LOSS train DiceLoss: 0.3171, LOSS train BCE: 1.6783, LOSS train BCE-DiceLoss: 0.3171, LOSS val DiceLoss: 0.4537, LOSS val BCE: 1.6730, LOSS val BCE-DiceLoss: 0.4537, METRIC val: 0.5105\n",
      "time consuming of epoch 311 is: 446.9482\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3039, train_bce_loss: 1.6820, train_bce_dl_loss: 0.3039, step time: 0.4277\n",
      "batch: 1/17, train_dl_loss: 0.3342, train_bce_loss: 1.6750, train_bce_dl_loss: 0.3342, step time: 0.3792\n",
      "batch: 2/17, train_dl_loss: 0.2977, train_bce_loss: 1.6831, train_bce_dl_loss: 0.2977, step time: 0.4221\n",
      "batch: 3/17, train_dl_loss: 0.3500, train_bce_loss: 1.6775, train_bce_dl_loss: 0.3500, step time: 0.3790\n",
      "batch: 4/17, train_dl_loss: 0.3015, train_bce_loss: 1.6756, train_bce_dl_loss: 0.3015, step time: 0.4365\n",
      "batch: 5/17, train_dl_loss: 0.3174, train_bce_loss: 1.6605, train_bce_dl_loss: 0.3174, step time: 0.3823\n",
      "batch: 6/17, train_dl_loss: 0.3597, train_bce_loss: 1.6852, train_bce_dl_loss: 0.3597, step time: 0.4273\n",
      "batch: 7/17, train_dl_loss: 0.3310, train_bce_loss: 1.6731, train_bce_dl_loss: 0.3310, step time: 0.3796\n",
      "batch: 8/17, train_dl_loss: 0.3090, train_bce_loss: 1.6589, train_bce_dl_loss: 0.3090, step time: 0.4176\n",
      "batch: 9/17, train_dl_loss: 0.2830, train_bce_loss: 1.6825, train_bce_dl_loss: 0.2830, step time: 0.3777\n",
      "batch: 10/17, train_dl_loss: 0.3442, train_bce_loss: 1.6703, train_bce_dl_loss: 0.3442, step time: 0.4471\n",
      "batch: 11/17, train_dl_loss: 0.3594, train_bce_loss: 1.6700, train_bce_dl_loss: 0.3594, step time: 0.4427\n",
      "batch: 12/17, train_dl_loss: 0.4000, train_bce_loss: 1.6772, train_bce_dl_loss: 0.4000, step time: 0.4338\n",
      "batch: 13/17, train_dl_loss: 0.4206, train_bce_loss: 1.6746, train_bce_dl_loss: 0.4206, step time: 0.3836\n",
      "batch: 14/17, train_dl_loss: 0.3424, train_bce_loss: 1.6918, train_bce_dl_loss: 0.3424, step time: 0.4252\n",
      "batch: 15/17, train_dl_loss: 0.2965, train_bce_loss: 1.6814, train_bce_dl_loss: 0.2965, step time: 0.3830\n",
      "batch: 16/17, train_dl_loss: 0.2954, train_bce_loss: 1.6846, train_bce_dl_loss: 0.2954, step time: 0.4100\n",
      "batch: 17/17, train_dl_loss: 0.2913, train_bce_loss: 1.6749, train_bce_dl_loss: 0.2913, step time: 0.1131\n",
      "LOSS train DiceLoss: 0.3298, LOSS train BCE: 1.6766, LOSS train BCE-DiceLoss: 0.3298, LOSS val DiceLoss: 0.4661, LOSS val BCE: 1.6786, LOSS val BCE-DiceLoss: 0.4661, METRIC val: 0.4952\n",
      "time consuming of epoch 312 is: 464.9152\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3545, train_bce_loss: 1.6733, train_bce_dl_loss: 0.3545, step time: 0.4274\n",
      "batch: 1/17, train_dl_loss: 0.3484, train_bce_loss: 1.6854, train_bce_dl_loss: 0.3484, step time: 0.3774\n",
      "batch: 2/17, train_dl_loss: 0.3199, train_bce_loss: 1.6755, train_bce_dl_loss: 0.3199, step time: 0.4309\n",
      "batch: 3/17, train_dl_loss: 0.3271, train_bce_loss: 1.6839, train_bce_dl_loss: 0.3271, step time: 0.3768\n",
      "batch: 4/17, train_dl_loss: 0.2623, train_bce_loss: 1.6850, train_bce_dl_loss: 0.2623, step time: 0.4237\n",
      "batch: 5/17, train_dl_loss: 0.3040, train_bce_loss: 1.6801, train_bce_dl_loss: 0.3040, step time: 0.3896\n",
      "batch: 6/17, train_dl_loss: 0.3567, train_bce_loss: 1.6879, train_bce_dl_loss: 0.3567, step time: 0.4255\n",
      "batch: 7/17, train_dl_loss: 0.2758, train_bce_loss: 1.6650, train_bce_dl_loss: 0.2758, step time: 0.4279\n",
      "batch: 8/17, train_dl_loss: 0.2776, train_bce_loss: 1.6847, train_bce_dl_loss: 0.2776, step time: 0.4547\n",
      "batch: 9/17, train_dl_loss: 0.3776, train_bce_loss: 1.6959, train_bce_dl_loss: 0.3776, step time: 0.4257\n",
      "batch: 10/17, train_dl_loss: 0.3682, train_bce_loss: 1.6831, train_bce_dl_loss: 0.3682, step time: 0.4388\n",
      "batch: 11/17, train_dl_loss: 0.2908, train_bce_loss: 1.6837, train_bce_dl_loss: 0.2908, step time: 0.4290\n",
      "batch: 12/17, train_dl_loss: 0.3183, train_bce_loss: 1.6646, train_bce_dl_loss: 0.3183, step time: 0.4369\n",
      "batch: 13/17, train_dl_loss: 0.4064, train_bce_loss: 1.6829, train_bce_dl_loss: 0.4064, step time: 0.4300\n",
      "batch: 14/17, train_dl_loss: 0.3186, train_bce_loss: 1.6845, train_bce_dl_loss: 0.3186, step time: 0.4199\n",
      "batch: 15/17, train_dl_loss: 0.2772, train_bce_loss: 1.6835, train_bce_dl_loss: 0.2772, step time: 0.4358\n",
      "batch: 16/17, train_dl_loss: 0.2701, train_bce_loss: 1.6865, train_bce_dl_loss: 0.2701, step time: 0.4258\n",
      "batch: 17/17, train_dl_loss: 0.3391, train_bce_loss: 1.6749, train_bce_dl_loss: 0.3391, step time: 0.1137\n",
      "LOSS train DiceLoss: 0.3218, LOSS train BCE: 1.6811, LOSS train BCE-DiceLoss: 0.3218, LOSS val DiceLoss: 0.4379, LOSS val BCE: 1.6741, LOSS val BCE-DiceLoss: 0.4379, METRIC val: 0.5273\n",
      "time consuming of epoch 313 is: 443.4207\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3738, train_bce_loss: 1.6730, train_bce_dl_loss: 0.3738, step time: 0.4333\n",
      "batch: 1/17, train_dl_loss: 0.2857, train_bce_loss: 1.6776, train_bce_dl_loss: 0.2857, step time: 0.3740\n",
      "batch: 2/17, train_dl_loss: 0.3678, train_bce_loss: 1.6832, train_bce_dl_loss: 0.3678, step time: 0.4288\n",
      "batch: 3/17, train_dl_loss: 0.4262, train_bce_loss: 1.6923, train_bce_dl_loss: 0.4262, step time: 0.3782\n",
      "batch: 4/17, train_dl_loss: 0.2949, train_bce_loss: 1.6875, train_bce_dl_loss: 0.2949, step time: 0.4447\n",
      "batch: 5/17, train_dl_loss: 0.3597, train_bce_loss: 1.7011, train_bce_dl_loss: 0.3597, step time: 0.4235\n",
      "batch: 6/17, train_dl_loss: 0.3724, train_bce_loss: 1.6944, train_bce_dl_loss: 0.3724, step time: 0.4186\n",
      "batch: 7/17, train_dl_loss: 0.2952, train_bce_loss: 1.6609, train_bce_dl_loss: 0.2952, step time: 0.4438\n",
      "batch: 8/17, train_dl_loss: 0.3188, train_bce_loss: 1.6875, train_bce_dl_loss: 0.3188, step time: 0.4312\n",
      "batch: 9/17, train_dl_loss: 0.3058, train_bce_loss: 1.6893, train_bce_dl_loss: 0.3058, step time: 0.4364\n",
      "batch: 10/17, train_dl_loss: 0.4938, train_bce_loss: 1.6810, train_bce_dl_loss: 0.4938, step time: 0.4240\n",
      "batch: 11/17, train_dl_loss: 0.3015, train_bce_loss: 1.6523, train_bce_dl_loss: 0.3015, step time: 0.4410\n",
      "batch: 12/17, train_dl_loss: 0.3266, train_bce_loss: 1.6639, train_bce_dl_loss: 0.3266, step time: 0.4304\n",
      "batch: 13/17, train_dl_loss: 0.4294, train_bce_loss: 1.6811, train_bce_dl_loss: 0.4294, step time: 0.3737\n",
      "batch: 14/17, train_dl_loss: 0.3563, train_bce_loss: 1.7004, train_bce_dl_loss: 0.3563, step time: 0.4325\n",
      "batch: 15/17, train_dl_loss: 0.3128, train_bce_loss: 1.6909, train_bce_dl_loss: 0.3128, step time: 0.3717\n",
      "batch: 16/17, train_dl_loss: 0.3465, train_bce_loss: 1.6966, train_bce_dl_loss: 0.3465, step time: 0.4259\n",
      "batch: 17/17, train_dl_loss: 0.3210, train_bce_loss: 1.6792, train_bce_dl_loss: 0.3210, step time: 0.1118\n",
      "LOSS train DiceLoss: 0.3493, LOSS train BCE: 1.6829, LOSS train BCE-DiceLoss: 0.3493, LOSS val DiceLoss: 0.4456, LOSS val BCE: 1.6757, LOSS val BCE-DiceLoss: 0.4456, METRIC val: 0.5174\n",
      "time consuming of epoch 314 is: 547.8131\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2893, train_bce_loss: 1.6679, train_bce_dl_loss: 0.2893, step time: 0.4169\n",
      "batch: 1/17, train_dl_loss: 0.3251, train_bce_loss: 1.6816, train_bce_dl_loss: 0.3251, step time: 0.3716\n",
      "batch: 2/17, train_dl_loss: 0.3357, train_bce_loss: 1.6769, train_bce_dl_loss: 0.3357, step time: 0.4302\n",
      "batch: 3/17, train_dl_loss: 0.3942, train_bce_loss: 1.6679, train_bce_dl_loss: 0.3942, step time: 0.3774\n",
      "batch: 4/17, train_dl_loss: 0.3290, train_bce_loss: 1.6647, train_bce_dl_loss: 0.3290, step time: 0.4161\n",
      "batch: 5/17, train_dl_loss: 0.3293, train_bce_loss: 1.6735, train_bce_dl_loss: 0.3293, step time: 0.3777\n",
      "batch: 6/17, train_dl_loss: 0.3429, train_bce_loss: 1.6782, train_bce_dl_loss: 0.3429, step time: 0.4393\n",
      "batch: 7/17, train_dl_loss: 0.2711, train_bce_loss: 1.6646, train_bce_dl_loss: 0.2711, step time: 0.3849\n",
      "batch: 8/17, train_dl_loss: 0.2914, train_bce_loss: 1.6777, train_bce_dl_loss: 0.2914, step time: 0.4302\n",
      "batch: 9/17, train_dl_loss: 0.3052, train_bce_loss: 1.6854, train_bce_dl_loss: 0.3052, step time: 0.4393\n",
      "batch: 10/17, train_dl_loss: 0.4231, train_bce_loss: 1.6728, train_bce_dl_loss: 0.4231, step time: 0.4280\n",
      "batch: 11/17, train_dl_loss: 0.2851, train_bce_loss: 1.6627, train_bce_dl_loss: 0.2851, step time: 0.4390\n",
      "batch: 12/17, train_dl_loss: 0.3695, train_bce_loss: 1.6662, train_bce_dl_loss: 0.3695, step time: 0.4205\n",
      "batch: 13/17, train_dl_loss: 0.4055, train_bce_loss: 1.6901, train_bce_dl_loss: 0.4055, step time: 0.3766\n",
      "batch: 14/17, train_dl_loss: 0.3137, train_bce_loss: 1.6879, train_bce_dl_loss: 0.3137, step time: 0.4227\n",
      "batch: 15/17, train_dl_loss: 0.2750, train_bce_loss: 1.6790, train_bce_dl_loss: 0.2750, step time: 0.3809\n",
      "batch: 16/17, train_dl_loss: 0.2986, train_bce_loss: 1.6877, train_bce_dl_loss: 0.2986, step time: 0.4243\n",
      "batch: 17/17, train_dl_loss: 0.4582, train_bce_loss: 1.6947, train_bce_dl_loss: 0.4582, step time: 0.1114\n",
      "LOSS train DiceLoss: 0.3357, LOSS train BCE: 1.6766, LOSS train BCE-DiceLoss: 0.3357, LOSS val DiceLoss: 0.4642, LOSS val BCE: 1.6756, LOSS val BCE-DiceLoss: 0.4642, METRIC val: 0.4970\n",
      "time consuming of epoch 315 is: 440.6529\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3092, train_bce_loss: 1.6672, train_bce_dl_loss: 0.3092, step time: 0.4296\n",
      "batch: 1/17, train_dl_loss: 0.3295, train_bce_loss: 1.6872, train_bce_dl_loss: 0.3295, step time: 0.3797\n",
      "batch: 2/17, train_dl_loss: 0.3207, train_bce_loss: 1.6827, train_bce_dl_loss: 0.3207, step time: 0.4334\n",
      "batch: 3/17, train_dl_loss: 0.4202, train_bce_loss: 1.6718, train_bce_dl_loss: 0.4202, step time: 0.3807\n",
      "batch: 4/17, train_dl_loss: 0.3469, train_bce_loss: 1.6786, train_bce_dl_loss: 0.3469, step time: 0.4346\n",
      "batch: 5/17, train_dl_loss: 0.3059, train_bce_loss: 1.6769, train_bce_dl_loss: 0.3059, step time: 0.3838\n",
      "batch: 6/17, train_dl_loss: 0.3692, train_bce_loss: 1.6794, train_bce_dl_loss: 0.3692, step time: 0.4235\n",
      "batch: 7/17, train_dl_loss: 0.2729, train_bce_loss: 1.6573, train_bce_dl_loss: 0.2729, step time: 0.3824\n",
      "batch: 8/17, train_dl_loss: 0.2873, train_bce_loss: 1.6586, train_bce_dl_loss: 0.2873, step time: 0.4523\n",
      "batch: 9/17, train_dl_loss: 0.3066, train_bce_loss: 1.6737, train_bce_dl_loss: 0.3066, step time: 0.3818\n",
      "batch: 10/17, train_dl_loss: 0.3866, train_bce_loss: 1.6714, train_bce_dl_loss: 0.3866, step time: 0.4352\n",
      "batch: 11/17, train_dl_loss: 0.3091, train_bce_loss: 1.6821, train_bce_dl_loss: 0.3091, step time: 0.4255\n",
      "batch: 12/17, train_dl_loss: 0.3266, train_bce_loss: 1.6682, train_bce_dl_loss: 0.3266, step time: 0.4287\n",
      "batch: 13/17, train_dl_loss: 0.5248, train_bce_loss: 1.6929, train_bce_dl_loss: 0.5248, step time: 0.3785\n",
      "batch: 14/17, train_dl_loss: 0.2868, train_bce_loss: 1.6884, train_bce_dl_loss: 0.2868, step time: 0.4169\n",
      "batch: 15/17, train_dl_loss: 0.2974, train_bce_loss: 1.6887, train_bce_dl_loss: 0.2974, step time: 0.3784\n",
      "batch: 16/17, train_dl_loss: 0.2813, train_bce_loss: 1.6747, train_bce_dl_loss: 0.2813, step time: 0.4268\n",
      "batch: 17/17, train_dl_loss: 0.2281, train_bce_loss: 1.6826, train_bce_dl_loss: 0.2281, step time: 0.1132\n",
      "LOSS train DiceLoss: 0.3283, LOSS train BCE: 1.6768, LOSS train BCE-DiceLoss: 0.3283, LOSS val DiceLoss: 0.4278, LOSS val BCE: 1.6736, LOSS val BCE-DiceLoss: 0.4278, METRIC val: 0.5378\n",
      "time consuming of epoch 316 is: 461.4558\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.3236, train_bce_loss: 1.6646, train_bce_dl_loss: 0.3236, step time: 0.4258\n",
      "batch: 1/17, train_dl_loss: 0.3218, train_bce_loss: 1.6952, train_bce_dl_loss: 0.3218, step time: 0.3780\n",
      "batch: 2/17, train_dl_loss: 0.3439, train_bce_loss: 1.6746, train_bce_dl_loss: 0.3439, step time: 0.4321\n",
      "batch: 3/17, train_dl_loss: 0.4062, train_bce_loss: 1.6770, train_bce_dl_loss: 0.4062, step time: 0.3786\n",
      "batch: 4/17, train_dl_loss: 0.2592, train_bce_loss: 1.6651, train_bce_dl_loss: 0.2592, step time: 0.4418\n",
      "batch: 5/17, train_dl_loss: 0.3403, train_bce_loss: 1.6660, train_bce_dl_loss: 0.3403, step time: 0.4344\n",
      "batch: 6/17, train_dl_loss: 0.3689, train_bce_loss: 1.6875, train_bce_dl_loss: 0.3689, step time: 0.4972\n",
      "batch: 7/17, train_dl_loss: 0.2552, train_bce_loss: 1.6673, train_bce_dl_loss: 0.2552, step time: 0.4423\n",
      "batch: 8/17, train_dl_loss: 0.2966, train_bce_loss: 1.6536, train_bce_dl_loss: 0.2966, step time: 0.4135\n",
      "batch: 9/17, train_dl_loss: 0.4095, train_bce_loss: 1.6807, train_bce_dl_loss: 0.4095, step time: 0.4210\n",
      "batch: 10/17, train_dl_loss: 0.3716, train_bce_loss: 1.6838, train_bce_dl_loss: 0.3716, step time: 0.4333\n",
      "batch: 11/17, train_dl_loss: 0.2805, train_bce_loss: 1.6692, train_bce_dl_loss: 0.2805, step time: 0.4395\n",
      "batch: 12/17, train_dl_loss: 0.3006, train_bce_loss: 1.6695, train_bce_dl_loss: 0.3006, step time: 0.4208\n",
      "batch: 13/17, train_dl_loss: 0.4339, train_bce_loss: 1.6861, train_bce_dl_loss: 0.4339, step time: 0.4257\n",
      "batch: 14/17, train_dl_loss: 0.3758, train_bce_loss: 1.6849, train_bce_dl_loss: 0.3758, step time: 0.4383\n",
      "batch: 15/17, train_dl_loss: 0.2917, train_bce_loss: 1.6853, train_bce_dl_loss: 0.2917, step time: 0.3804\n",
      "batch: 16/17, train_dl_loss: 0.2907, train_bce_loss: 1.6824, train_bce_dl_loss: 0.2907, step time: 0.4096\n",
      "batch: 17/17, train_dl_loss: 0.2934, train_bce_loss: 1.6734, train_bce_dl_loss: 0.2934, step time: 0.1122\n",
      "LOSS train DiceLoss: 0.3313, LOSS train BCE: 1.6759, LOSS train BCE-DiceLoss: 0.3313, LOSS val DiceLoss: 0.4324, LOSS val BCE: 1.6750, LOSS val BCE-DiceLoss: 0.4324, METRIC val: 0.5320\n",
      "time consuming of epoch 317 is: 417.0974\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2965, train_bce_loss: 1.6645, train_bce_dl_loss: 0.2965, step time: 0.4196\n",
      "batch: 1/17, train_dl_loss: 0.2939, train_bce_loss: 1.6788, train_bce_dl_loss: 0.2939, step time: 0.3763\n",
      "batch: 2/17, train_dl_loss: 0.4162, train_bce_loss: 1.6839, train_bce_dl_loss: 0.4162, step time: 0.4759\n",
      "batch: 3/17, train_dl_loss: 0.3513, train_bce_loss: 1.6830, train_bce_dl_loss: 0.3513, step time: 0.3730\n",
      "batch: 4/17, train_dl_loss: 0.2866, train_bce_loss: 1.6849, train_bce_dl_loss: 0.2866, step time: 0.4377\n",
      "batch: 5/17, train_dl_loss: 0.2956, train_bce_loss: 1.6714, train_bce_dl_loss: 0.2956, step time: 0.3856\n",
      "batch: 6/17, train_dl_loss: 0.3469, train_bce_loss: 1.6887, train_bce_dl_loss: 0.3469, step time: 0.4325\n",
      "batch: 7/17, train_dl_loss: 0.3216, train_bce_loss: 1.6611, train_bce_dl_loss: 0.3216, step time: 0.3816\n",
      "batch: 8/17, train_dl_loss: 0.3276, train_bce_loss: 1.6823, train_bce_dl_loss: 0.3276, step time: 0.4326\n",
      "batch: 9/17, train_dl_loss: 0.3173, train_bce_loss: 1.6763, train_bce_dl_loss: 0.3173, step time: 0.3823\n",
      "batch: 10/17, train_dl_loss: 0.4436, train_bce_loss: 1.6841, train_bce_dl_loss: 0.4436, step time: 0.4294\n",
      "batch: 11/17, train_dl_loss: 0.2585, train_bce_loss: 1.6647, train_bce_dl_loss: 0.2585, step time: 0.4251\n",
      "batch: 12/17, train_dl_loss: 0.2931, train_bce_loss: 1.6652, train_bce_dl_loss: 0.2931, step time: 0.4287\n",
      "batch: 13/17, train_dl_loss: 0.3892, train_bce_loss: 1.6932, train_bce_dl_loss: 0.3892, step time: 0.3919\n",
      "batch: 14/17, train_dl_loss: 0.2844, train_bce_loss: 1.6831, train_bce_dl_loss: 0.2844, step time: 0.4223\n",
      "batch: 15/17, train_dl_loss: 0.2842, train_bce_loss: 1.6904, train_bce_dl_loss: 0.2842, step time: 0.3723\n",
      "batch: 16/17, train_dl_loss: 0.2768, train_bce_loss: 1.6890, train_bce_dl_loss: 0.2768, step time: 0.4139\n",
      "batch: 17/17, train_dl_loss: 0.2667, train_bce_loss: 1.7001, train_bce_dl_loss: 0.2667, step time: 0.1115\n",
      "LOSS train DiceLoss: 0.3194, LOSS train BCE: 1.6803, LOSS train BCE-DiceLoss: 0.3194, LOSS val DiceLoss: 0.4365, LOSS val BCE: 1.6777, LOSS val BCE-DiceLoss: 0.4365, METRIC val: 0.5290\n",
      "time consuming of epoch 318 is: 438.7420\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2980, train_bce_loss: 1.6789, train_bce_dl_loss: 0.2980, step time: 0.4357\n",
      "batch: 1/17, train_dl_loss: 0.3162, train_bce_loss: 1.6879, train_bce_dl_loss: 0.3162, step time: 0.3801\n",
      "batch: 2/17, train_dl_loss: 0.3135, train_bce_loss: 1.6889, train_bce_dl_loss: 0.3135, step time: 0.9822\n",
      "batch: 3/17, train_dl_loss: 0.3957, train_bce_loss: 1.6783, train_bce_dl_loss: 0.3957, step time: 0.3798\n",
      "batch: 4/17, train_dl_loss: 0.2664, train_bce_loss: 1.6766, train_bce_dl_loss: 0.2664, step time: 0.4309\n",
      "batch: 5/17, train_dl_loss: 0.2709, train_bce_loss: 1.6706, train_bce_dl_loss: 0.2709, step time: 0.3828\n",
      "batch: 6/17, train_dl_loss: 0.3724, train_bce_loss: 1.6829, train_bce_dl_loss: 0.3724, step time: 0.4297\n",
      "batch: 7/17, train_dl_loss: 0.2868, train_bce_loss: 1.6910, train_bce_dl_loss: 0.2868, step time: 0.3778\n",
      "batch: 8/17, train_dl_loss: 0.2776, train_bce_loss: 1.6909, train_bce_dl_loss: 0.2776, step time: 0.4224\n",
      "batch: 9/17, train_dl_loss: 0.3193, train_bce_loss: 1.6986, train_bce_dl_loss: 0.3193, step time: 0.3767\n",
      "batch: 10/17, train_dl_loss: 0.3525, train_bce_loss: 1.6809, train_bce_dl_loss: 0.3525, step time: 0.4162\n",
      "batch: 11/17, train_dl_loss: 0.3473, train_bce_loss: 1.6975, train_bce_dl_loss: 0.3473, step time: 0.4309\n",
      "batch: 12/17, train_dl_loss: 0.3598, train_bce_loss: 1.6766, train_bce_dl_loss: 0.3598, step time: 0.4283\n",
      "batch: 13/17, train_dl_loss: 0.3975, train_bce_loss: 1.6859, train_bce_dl_loss: 0.3975, step time: 0.3733\n",
      "batch: 14/17, train_dl_loss: 0.3213, train_bce_loss: 1.7000, train_bce_dl_loss: 0.3213, step time: 0.4278\n",
      "batch: 15/17, train_dl_loss: 0.2902, train_bce_loss: 1.6789, train_bce_dl_loss: 0.2902, step time: 0.3781\n",
      "batch: 16/17, train_dl_loss: 0.2808, train_bce_loss: 1.6933, train_bce_dl_loss: 0.2808, step time: 0.4092\n",
      "batch: 17/17, train_dl_loss: 0.2937, train_bce_loss: 1.6789, train_bce_dl_loss: 0.2937, step time: 0.1114\n",
      "LOSS train DiceLoss: 0.3200, LOSS train BCE: 1.6854, LOSS train BCE-DiceLoss: 0.3200, LOSS val DiceLoss: 0.4503, LOSS val BCE: 1.6802, LOSS val BCE-DiceLoss: 0.4503, METRIC val: 0.5165\n",
      "time consuming of epoch 319 is: 425.4643\n",
      "----------\n",
      "EPOCH 241/320\n",
      "batch: 0/17, train_dl_loss: 0.2997, train_bce_loss: 1.6822, train_bce_dl_loss: 0.2997, step time: 0.4367\n",
      "batch: 1/17, train_dl_loss: 0.3927, train_bce_loss: 1.6974, train_bce_dl_loss: 0.3927, step time: 0.3731\n",
      "batch: 2/17, train_dl_loss: 0.2977, train_bce_loss: 1.6762, train_bce_dl_loss: 0.2977, step time: 0.4126\n",
      "batch: 3/17, train_dl_loss: 0.3871, train_bce_loss: 1.6846, train_bce_dl_loss: 0.3871, step time: 0.4330\n",
      "batch: 4/17, train_dl_loss: 0.2782, train_bce_loss: 1.6725, train_bce_dl_loss: 0.2782, step time: 0.4257\n",
      "batch: 5/17, train_dl_loss: 0.2775, train_bce_loss: 1.6870, train_bce_dl_loss: 0.2775, step time: 0.3822\n",
      "batch: 6/17, train_dl_loss: 0.3804, train_bce_loss: 1.6860, train_bce_dl_loss: 0.3804, step time: 0.4323\n",
      "batch: 7/17, train_dl_loss: 0.2800, train_bce_loss: 1.6653, train_bce_dl_loss: 0.2800, step time: 0.3833\n",
      "batch: 8/17, train_dl_loss: 0.3105, train_bce_loss: 1.6670, train_bce_dl_loss: 0.3105, step time: 0.4179\n",
      "batch: 9/17, train_dl_loss: 0.3071, train_bce_loss: 1.6812, train_bce_dl_loss: 0.3071, step time: 0.4326\n",
      "batch: 10/17, train_dl_loss: 0.3824, train_bce_loss: 1.6824, train_bce_dl_loss: 0.3824, step time: 0.4357\n",
      "batch: 11/17, train_dl_loss: 0.3122, train_bce_loss: 1.6689, train_bce_dl_loss: 0.3122, step time: 0.4204\n",
      "batch: 12/17, train_dl_loss: 0.5072, train_bce_loss: 1.6977, train_bce_dl_loss: 0.5072, step time: 0.4230\n",
      "batch: 13/17, train_dl_loss: 0.4152, train_bce_loss: 1.6935, train_bce_dl_loss: 0.4152, step time: 0.3839\n",
      "batch: 14/17, train_dl_loss: 0.3112, train_bce_loss: 1.6836, train_bce_dl_loss: 0.3112, step time: 0.4298\n",
      "batch: 15/17, train_dl_loss: 0.2811, train_bce_loss: 1.6824, train_bce_dl_loss: 0.2811, step time: 0.3761\n",
      "batch: 16/17, train_dl_loss: 0.3116, train_bce_loss: 1.6896, train_bce_dl_loss: 0.3116, step time: 0.4189\n",
      "batch: 17/17, train_dl_loss: 0.3119, train_bce_loss: 1.6773, train_bce_dl_loss: 0.3119, step time: 0.1121\n",
      "LOSS train DiceLoss: 0.3358, LOSS train BCE: 1.6819, LOSS train BCE-DiceLoss: 0.3358, LOSS val DiceLoss: 0.4418, LOSS val BCE: 1.6809, LOSS val BCE-DiceLoss: 0.4418, METRIC val: 0.5250\n",
      "time consuming of epoch 320 is: 415.5414\n"
     ]
    }
   ],
   "source": [
    "# Resuming training loop\n",
    "resuming_train_loop(model=unet_model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    diceloss_function=dice_loss_fn,\n",
    "                    bce_function=bce_loss_fn,\n",
    "                    bce_diceloss_function=criterion,\n",
    "                    metric=dice_metric,\n",
    "                    optimizer=optimizer,\n",
    "                    lr_scheduler=lr_scheduler,\n",
    "                    start_epoch=start_epoch,\n",
    "                    config=config,\n",
    "                    output_dir=OUTPUT_DIR,\n",
    "                    output_file=OUTPUT_FILE,\n",
    "                    device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4af94",
   "metadata": {
    "papermill": {
     "duration": 0.121317,
     "end_time": "2024-08-21T21:26:06.271893",
     "exception": false,
     "start_time": "2024-08-21T21:26:06.150576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Plot Training/Validation Losses & Validation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "252e5de2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T21:26:06.518784Z",
     "iopub.status.busy": "2024-08-21T21:26:06.518408Z",
     "iopub.status.idle": "2024-08-21T21:26:07.399529Z",
     "shell.execute_reply": "2024-08-21T21:26:07.398689Z"
    },
    "papermill": {
     "duration": 1.006566,
     "end_time": "2024-08-21T21:26:07.401629",
     "exception": false,
     "start_time": "2024-08-21T21:26:06.395063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load txt file\n",
    "path = f\"/kaggle/working/{config['ID']}_train_val_losses.csv\"\n",
    "train_val_losses = pd.read_csv(path)\n",
    "\n",
    "# Plot and Save Train & Val Losses as png in output dir\n",
    "training_plot(train_val_losses, OUTPUT_DIR, config)\n",
    "\n",
    "# Plot and Save Val Metric as png in output dir\n",
    "validation_metric_plot(train_val_losses, OUTPUT_DIR, config)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 4050810,
     "sourceId": 36363,
     "sourceType": "competition"
    },
    {
     "datasetId": 5474572,
     "sourceId": 9075209,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5564224,
     "sourceId": 9202981,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5569045,
     "sourceId": 9210129,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5574103,
     "sourceId": 9217666,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 191398178,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36899.789079,
   "end_time": "2024-08-21T21:26:10.324154",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-21T11:11:10.535075",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
